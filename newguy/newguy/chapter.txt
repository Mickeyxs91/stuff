


                                                                       section 4
                                              swarm basic features and how to use them in your workflow
18-----------------------------------------------------swarm stacks and production grade compose----------------------------------------
.......
C:\Users\michael.kourbelis>
docker swarm leave --force
Node left the swarm.

C:\Users\michael.kourbelis>
docker swarm init
Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on 
different interfaces (10.0.2.15 on eth0 and 192.168.99.117 on eth1) - specify one with --advertise-addr

C:\Users\michael.kourbelis>
docker swarm init 192.168.99.117
"docker swarm init" accepts no arguments.
See 'docker swarm init --help'.

Usage:  docker swarm init [OPTIONS]

Initialize a swarm

C:\Users\michael.kourbelis>
docker swarm init --advertise-addr 192.168.99.117
Swarm initialized: current node (4rw3mfwgbdoguafl63e39sfqf) is now a manager.

To add a worker to this swarm, run the following command:

docker swarm join --token SWMTKN-1-1sq0tfe1bg2m54lx4bsi6gm5yg49nwtrkzswlnke7j9cvsubpb-f05akie2fbbgz4mm5dc4qmx7u 192.168.99.117:2377


C:\Users\michael.kourbelis>
docker-machine create nodei

C:\Users\michael.kourbelis>
docker-machine create nodeii

C:\Users\michael.kourbelis>
docker-machine create nodeiii

C:\Users\michael.kourbelis> docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
4rw3mfwgbdoguafl63e39sfqf *   default             Ready               Active              Leader              19.03.12


now lets look at the stacks and how they work....
i am back in my 3 node swarm that i built earlier and basically i am going to work on the default node as it is the basic one 
and from there i can add the other nodes at the swarm and make them not leaders as leader is one but i can make them managers (reachables).



C:\Users\michael.kourbelis>docker-machine ssh nodei
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@nodei:~$ docker node update  --role manager nodei

Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.

docker@nodei:~$ docker swarm join --token SWMTKN-1-1sq0tfe1bg2m54lx4bsi6gm5yg49nwtrkzswlnke7j9cvsubpb-f05akie2fbbgz4mm5dc4qmx7u 192.168.99.117:2377
This node joined a swarm as a worker.
docker@nodei:~$ exit
logout

   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@nodeii:~$ docker swarm join --token SWMTKN-1-1sq0tfe1bg2m54lx4bsi6gm5yg49nwtrkzswlnke7j9cvsubpb-f05akie2fbbgz4mm5dc4qmx7u 192.168.99.117:2377
This node joined a swarm as a worker.
docker@nodeii:~$ exit
logout

C:\Users\michael.kourbelis>docker-machine ssh nodeiii
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@nodeiii:~$ docker swarm join --token SWMTKN-1-1sq0tfe1bg2m54lx4bsi6gm5yg49nwtrkzswlnke7j9cvsubpb-f05akie2fbbgz4mm5dc4qmx7u 192.168.99.117:2377
This node joined a swarm as a worker.
docker@nodeiii:~$ docker node update  --role manager nodeiii
Error response from daemon: This node is not a swarm manager. Worker nodes can't be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.
docker@nodeiii:~$ exit
logout
exit status 1

C:\Users\michael.kourbelis>docker node update  --role manager nodei
nodei

C:\Users\michael.kourbelis>docker node update  --role manager nodeii
nodeii

C:\Users\michael.kourbelis>docker node update  --role manager nodeiii
nodeiii


C:\Users\michael.kourbelis> docker node ls
ID                            HOSTNAME            STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION
4rw3mfwgbdoguafl63e39sfqf *   default             Ready               Active              Leader              19.03.12
gnan5i9dr3siwf2trwjthlvbt     nodei               Ready               Active              Reachable           19.03.12
0w3bkljkf1seomurqvoyrrbjp     nodeii              Ready               Active              Reachable           19.03.12
dq7i2cuzid0l8mn25a5la36us     nodeiii             Ready               Active              Reachable           19.03.12

and now i can enter nodei and do some stuff, here we are going to use the voting app example and if you remember our design for that 
it was five different services and the all had dependencies on each other and it ultimately gave us two different websites,
you went through and i am sure you crafted the best services list with all the values and options that it needed and it was perfect right?
well i am here to tell you that now is no longer needed....it was really important for us to use them  and not everything is going to need a stack file.
but now i have a stack file, lets just take a peek at it rela quick because you will notice that it looks very much like a compose file because it is.
the only real key here thing you have to change as it has to be at least version 3 or higher.
the latest version right now is version 3.1 but i am sure that it will change as it continues to change and mature, but you need version 3 in order to use stacks,
you will notice that we have our redis and our db and our vote and all these things that are probably very similar to you from your own work,you are probably 
very familiar with those and the images they need and the ports tey need open and all that.....but in this case you will notice deploy.
deploy here is a new option you will see here that it was specifying how many replicas i want, which is how many copies of that image that need to run at a time, and 
what happens when i do an update, so when i do an actual stack update which will then do service updates....
how do i want that to roll out ? 
do i want to go down at the same time ?
do i only want one at a time ?
how much delay between them ?.....there's all sorts of options here

we are just scratching the surface, but you can see like i have a restart policy here that if the containers fails it will automatically restart it :

   restart_policy:
         condition: on-failure


you can see  down here under the database i've actually had constraints put in to make sure that it's on a specific node and we haven't talked really a lot about 
constraints yet but there are ways for us to label objects that is containers or images or really anything that we can create or destroy in swarm or in 
Docker itself.

we can actually assign labels to any of those, including nodes and a node that is a manager gets its own labels and this is very easy for us to to do to just say
hey this container has has to run on a node that has this particular role.......
As we scroll down we ve got some parallelism options which we have seen before....the delay option is pretty cool if you have some sort of 
warm up time when you have sort of warm up time when you have containers that spin up, maybe they started ut the don't actually go live for maybe 60 seconds 
or something, you can add delays there and we will actually look at those later during production blue green deployments but for now you can see those are 
all pretty standard we've actually got even more options down here, you can actually see that i assign this a specific label (labels: [APP=VOTING])...
i have a window and a max attempt for a restart policy so if it tries to restart and it continues to fail its not going to try more than three times.....

       restart_policy:
         condition: on-failure
         delay: 10s
         max_attempts: 3
         window: 120s

lets check it out, all i have to do  is do a docker stack deploy -c (-c flag stands for compose), so i am going to use a compose file and call it 
vote app through "voteapp"........." docker stack deploy -c compose.yml voteapp "...........
and there we go.....

C:\Users\michael.kourbelis\Desktop\composee>docker stack deploy -c compose.yml voteapp
Creating network voteapp_default
Creating network voteapp_frontend
Creating network voteapp_backend
Creating service voteapp_redis
Creating service voteapp_db
Creating service voteapp_vote
Creating service voteapp_result
Creating service voteapp_worker
Creating service voteapp_visualizer

it didn't create actually create everything and spin it up thta fast....all it did was create those objects in the scheduler, which will then 
go through the process of creating the services which then creating the tasks which then creating the containers, it also has to create the networks as 
you will see here and remember that here we have the frontend and the backend   so it created those as well as a default network which 
that particular stack file that i had was using a default network and then you see this option for visualizer which is a new one we did not use before 
and we will see that in a minute.
so lets take a look at the docker stack command a llitle more......



C:\Users\michael.kourbelis\Desktop\composee>docker stack

Usage:  docker stack [OPTIONS] COMMAND

Manage Docker stacks

Options:
      --orchestrator string   Orchestrator to use (swarm|kubernetes|all)

Commands:
  deploy      Deploy a new stack or update an existing stack
  ls          List stacks
  ps          List the tasks in the stack
  rm          Remove one or more stacks
  services    List the services in the stack

Run 'docker stack COMMAND --help' for more information on a command.

C:\Users\michael.kourbelis\Desktop\composee>



you will see that we can deploy we have done that already and then you can see that we have ls, ps, rm and services, so this command doesn't have a 
whole lot of features to it, its preety simple because all of the functionality is really in the compose file and really in the objects its creating 
and since we have done that already we can do things like "docker stack ls" which shows just shows us all of our stacks and then if i do 
docker stack ps voteapp you see the actual tasks and then you can see which node they are running on......


C:\Users\michael.kourbelis\Desktop\composee>docker stack ps voteapp
ID                  NAME                   IMAGE                                          NODE                DESIRED STATE       CURRENT STATE            ERROR                       PORTS
5gotupbkljjb        voteapp_db.1           postgres:9.4                                   default             Ready               Ready 2 seconds ago
5m4h9xo62n9e         \_ voteapp_db.1       postgres:9.4                                   nodeiii             Shutdown            Failed 3 seconds ago     "task: non-zero exit (1)"
mgj7byizthmg         \_ voteapp_db.1       postgres:9.4                                   nodeii              Shutdown            Failed 9 seconds ago     "task: non-zero exit (1)"
w7mt2wlgsbn9         \_ voteapp_db.1       postgres:9.4                                   nodeii              Shutdown            Failed 15 seconds ago    "task: non-zero exit (1)"
u9k8engjgm1b         \_ voteapp_db.1       postgres:9.4                                   nodeii              Shutdown            Failed 20 seconds ago    "task: non-zero exit (1)"
i5pn01yz5v41        voteapp_result.1       dockersamples/examplevotingapp_result:before   nodeii              Running             Running 12 minutes ago
lvbmpf7e7w6k        voteapp_worker.1       dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed 28 minutes ago    "task: non-zero exit (1)"
latayiwyp53y         \_ voteapp_worker.1   dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed 29 minutes ago    "task: non-zero exit (1)"
uqslibms6yuq         \_ voteapp_worker.1   dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed 29 minutes ago    "task: non-zero exit (1)"
316d1wjhniga        voteapp_visualizer.1   dockersamples/visualizer:latest                nodeiii             Running             Running 31 minutes ago
scdn99cy93hm        voteapp_worker.1       dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed 30 minutes ago    "task: non-zero exit (1)"
kdf32a1qakqd        voteapp_result.1       dockersamples/examplevotingapp_result:before   default             Shutdown            Failed 12 minutes ago    "task: non-zero exit (1)"
tog4zfmhlpjg        voteapp_vote.1         dockersamples/examplevotingapp_vote:before     nodeiii             Running             Running 33 minutes ago
6i8pqvxbg9dt        voteapp_redis.1        redis:alpine                                   nodei               Running             Running 33 minutes ago
nzvufrjidce6        voteapp_vote.2         dockersamples/examplevotingapp_vote:before     nodei               Running             Running 33 minutes ago
zfgdx5vgl74z        voteapp_redis.2        redis:alpine                                   default             Running             Running 33 minutes ago

C:\Users\michael.kourbelis\Desktop\composee>



its not actually the containers its actally te tasks that we are seeing here because if it was the actual container we would see a big long name 
because if you remember the services we created earlier they had these really long names if we actually went and did it docker ps. right?....
if i did a docker container ls or docker container ps we will see a really long name at the names attribute


C:\Users\michael.kourbelis\Desktop\composee> docker container ls
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
764e92abd367        redis:alpine        "docker-entrypoint.s…"   46 minutes ago      Up 46 minutes       6379/tcp            voteapp_redis.2.zfgdx5vgl74zunorg4l58qms5


and that because the all get a guid because every container have to be uniquely named and thats how they guarantee that they are always unique and never collide.
And the last one is ....  "docker stack services voteapp" and this is the best because it shows me my replicas and its kind like doing a " docker service ls "


C:\Users\michael.kourbelis\Desktop\composee> docker stack services voteapp
ID                  NAME                 MODE                REPLICAS            IMAGE                                          PORTS
2ffzqqj3edvq        voteapp_redis        replicated          2/2                 redis:alpine                                   *:30000->6379/tcp
p1kovsw4so7z        voteapp_worker       replicated          0/1                 dockersamples/examplevotingapp_worker:latest
sag7eovg3b9g        voteapp_result       replicated          1/1                 dockersamples/examplevotingapp_result:before   *:5003->80/tcp
szem3sh6pbhz        voteapp_db           replicated          0/1                 postgres:9.4
thpoqadkbw33        voteapp_vote         replicated          2/2                 dockersamples/examplevotingapp_vote:before     *:5000->80/tcp
xec6633ojqn8        voteapp_visualizer   replicated          1/1                 dockersamples/visualizer:latest                *:8080->8080/tcp

C:\Users\michael.kourbelis\Desktop\composee>


it shows me how many replicas i have started so i know whether i ve got the proper number of containers already started, and then if i want to dive deeper then i could 
do ...docker stack ps.....where there we can get the task names here and we see what nodes they are runnning on....

C:\Users\michael.kourbelis\Desktop\composee>docker stack ps voteapp
ID                  NAME                   IMAGE                                          NODE                DESIRED STATE       CURRENT STATE               ERROR                       PORTS
r9loanac7agu        voteapp_db.1           postgres:9.4                                   nodeiii             Ready               Ready 1 second ago
yel97xslg67f         \_ voteapp_db.1       postgres:9.4                                   nodeiii             Shutdown            Failed 1 second ago         "task: non-zero exit (1)"
p48e6ifsxm8l         \_ voteapp_db.1       postgres:9.4                                   nodeii              Shutdown            Failed 7 seconds ago        "task: non-zero exit (1)"
x89ire7azwus         \_ voteapp_db.1       postgres:9.4                                   default             Shutdown            Failed 13 seconds ago       "task: non-zero exit (1)"
7itw873emonu         \_ voteapp_db.1       postgres:9.4                                   default             Shutdown            Failed 19 seconds ago       "task: non-zero exit (1)"
5bgdexzrhmg5        voteapp_result.1       dockersamples/examplevotingapp_result:before   nodeii              Running             Running 3 minutes ago
7ixxdjwcd892         \_ voteapp_result.1   dockersamples/examplevotingapp_result:before   default             Shutdown            Failed 3 minutes ago        "task: non-zero exit (1)"
t808ut1aq4d3         \_ voteapp_result.1   dockersamples/examplevotingapp_result:before   nodeii              Shutdown            Failed 22 minutes ago       "task: non-zero exit (1)"
i5pn01yz5v41         \_ voteapp_result.1   dockersamples/examplevotingapp_result:before   nodeii              Shutdown            Failed 41 minutes ago       "task: non-zero exit (1)"
lvbmpf7e7w6k        voteapp_worker.1       dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed about an hour ago    "task: non-zero exit (1)"
latayiwyp53y         \_ voteapp_worker.1   dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed about an hour ago    "task: non-zero exit (1)"
uqslibms6yuq         \_ voteapp_worker.1   dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed about an hour ago    "task: non-zero exit (1)"
316d1wjhniga        voteapp_visualizer.1   dockersamples/visualizer:latest                nodeiii             Running             Running about an hour ago
scdn99cy93hm        voteapp_worker.1       dockersamples/examplevotingapp_worker:latest   nodeii              Shutdown            Failed about an hour ago    "task: non-zero exit (1)"
kdf32a1qakqd        voteapp_result.1       dockersamples/examplevotingapp_result:before   default             Shutdown            Failed about an hour ago    "task: non-zero exit (1)"
tog4zfmhlpjg        voteapp_vote.1         dockersamples/examplevotingapp_vote:before     nodeiii             Running             Running about an hour ago
6i8pqvxbg9dt        voteapp_redis.1        redis:alpine                                   nodei               Running             Running about an hour ago
nzvufrjidce6        voteapp_vote.2         dockersamples/examplevotingapp_vote:before     nodei               Running             Running about an hour ago
zfgdx5vgl74z        voteapp_redis.2        redis:alpine                                   default             Running             Running about an hour ago

C:\Users\michael.kourbelis\Desktop\composee>



so these two commands can give you a complete picture of how this entire application is running and if i wanted to deep dive into networking i could do a docker
network " docker network ls " just ike we normally do and you can see these three new overlay networks that were created for this app and notice that the app name is 
always at the beggining as the stack alwasy precedes the name of the service so each stack will make sure that that its name is at the front

C:\Users\michael.kourbelis\Desktop\composee>docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
dae42f9c3ff6        bridge              bridge              local
c9a6f01edc60        docker_gwbridge     bridge              local
acf36657a3b9        host                host                local
0457rj03mn6u        ingress             overlay             swarm
35c6d0774ac4        new_default         bridge              local
cc4f3c68d312        none                null                local
0d4d7qts9rxv        voteapp_backend     overlay             swarm
mmk46nqai7qf        voteapp_default     overlay             swarm
rtasu3ntcy08        voteapp_frontend    overlay             swarm

C:\Users\michael.kourbelis\Desktop\composee>
 
....and then lets go and check it out.... leyts type at the browser the ip of the service with the port specified on the compose file and then you see the site 

http://192.168.99.117:5003/



and then on http://192.168.99.117:8080 we have the visualizer which is a preety neat tool for demonstration purposes thats actually made by docker by the way and these are all an open source 
repo that you can see on the resources section of this section 
and so we have different colors for each service so
the voteapp is with blue color and the vote is a pink, 
now if we go tou our compose file and make a little change that has to do with the replicas i.e. to change them from 3 to five, we could go and update the service, but that's knd of antipattern.
if we go in and type manually commands into the service like docker service update that would mean that next time i reapply this YAML file, it's going to overwrite those changes.
so if you ever done a cloud information or formation i dont know in aws or any other configuration management you know that once you are using a config file like this to manage your 
infrastracture you really want to always use this file because its going to be the source of truth.
so it probably  in production would be some sort of file that you keep in a git Repository that you have version control on and you control changes, and that way you can roll back changes and in this 
case, i'm just going to change that file real quick and we are going to run the same command............docker stack  deploy.....and notice that we dont have an update because we are going to deploy to 
the same stack and its going to recognize that its existing and that we are going to update it based on these changes, so in order to update it we have got to make sure we use the right name....
so its " docker stack deploy voteapp " aned then you will notice that it says updating services but recognizes that it needs to change them.
and then if we go over to our visualizer you can see that we have already got it i.e. we have five of these voting apps already running, if they took a while to run and launch we could actually 
see their states change in hers as the came online  but its a web app so that it will starts real quickly.
okay that stucks now lets add on these the secrets to our stacks. 


18-----------------------------------------------------swarm stacks and production grade compose----------------------------------------

19-------------------------------secrets storage for swarm : protecting your environment variables---------------------------------------

  alright so a new feature in 1.13.1 was full support for secrets, 
  if i had to create a tag line of what this is its basically the easiest secure solution for storing secrets in swarm...i say the easiest because it's built into swarm, 
  it comes out of the box and there's nothing you need to do to use it as long as you ve initialized your swarm and you are on version 1.13 or newer 
  you've got secrets.
  i say its secure because it was designed from the ground up to be encrypted on disk to be encrypted in transit and to only be available to the places it needs to be, and that's really what we need.
  so what is a secret ?
  a secret in this case is classifying anything that you don't want on the front page of a newspaper. 
  if it got on the front page and you had to go change it that's a secret, 
  its a username or password 
  its a tls certificate or the keys to that 
  its an ssh key 
  its a twitter api key 
  its an amazon key and in general 
  its anything that you need to allow connectivity between stuff is probably a secret 
  and you should be protecting it.

  until now we haven't had a lot of great options for swarm, 
  now there is definitely lots of options out there like vault and other great tools for storing secrets but they weren't built in and the required a seperate 
  infrastracture set-up just so that you could even start using them.
  so we can store in here anything that's a string or a binary up to 5mb in size and the coolest part about this is that it doesn't actually require your app to be rewritten in order to use it.
  you don't have to have your app talk to a web service somewhere else in order to get these.

  so lets see how it works...........................

  as of 1.13.0 the swarm raft database is encrypted on disk by default.
  if you install docker and do a swarm init like we have done before that's an encrypted database and when it shuts down the service, its encrypted with the keys stored securely , 
  its only stored on the disk of the manager nodes and they are the only ones that have the keys to unlock it or decrypt it.
  this is already existing in swarm but basically the way that the keys get down to the containers is through the control plane or the encrypted tls network communications between the managers and
  the workers and that connection was already secure, it already used tls and mutual PKI authentication so it was a great way to use that existing channel for bringing these secrets down to our containers.
  the way we get them around is we actually first put them into the swarm database using docker secrets  commands and then we assign them to the services whether we use the service commands  themselves 
  or a stack file to tell swarm who is allowed to use this secrets.
  the key here is that just because there's a container on a host or on a node and you've assigned the key to that service doesn't mean other containers can get access to it,
  since this is built in to the docker engine, the docker worker keeps that key secure in memory only and only gets down to the containers on that node that need them.
  now how they are presented in the file system to the container is it looks like a file on the hard drive to your apps inside the container 
  but its not actually that, they are ot actually running on disk, they are in memory only using  a ramfs  file system an you wull get to them underneath the " /run/secrets " directory where it will by default 
  be the name you gave the secret as a file and then when you just access that file you will see the one secret that's in it. so if you think of this as like a key value store, the key is the name of the 
  file and the value is whats in it.
  we can also set up aliases so we can have multiple names for the same key and we will see how that comes into play later 

  /run/secrets/<secret_name>
  /run/secrets/<secret_alias>


for local development if you are using a stack file that has secret assignments in it it will actually work in docker compose on your local machine.
now again docker-compose the command line should never be used in production on a production server.
in this case in particular it's actually faking security.....so whats happening is we really want the docker developer or the docker user on their machine to be able to use the  
stack files as much as possible....so the containers that you run on your local machine will actually see the secrets just like they would in swarm  but we dont have swarm on our local machine unless 
we initialize it there,  which most people aren't going to do. 
and the way we store the secrets is in the swarm database, so this may be kind of obvious to you but secrets depends on swarm, its a swarm only thing...if you dont have swarm you can't use secrets.
However docker-compose command has a workaround where it actually mounts the secrets in a clear text file into the local container, now that's not secure but it does allow us to use secrets locallly 
on our machine.....it's just not something that you would want to use in production which is why we have the secure store for swarm.

19-------------------------------secrets storage for swarm : protecting your environment variables---------------------------------------


20------------------------------------------------------------------using secrets in swarm services---------------------------------------

    allright so now that we have gone through secrets and the features and how it works and all that lets actually look at some practicals, 
    so what i have here is a 3 node swarm set up like before and i am in a secrets sample 1 directory  that you see in the repo and all ive got in there is one text file 
    and that text file is just a simple username not even actually a password.
    there is two ways we can actually create a secret inside of swarm ......and one of them is to give it a file and another one is to pass a value at the command line.
    so lets use the file first that we have in our desktop 
    if we were in linux we create the three docker machines we made them managers from the node1 then we go to the node1 and work from there and for the others
    now we are in windows we can make the docker machines we make them managers and load the files from git hub as each doecker machine is a linux operating system but we prefer to work 
    simply from our desktop.....so we just simple create the file  with the text and use it from its location via the docker command below that names it during creation that say.....
    but first we make the swarm init and then we proceed to the the command....
    docker secret xfile texty.txt 
    which spits back the id like it does for other objects and lets create another one, lets actually 
    that puts the password of them, where this time we will echo it from the command line e.g. 
    echo "123456789" | docker secret create one - 


       C:\Users\michael.kourbelis\Desktop\composeea>docker swarm init --advertise-addr 192.168.99.126
         Swarm initialized: current node (lygyx4jabzuwm5ro9fvjzgaoa) is now a manager.

         To add a worker to this swarm, run the following command:

            docker swarm join --token SWMTKN-1-351ls320nc2o9jgzd2egy31lzdtigu5a97h25swppchci4ajad-0k35rx3yk7sh62ums4rsg595a 192.168.99.126:2377

         To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.


            C:\Users\michael.kourbelis\Desktop\composeea>docker secret create xfile texty.txt
            rjjno7ait8n4hrnvbpezhacd4

            C:\Users\michael.kourbelis\Desktop\composeea>echo "123456789" | docker secret create one -
            qex6290nxm2g1lt685xo9evzy
   
   if you see what i am doing i am actually typing out a password in their command line with echo and echoing it in into the creation command  
   and notice the dash on the end because that's telling the command to read from the standard input, which is what we giving it with the echo command,
   now i should say that both of these have drawbacks,
   the first one you're actually storing the password on the hard drive of the server on the host  and we really wouldn't want to do that 
   so maybe you would be using the remote API from that local command line on your machine and then pass in the files that way
   and in the second one its actually going into the history of our bash file for our root user so then technically if someone were able to get into root they could actually get this password out.
   when you are dealing with your own production systems you will need to look at various ways to get arounf these two potential security concerns.


   so what we have here is now if i do a docker secret ls you can see that i have both of them in there


      C:\Users\michael.kourbelis\Desktop\composeea>docker secret ls
      ID                          NAME      DRIVER    CREATED             UPDATED
      qex6290nxm2g1lt685xo9evzy   one                 39 minutes ago      39 minutes ago
      rjjno7ait8n4hrnvbpezhacd4   xfile               About an hour ago   About an hour ago


   now i can actually inspect them but i am not going to ever see the password or the actual secret as its not going to   give us the information right,because if it was this easy to get the information it 
   wouldn't be a secret right? 


    C:\Users\michael.kourbelis\Desktop\composeea>docker secret inspect one
[
    {
        "ID": "qex6290nxm2g1lt685xo9evzy",
        "Version": {
            "Index": 12
        },
        "CreatedAt": "2022-06-08T11:21:44.046219508Z",
        "UpdatedAt": "2022-06-08T11:21:44.046219508Z",
        "Spec": {
            "Name": "one",
            "Labels": {}
        }
    }
]

C:\Users\michael.kourbelis\Desktop\composeea>docker secret inspect xfile
[
    {
        "ID": "rjjno7ait8n4hrnvbpezhacd4",
        "Version": {
            "Index": 11
        },
        "CreatedAt": "2022-06-08T10:52:05.318012395Z",
        "UpdatedAt": "2022-06-08T10:52:05.318012395Z",
        "Spec": {
            "Name": "xfile",
            "Labels": {}
        }
    }
]

C:\Users\michael.kourbelis\Desktop\composeea>


so the goal here is that once you put the secret in the system uts stored in the database and the only   thing that's going to have access to the decrypted secrets are going to
e the containers and services we assign to....so lets do that now, i am going to create a service manually


docker service create --name my_webb --secret one --secret xfile 
-e POSTGRES_PASSWORD_FILE=C:/Users/michael.kourbelis/Desktop/secrets/texty 
-e POSTGRES_USER_FILE=C:/Users/michael.kourbelis/Desktop/secrets/textyy nginx

i call the service "my_web" and we are going to map the secret to it....basically we are telling this create command take this secret called  
one or xfile (i can also use the id of the secret and assign it to this service)
so that all containers in this service will see the secret one for the user and one for the password 
ok this maps the secrets to the service so that they show up as files inside the container   but it does not tell nginx that we are creating from this image or how to use those secrets.
so usually we need to do something with the configuration of the image and in this case the official images from docker hub have settled on a standard where you use environment variables, 
but instead of passing in maybe something like postgres_passwords that would be hard to use with a file  
we have to actually like cat out the file into the environment variable (-e POSTGRES_PASSWORD_FILE=C:/Users/michael.kourbelis/Desktop/secrets/texty) 

and that's a little bit of a pain so they have the standard where if you specify a file and this would be the 
path (C:/Users/michael.kourbelis/Desktop/secrets/texty) so if i do that, that's actually in the startup of the image that will look for this environmet variable being filled out
and if it is it will them pull that file (texty.txt) in and store it in the environment variable.....i.e the actual contents of that file.
this is a really easy way to consume these secrets that are in the files, but it does mean  that the images you use need to to have this standard in place and the same goes for the user 
(-e POSTGRES_USER_FILE)


hopefully this makes sense to you, when i create the service its going to do the typical thing it does when it issues a scheduling request for a new container, its going to create
one postgres database   and its going to pass it  the environment variables for the locations of those two secrets and then its going to map in a tempfs, its actually going to map in what look like files 
but again it is really a RAM file system or a tmpfs.....so lets see what we can see in there 


C:\Users\michael.kourbelis\Desktop\composeea> 
docker service create --name my_webb --secret one --secret xfile -e POSTGRES_PASSWORD_FILE=C:/Users/michael.kourbelis/Desktop/secrets/texty -e POSTGRES_USER_FILE=C:/Users/michael.kourbelis/Desktop/secrets/textyy nginx
zn3mqzvanqb1uziv0fjzfalyn
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged



in order to use the above command you must create the secrets first


            C:\Users\michael.kourbelis\Desktop\composeea>docker secret create xfile texty.txt
            rjjno7ait8n4hrnvbpezhacd4

            C:\Users\michael.kourbelis\Desktop\composeea>echo "123456789" | docker secret create one -
            qex6290nxm2g1lt685xo9evzy
-----------------------------------------------------------------important------------------------------------------------------------------------------

so now that we have the my_webb service lets go to see what we have to see through the command ....docker service ps my_webb.... so as to see in what node is running on 

C:\Users\michael.kourbelis\Desktop\composeea\sumup>docker service ps my_webb
ID             NAME        IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
r6zk3e5unj6e   my_webb.1   nginx:latest   default   Running         Running 23 minutes ago


now lets do an exec in that node to see what exists inside this container for that service  the name of which comes out from the command below 

C:\Users\michael.kourbelis\Desktop\composeea\sumup> docker container ls
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS     NAMES
a5c36de34436   nginx:latest   "/docker-entrypoint.…"   28 minutes ago   Up 28 minutes   80/tcp    my_webb.1.r6zk3e5unj6evrs85x75k0qhx

C:\Users\michael.kourbelis\Desktop\composeea\sumup>  docker exec -it  my_webb.1.r6zk3e5unj6evrs85x75k0qhx bash 




and now we are able to do an ls on secrets but again reember that you are on a different os now and your secrets will not be there and 
 must be imported with a volume on that container, so back on your windows machine and to your work 
 we can actually see the logs of this  ....my_webb.1... via the 

 docker logs my_webb.1.r6zk3e5unj6evrs85x75k0qhx



 C:\Users\michael.kourbelis\Desktop\composeea\sumup> docker logs my_webb.1.r6zk3e5unj6evrs85x75k0qhx
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/06/09 11:29:07 [notice] 1#1: using the "epoll" event method
2022/06/09 11:29:07 [notice] 1#1: nginx/1.21.6
2022/06/09 11:29:07 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6)
2022/06/09 11:29:07 [notice] 1#1: OS: Linux 4.19.130-boot2docker
2022/06/09 11:29:07 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/06/09 11:29:07 [notice] 1#1: start worker processes
2022/06/09 11:29:07 [notice] 1#1: start worker process 31




and now we know that it actually works because if it didn't it definately it didn't have those files and the actual database would keep recraeting itself and not be able to work 
because it didn't have a password and a user name to create the database 
so if you do a docker service ps again what you would see if it wasn't working correctly in a database scenario is the database would actually be failing, 
and it would keep restaring it and you would see new containers being created here.


docker service ps my_webb



C:\Users\michael.kourbelis\Desktop\composeea\sumup>docker service ps my_webb
ID             NAME        IMAGE          NODE      DESIRED STATE   CURRENT STATE               ERROR     PORTS
r6zk3e5unj6e   my_webb.1   nginx:latest   default   Running         Running about an hour ago



so now that we've actually assigned it to there we can actually use docker service update to remove the secret.......docker service update --secret-rm
there is also secret-add but if i removed one of these secrets what would actualy happen is it will redeploy the container because secrets are a part of the immutable design of services 
if anything in the container has to change for the service the service will not go in and change something inside the container.
it will actually stop the container and redeploy a new one 

obviously that's not ideal for databases, so we are going to have to come up with dfferent plan for how we would update database passwords and that's somethiong we can talk about later 

for now you must knopw that you can remove such containers and add adittional ones to an existing service it's just going to recreate the container when you do it. 

20------------------------------------------------------------------using secrets in swarm services---------------------------------------

21------------------------------------------------------------------using secrets with swarm stacks---------------------------------------

   now that we have seen secrets with services lets look at secrets with stacks, in this directory
   i am in secrets sample two and in this directory i have a compose file and then two secrets stored in text files in the directory sumup
   so its the similar password and user names that we had in the previous lecture but now we have defined all this in a compose file.
   we have several things different here i.e. in compose.yml the version must be 3.1 

   in order to have secrtes  we have to be at the .1 release of the 3rd version, we need 3 in order to have stacks with secrets we need it to be 3.1 or higher.
   the second thing we will notice is that down here at the bottom  we have this root secrets key now 

   secrets:
  texty:
      file: ./texty.txt
  textyy:
       file: ./textyy.txt

which is the point that we define our secrets, now the two ways that you can do secrets in a compose file are either using a file for each secret or have the secrets pre-created.
and below the keyword secrets we are just using files but what we could do is cretae those secrets on our own in some other method either through the CLI like you've seen or maybe through the API directly.
then we would just instead of the file underneath it, it would just say external  : and then it would be the name of the secret inside the secrets list 

we need to tell the compose file about our secrets and where they are and then we actually assign them to the services that need them and that's key because what we are really saying here is that only 
the container that wants our secret
will gets our secret
and if this was a complicated compose file where we had multiple services we may have different secrets for different services 
then 
we would first define all of them at the bottom and then we would assign them specifically at each service.
now i would say that this is actually considered the short form or the easy way to do the secrets under a service......there is actually a long form that allows you to define things like the permissions 
and the users that are allowed to access that using standard linux mode and user ID syntaxes.

so if you running applications as a non -root user and you want to target these secrets to only that user being able to access them, 
but for simplicity's sake in this first example we are just using the short form......

and all i need to do is to use that file in a standard stack deploy command, so docker stack deploy -c compose.yml mymaan  


C:\Users\michael.kourbelis\Desktop\new>docker stack deploy -c compose.yml mymaan
Creating network mymaan_default
Creating secret mymaan_texty
Creating secret mymaan_textyy
Creating service mymaan_psql






as you see it actually created the network first then the sedcrets and then the service, now if i go and did a docker secret look up (docker secret ls) then you will see that the 
two are in there and it follows the same name convention as all other stack components, where it's always the stack name and then the name of the object.


C:\Users\michael.kourbelis\Desktop\new>docker secret ls
ID                          NAME            DRIVER    CREATED          UPDATED
vcl8029t7562to6vaz3bd15h9   mymaan_texty              15 minutes ago   15 minutes ago
hbbzig9tq5sni1uepgo1j53rf   mymaan_textyy             15 minutes ago   15 minutes ago
npgbux3jawojy9s1s1k9dv1c3   one                       3 hours ago      3 hours ago
z5rkksh417f7op1u24ea40q8p   xfile                     3 hours ago      3 hours ago



just like in the previous lecture if i jumbed on the sql server and actually looked for those files they would be there just like they were before; only now they're managed inside our stack file.
The nice thing here is if i removed my stack it also cleans up the secrets and gets rid of them.
in the previous example if we wanted to remove our secrets, we would have had to do a docker secret rm (docker secret rm mymann) to actually remove each one.

and a last reminder in these examples we have been using text files on this server and that we are talking about 3-node swarm here, but if you are in a production environment or really anything that's not on 
your local machine you should not be keepin the secrets in files or in the bash history file or any place that could possibly be on that host.
thats kind of defeating the p[urppose of having the secrets in the first place. 
whatever your process is for getting secrets into the swarm.......just know that you may need cleanup once you're done there, so that you don't leave residual secrets around that are easy for people to get   


21------------------------------------------------------------------using secrets with swarm stacks---------------------------------------
                                              swarm basic features and how to use them in your workflow
                                                                            section 4
------------------------------------------------------------------------------------------------------------------------------------------------------------------------






                                                                           section 5
                                                                      swarm app lifecycle
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
22--------------------------------------------------------using secrets with local Docker compose-----------------------------------------------------------------------

what about secrets for local development using the docker compose command line, we've been talking about swarm up until now, but i am back on my machine and 
i have docker compose install which i did not have in my swarm....because again  compose is not a production tool  its designed for development.

I am in the same secret sample directory we had before, you can see that i have the two password files and the docker compose file that we had in the swarm.
just to prove that i'm not in a swarm i can do a docker node ls and the message 
"error response from daemon: this node is not a swarm manager. Use docker swarm init or docker swarm join" to connect this node to swarm and try again.

clearly tells us that this is not a swarm manager i am not in a swarm so i don't have access to the swarm database or the ability to put secrets in it.

so how we deal with this in local development ?
well ideally we can still use the same compose file we can use the same objects like the environment variables for postgres and docker had to come up with a way to make this work in test and dev, 
if we do.....docker-compose up -d 


C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty

docker service create --name my_webb --secret one --secret xfile -e POSTGRES_PASSWORD_FILE= C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty -e POSTGRES_USER_FILE= C:/Users/michael.kourbelis/Desktop/composeea/sumup/textyy nginx



C:\Users\michael.kourbelis\Desktop\composeea\sumup>docker-compose up -d
time="2022-06-10T13:16:49+03:00" level=warning msg="Found orphan containers ([sumup-vote-1 sumup-vote-2 sumup-result-1 sumup-redis-2 sumup-redis-1 sumup-visualizer-1 sumup-worker-1 sumup-db-1]) for this project. 
If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up."

C:\Users\michael.kourbelis\Desktop\composeea\sumup>docker-compose up --remove-orphans
[+] Running 8/8
 - Container sumup-db-1          Removed                                                                                                                                                                                                0.1s
 - Container sumup-redis-2       Removed                                                                                                                                                                                                0.5s
 - Container sumup-visualizer-1  Removed                                                                                                                                                                                                0.5s
 - Container sumup-worker-1      Removed                                                                                                                                                                                                0.1s
 - Container sumup-redis-1       Removed                                                                                                                                                                                                0.5s
 - Container sumup-vote-2        Removed                                                                                                                                                                                                0.6s
 - Container sumup-result-1      Removed                                                                                                                                                                                               13.5s
 - Container sumup-vote-1        Removed                                                                                                                                                                                                0.1s
 - Container sumup-psql-1        Creating                                                                                                                                                                                               0.0s
Error response from daemon: invalid mount config for type "bind": invalid mount path: 'C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty.txt' mount path must be absolute

C:\Users\michael.kourbelis\Desktop\composeea\sumup>



C:\Users\michael.kourbelis\Desktop\composeea\sumup>docker secret create xfile texty.txt
1a3hfkxpz1gbie41kcqw00cv9

C:\Users\michael.kourbelis\Desktop\composeea\sumup>echo "123456789" | docker secret create one -
oxganhogtznund9xsmxvbzt5q

C:\Users\michael.kourbelis\Desktop\composeea\sumup>



C:\Users\michael.kourbelis\Desktop\composeea\sumup>docker-compose up -d
[+] Running 0/0
 - Container sumup-psql-1  Creating                                                                                                                                                                                                     0.0s
Error response from daemon: invalid mount config for type "bind": invalid mount path: 'C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty.txt' mount path 
must be absolute


invalid mount path: 'C:/Users/michael.kourbelis/Desktop/composeea/texty.txt' mount path must be absolute

C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty

docker service create --name my_webb --secret one --secret xfile -e POSTGRES_PASSWORD_FILE= C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty -e POSTGRES_USER_FILE= C:/Users/michael.kourbelis/Desktop/composeea/sumup/textyy nginx

Error response from daemon: rpc error: code = InvalidArgument desc = ContainerSpec: "C:/Users/michael.kourbelis/Desktop/composeea/sumup/texty" is not a valid repository/tag


docker service create --name my_webb --secret one --secret xfile -e POSTGRES_PASSWORD_FILE=C:/Users/michael.kourbelis/Desktop/secrets/texty -e POSTGRES_USER_FILE=C:/Users/michael.kourbelis/Desktop/secrets/textyy nginx
ez3lt44w4hf7i07xrydino2x5
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

C:\Users\michael.kourbelis\Desktop\composeea>







C:\Users\michael.kourbelis\Desktop\composeea>docker stack deploy -c compose.yml mydb
Creating network mydb_default
open C:\Users\michael.kourbelis\Desktop\composeea\textyy.txt: The system cannot find the file specified.

C:\Users\michael.kourbelis\Desktop\composeea>docker stack deploy -c compose.yml mydb
Creating secret mydb_texty
Creating secret mydb_textyy
Creating service mydb_psql

C:\Users\michael.kourbelis\Desktop\composeea>docker secret ls
ID                          NAME          DRIVER    CREATED          UPDATED
tnoaw6lu7epwqhspe5csbgvry   mydb_texty              20 seconds ago   20 seconds ago
qauyb16n4e4fj7iw0gxkw6sq9   mydb_textyy             20 seconds ago   20 seconds ago



docker stack deploy -c docker-compose.yml mymaan
------------------------------------------------------------------------------------------

well i put it to linux and played really simple.........oooo and i renamed the compose.yml to docker-compose.yml


-----------------------------------------------------------------------------------------------
version: "3.1"
services:
   psql:
     image: postgres
     secrets:
       - texty
       - textyy
     environment:
       POSTGRES_PASSWORD_FILE:  /home/mike/Desktop/composeea/texty
       POSTGRES_USER_FILE: /home/mike/Desktop/composeea/textyy
secrets:
  texty:
      file: ./texty.txt
  textyy:
      file: ./textyy.txt
-----------------------------------------------------------------------------------------------






mike@mike-VirtualBox:~/Desktop/composeea$ 

sudo docker swarm init
Swarm initialized: current node (a3vcsgq2ksgl2afhyoyezibhz) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-2yx42x5gzuhkrejy3hzqmzeeo2ncyzp28o0jkt2ag5d6zz8r3f-411lwgie9baaqkiwqaam1bj25 10.0.2.15:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.



mike@mike-VirtualBox:~/Desktop/composeea$ 

sudo docker stack deploy -c docker-compose.yml mymaan

Creating network mymaan_default
Creating secret mymaan_one
Creating secret mymaan_xfile
service psql: undefined secret "texty"
mike@mike-VirtualBox:~/Desktop/composeea$ 
docker secret ls

Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/secrets": dial unix /var/run/docker.sock: connect: permission denied
mike@mike-VirtualBox:~/Desktop/composeea$ 

sudo docker secret ls

ID                          NAME           DRIVER    CREATED          UPDATED
5vf16k1imwvscwva0bou7pmgi   mymaan_one               46 seconds ago   46 seconds ago
npao68kamj56naquio6rx6hxl   mymaan_xfile             46 seconds ago   46 seconds ago
mike@mike-VirtualBox:~/Desktop/composeea$ docker-compose up -d
ERROR: 
        Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: docker-compose.yml, docker-compose.yaml



mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker-compose up -d
ERROR: Service "psql" uses an undefined secret "texty" 


here this is because i have name the 

  secrets:
       - texty
       - textyy

and then at the end i had 

secrets:
  xfile:
      file: ./texty.txt
  textyy:
      one: ./textyy.txt

so i change the xfile with texty and the one with textyy


mike@mike-VirtualBox:~/Desktop/composeea$ 

sudo docker-compose up -d
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Creating network "composeea_default" with the default driver
Pulling psql (postgres:latest)...
latest: Pulling from library/postgres
42c077c10790: Pull complete
3c2843bc3122: Pull complete
12e1d6a2dd60: Pull complete
9ae1101c4068: Pull complete
fb05d2fd4701: Pull complete
9785a964a677: Pull complete
16fc798b0e72: Pull complete
f1a0bfa2327a: Pull complete
fd2d68720749: Pull complete
83b23beac012: Pull complete
7962517582d4: Pull complete
6b4a569b8013: Pull complete
ad029fbc8984: Pull complete
Digest: sha256:2d1e636f07781d4799b3f2edbff78a0a5494f24c4512cb56a83ebfd0e04ec074
Status: Downloaded newer image for postgres:latest
Creating composeea_psql_1 ... 
Creating composeea_psql_1 ... done
mike@mike-VirtualBox:~/Desktop/composeea$ A



now we are going to use the command....sudo docker-compose exec psql cat /home/mike/Desktop/composeea/texty....just to see this 

ERROR: No container found for psql_1

sudo docker service create --name psql --secret one --secret xfile -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/newguy/texty postgres

secret not found: one




C:\Users\michael.kourbelis\Desktop\composeea\sumup> docker secret create xfile texty.txt
1a3hfkxpz1gbie41kcqw00cv9

C:\Users\michael.kourbelis\Desktop\composeea\sumup> echo "123456789" | docker secret create one -
oxganhogtznund9xsmxvbzt5q

C:\Users\michael.kourbelis\Desktop\composeea\sumup>

mike@mike-VirtualBox:~/Desktop/composeea$  docker secret create xfile texty.txt

Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/secrets/create": dial unix /var/run/docker.sock: connect: permission denied

mike@mike-VirtualBox:~/Desktop/composeea$ sudo  docker secret create xfile texty.txt
\0kyqte933xh9x0acjb2e6kvor

mike@mike-VirtualBox:~/Desktop/composeea$ sudo  echo "123456789" | docker secret create one -
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post "http://%2Fvar%2Frun%2Fdocker.sock/v1.24/secrets/create": dial unix /var/run/docker.sock: connect: permission denied

mike@mike-VirtualBox:~/Desktop/composeea$ echo "123456789" | sudo  docker secret create one -
p1wvpiptmuszhywuvblf9gib4
mike@mike-VirtualBox:~/Desktop/composeea$ 
sudo docker stack deploy -c docker-compose.yml psql
sudo docker secret create xfile texty.txt
echo "123456789" | sudo docker secret create one -
sudo docker run -d --name nginx  -p 8080:80   nginx:alpine
sudo docker service create --name psql --secret texty --secret textyy -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/composeea/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/composeea/textyy nginx:alpine
sudo docker exec -ti pssql bash



---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

sudo docker secret create texty texty.txt
sudo docker secret create textyy textyy.txt

sudo docker service create --name psql --secret texty --secret textyy -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/composeea/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/composeea/textyy nginx:alpine

sudo docker service ps psql


ID             NAME      IMAGE          NODE              DESIRED STATE   CURRENT STATE            ERROR     PORTS
deiqcmi6niq5   psql.1    nginx:alpine   mike-VirtualBox   Running         Running 54 seconds ago 

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker container ls
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS                                   NAMES
564c3a5c6c12   nginx:alpine   "/docker-entrypoint.…"   2 minutes ago    Up 2 minutes    80/tcp                                  psql.1.deiqcmi6niq5qc8uixusq915n
2aa194b91ae6   nginx:alpine   "/docker-entrypoint.…"   41 minutes ago   Up 41 minutes   0.0.0.0:8080->80/tcp, :::8080->80/tcp   pssql
mike@mike-VirtualBox:~/Desktop/composeea$ 


sudo docker exec -it  psql.1.deiqcmi6niq5qc8uixusq915n bash
OCI runtime exec failed: exec failed: unable to start container process: exec: "bash": executable file not found in $PATH: unknown

sudo docker exec -it  psql.1.deiqcmi6niq5qc8uixusq915n sh

mike@mike-VirtualBox:~/Desktop/composeea$ 
sudo docker exec -it  psql.1.deiqcmi6niq5qc8uixusq915n /bin/bash
OCI runtime exec failed: exec failed: unable to start container process: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown

mike@mike-VirtualBox:~/Desktop/composeea$ 
sudo docker exec -it  psql.1.deiqcmi6niq5qc8uixusq915n //bin// sh

OCI runtime exec failed: exec failed: unable to start container process: exec: "//bin//": permission denied: unknown
mike@mike-VirtualBox:~/Desktop/composeea$ 
sudo docker exec -it  psql.1.deiqcmi6niq5qc8uixusq915n sh
/ # ls
bin                   docker-entrypoint.sh  lib                   opt                   run                   sys                   var
dev                   etc                   media                 proc                  sbin                  tmp
docker-entrypoint.d   home                  mnt                   root                  srv                   usr
/ # cd run
/run # ls
nginx.pid  secrets
/run # cd secrets
/run/secrets # ls
texty   textyy
/run/secrets # cat texty


mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker logs psql.1.deiqcmi6niq5qc8uixusq915n
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/06/14 12:54:00 [notice] 1#1: using the "epoll" event method
2022/06/14 12:54:00 [notice] 1#1: nginx/1.21.6
2022/06/14 12:54:00 [notice] 1#1: built by gcc 10.3.1 20211027 (Alpine 10.3.1_git20211027) 
2022/06/14 12:54:00 [notice] 1#1: OS: Linux 4.15.0-184-generic
2022/06/14 12:54:00 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/06/14 12:54:00 [notice] 1#1: start worker processes
2022/06/14 12:54:00 [notice] 1#1: start worker process 31
mike@mike-VirtualBox:~/Desktop/composeea$ 




mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker service ps psql
ID             NAME      IMAGE          NODE              DESIRED STATE   CURRENT STATE            ERROR     PORTS
deiqcmi6niq5   psql.1    nginx:alpine   mike-VirtualBox   Running         Running 20 minutes ago             
mike@mike-VirtualBox:~/Desktop/composeea$




alright now that we have seen secretes with services we are going to see secrets with stacks so in this directory " /home/mike/Desktop/composeea/ "
we have a compose file and then two secrets stored in text files 
and all of these are defined in a compose file......and in order to use that file in a standard stack deploy command.....

sudo docker stack deploy -c docker-compose.yml mydb
Creating network mydb_default
Creating secret mydb_texty
Creating secret mydb_textyy
Creating service mydb_psql



now if i do......sudo docker secret ls i will see the two secrets in there and the follow the same name convention 

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker secret ls
ID                          NAME          DRIVER    CREATED          UPDATED
yb2fgannnhqzp1bl4rn24p9w1   mydb_texty              45 seconds ago   45 seconds ago
mgpp3gx3go8lcy813zppver6m   mydb_textyy             45 seconds ago   45 seconds ago
x3zmljnr242dtiv8qqt380vsh   texty                   31 minutes ago   31 minutes ago
blhxu0b8viw9gs12rv6labqjw   textyy                  31 minutes ago   31 minutes ago


compose is not a production tool is designed for development 

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker node ls
Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.

this tells us that we are not in swarm manager i am not in a swarm so i dont have access to the swarm database or the ability to put secrets in it....so how we deal wit this in local development
well ideally we can use the same compose file we can still use the same objects like the environment variables like we did in postgress.....and docke have to come up 
with a way to make this work in test and dev and this is achieved by doing " docker-compose up -d "

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker-compose up -d
Creating network "composeea_default" with the default driver
Creating composeea_psql_1 ... 
Creating composeea_psql_1 ... done
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker container ls
CONTAINER ID   IMAGE          COMMAND                  CREATED             STATUS             PORTS                                   NAMES
7a809677a5d4   nginx:alpine   "/docker-entrypoint.…"   5 seconds ago       Up 3 seconds       80/tcp                                  composeea_psql_1
2aa194b91ae6   nginx:alpine   "/docker-entrypoint.…"   About an hour ago   Up About an hour   0.0.0.0:8080->80/tcp, :::8080->80/tcp   pssql
mike@mike-VirtualBox:~/Desktop/composeea$ 

and now we do " sudo docker-compose exec  composeea_psql_1 cat /home/mike/Desktop/composeea/texty"

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker-compose exec  composeea_psql_1 cat /home/mike/Desktop/composeea/texty
[sudo] password for mike: 
ERROR: No such service: composeea_psql_1



add the "   container_name: myappp " to the compose.yml and run again the command,  sudo docker-compose up -d

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker container ls
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS                                   NAMES
5b3d1ccb51e3   nginx:alpine   "/docker-entrypoint.…"   15 seconds ago   Up 13 seconds   80/tcp                                  myappp
2aa194b91ae6   nginx:alpine   "/docker-entrypoint.…"   2 hours ago      Up 2 hours      0.0.0.0:8080->80/tcp, :::8080->80/tcp   pssql
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker-compose exec  myappp  cat /run/secrets/texty
ERROR: No such service: myappp



mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker service create --name myappp --secret texty --secret textyy -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/composeea/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/composeea/textyy nginx:alpine
Error response from daemon: This node is not a swarm manager. Use "docker swarm init" or "docker swarm join" to connect this node to swarm and try again.
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker swarm init
Swarm initialized: current node (kngt6iqd0x24la714bajjfuuw) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-36fuasjdu8v1ok4a9calvw1o076qhhxkjlsqtitev51zry5djw-8ezoqzzxvo3aylchyawmqx651 10.0.2.15:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker service create --name myappp --secret texty --secret textyy -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/composeea/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/composeea/textyy nginx:alpine
secret not found: texty

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker secret ls
ID        NAME      DRIVER    CREATED   UPDATED
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker secret create texty texty.txt
w01nqq695sp64if1zgc27gbyr
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker secret create textyy textyy.txt
djx3n3qh2xaqm60i9slhn2pv6
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker service create --name myappp --secret texty --secret textyy -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/composeea/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/composeea/textyy nginx:alpine
ce329ganf7rtwjr7b8z9ugpwi
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service converged 

mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker-compose exec  myappp  cat /run/secrets/texty
ERROR: No such service: myappp
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
ce329ganf7rt   myappp    replicated   1/1        nginx:alpine   
mike@mike-VirtualBox:~/Desktop/composeea$ sudo docker-compose exec  ce329ganf7rt  cat /run/secrets/texty
ERROR: No such service: ce329ganf7rt








------------------------------------------------------------------------------------------------------------------------

I think you got the relation of docker and docker-compose wrong:

docker-compose is a wrapper around docker. To do its job docker-compose needs its config: docker-compose.yaml

Spinning your example further:

create docker-compose.yaml:

version: '2'
services:
  web:
    container_name: myapp
    build: .
    command: node app.js
    ports:
      - "9000:3000"

use docker-compose to start the container and run a command in the running container:

docker-compose up
docker-compose exec composeea_psql_1 /bin/bash


docker-compose uses the name of the service - in your case this is web - whereas docker uses the container name - in this case myapp.

So to run /bin/bash through docker, you would use the following:

docker exec -ti myapp /bin/bash


you could remove the container_name from docker-compose.yaml, then the container would be named automatically by docker-compose - 
similar to the service, but prefixed with the name of the docker-compose stack (the foldername where docker-compose.yaml is located).



------------------------------------------------------------------------------------------------------------------------





















i think that the command "sudo docker-compose exec  myappp  cat /run/secrets/texty" is a little bit wrong as all the time displays there is no such service while i have a servicer with that name, 
a possible solution is to give the command 
sudo docker exec -it myappp sh.......and from there go to /run/secrets and display the text inside the secret txt via the container which is deployed from the command " docker compose up -d "

and from the time that we see that we are wondering how did our secret get in there right?
because we don't have the database....
well it turns out that some stuff are performed behind the scenes......that what's actually happening with compose is not secure but it works
it basically bind mounts at runtime that actually file on my hard drive into the container 

so is really just doing a -v with that particular file in the background, again this is totally not secure and it's not supposed to be 
but its a way to get around this problem and allow us to develop witn the same process and the same environment secret information that we would have in production,
only now we can do it locally too......which is great because now that means we can develop using the samje launch scripts and the same way we get the environment variables into our container 
just like we would in swarm and that's what we really want, we want to match our production environment as much as we possibly can locally.
you need the latest version of docker compose to do this.....

it is a believe that it only works in docker compose 11, so i hopw you think that's preety cool because that was a good compromise for them to make in order to lets us use the same secret 
commands. 

now it will be pointed out that this only works with file-based secrets and not with the external that we talkwed about \

so if we look at the compose filw really quick........



version: "3.1"

services:
   psql:
     container_name: myappp
     image: postgres
     secrets:
       - texty
       - textyy
     environment:
       POSTGRES_PASSWORD_FILE: C:/Users/michael.kourbelis/Desktop/composeea/texty
       POSTGRES_USER_FILE: C:/Users/michael.kourbelis/Desktop/composeea/textyy
secrets:
  texty:
      file: ./texty.txt
  textyy:
      file: ./textyy.txt


i would have to use file-based ones for my local development, somaybe if you are using external in your production you just might have to have a different compose file for your development that 
would have the file attribute here " file: ./texty.txt "  and specify sample dummy files in the  same directory or somewhere else you might store them that are just using simple password for development 






sudo docker service create --name myappp --secret texty --secret textyy -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/composeea/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/composeea/textyy nginx:alpine

sudo docker exec -it composeea_psql_1 sh



This is the equivalent of docker exec. With this subcommand you can run arbitrary commands in your services. Commands are by default allocating a TTY, 
so you can use a command such as docker-compose exec web sh to get an interactive prompt.



22--------------------------------------------------------using secrets with local Docker compose-----------------------------------------------------------------------

23--------------------------------------------------------full app lifecycle: dev build and deploy with a single compose design-----------------------------------------------------------------------

in this section we covered a lot about swarm, swarm stacks and secrets are kind of like a trilogy of awesome features that can really make things much easier in production for us.
but i want to show you  what it might be like if you actually took your compose files, organize them in a couple of ways and this is what i basically call living the dream.

It turns out that you can actually use a single file to do a lot of things but sometimes your complexity grows and you are gonna need multiple compose files.....
i want to just run through real quick......you don't actually have to do this yourself if you don't want to, i am just going to show you an example of how these compose files might work together 
to build up your environment as you go.

-local docker-compose up developmen environment 
in this scenario we are going to use docker compose up for our local development environment   

-remote docker-compose up CI environment 
we are going to use a docker compose up config and file for our CI environment to do integration testing 

-remote docker stack deploy production environment
and then in production we are going to use those files for docker stack deploy to actually deploy the production environment with a stack.....



so i am on my local machine and we are going to be using the example of the drupal scenario with a database server and a web frontend 
we have the dockerfile we have used in our previous assignments.....we are rebuilding a custom yet simple drupal config with a template 
and then we are going to have this default compose file (docker-compose.yml).




docker-compose.yml
------------------

version: "3.1"

services:
  drupal:
     image: bretfisher/custom-drupal:latest

postgres:
     image: postgres:9.6


and that we are going to do is called "override", an override is where i have the standard docker compose file and it sets the defaults that are the same across all  
my environments....then i have this override file that by default docker-compose if it's named this exact name docker-compose.override.yml   
it will automatically bring this in whenever i do a docker compose up. 

you will see that in this scenario we are assuming local development because we really want the hand typed commands we are going to type to be easiest locally, 
normally in your CI environment it's all automated so we dont really care if those commands are a little bit longer or we really want locally is the easy docker compose up.
so the really cool thing is docker compose will read this file automatically and it will apply this over top or override any settings in the docker compose.yml file 
and notice on the above yaml file on the drupal settings i don't have the image name    
because its specified on the yaml file above.....now in the override yaml file below i override 
by saying i want to build the image locally using the dockerfile in this current directory (  build: .)
i want to create a port on 8080 for local development and i am setting up some volumes and you will even notice i gave you an example here of a bind mount, where i might be doing a custom 
theme (- ./themes:var/www/html/themes), and i want to mount my theme on my host into the container like we did in previous sections, so that i can change it locally and then see it right away on the server 
and by the way for this example i don't actually know how to develop themes in drupal i am not exactly sure that if i change a file in there it will automatically be reflected, 
i just wanted to show an example of how when you're doing development in web typically this is the way you would do it without having to stop and start the compose every time.

down here and under the postgres we have the environment variable 
( - POSTGRES_PASSWORD_FILE=/run/secrets/psql-pw)
and the secret settings like before we have the defined volumes and you will see at the bottom i actually have the file-based secret
(file: psql-fake-password.txt) because when we are doing local development 
we have to use the file-based secret.


docker-compose.override.yml
--------------------------

version: "3.1"

services:

   drupal:
     build: .
     ports:
       - "8080:80"
     volumes:
       - drupal-modules:/var/www/html/modules
       - drupal-profiles:/var/www/html/profiles
       - drupal-sites:/var/www/html/sites
       - ./themes:var/www/html/themes
    
    postgres:
       environment:
          - POSTGRES_PASSWORD_FILE=/run/secrets/psql-pw
       secrets:
          - psql-pw
       volumes:
          - drupal-data:/var/lib/postgresql/data

volumes:
   drupal-data:
   drupal-modules:
   drupal-profiles:
   drupal-sites:
   drupal-themes:

secrets:
  psql-pw:
     file: psql-fake-password.txt



but things get a little interesting when i look at this prod (docker-compose.prod.yml) or test (docker-compose.test.yml) files and the way this is gonna work is, 
remember that the .override.yml file automatically gets picked up by the docker compose command line......while in prod or test i am going to have to specify them manually.
and so for test we are going to have to use the -f command.

because if you remember from earlier sections the -f is when we do a docker compose that we want to specify a custom file....and that is going to be shown in a minute.
and then in production since we are not going to actually have the docker compose command line on a production server what we are going to do here is we are actually going to use a docker compose
config command and the config command is actually going to do an output by squishing together or combining the output of  multiple config files and tis is really cool.

really quick the test file just has the drupal and the postgres and imagine that if this was your jenkins CI or codeship CI solution where i want it to build the image every time 
i commit my code and i want to call it this and i want it to be on this port for testing purposes. 


docker-compose.test.yml
-----------------------

version: "3.1"

services:

   drupal:
     image: bretfisher/custom-drupal
     build: .
     ports:
       - "80:80"
    
    postgres:
      environment:
         -POSTGRES_PASSWORD_FILE=/run/secrets/psql-pw
      secrets:
         -psql-pw
      volumes:
        - ./sample-data:/var/lib/postgesql/data
secrets:
   psql-pw:
      file: psql-fake-password.txt





And then i am going to use a fake password. but i don't need to define any of the volumes because i am not going to actually try to keep named volume data because again it's just a CI platform 
so as soon as it passes test or fails tests it will get rid of everything.
and then in this scenario you might see that i ve actually go this sample data scenario where maybe in you CI solution you have simply databases sitting there that come from either a custom GIT repository
or maybe they are FTP downloaded, or something happens during the initialization of your CI where it actually downloads a database file.
And instead of us having to create our database every single time we do CI testing we would just mount this directory of sample data into where our postgress data is supposed to be and that way we could
guarantee we had the same sample data every single time we do a CI test.

so i am not going to go into that any further, i just wanted to show that might be how this file for CI would be different...and then in production we have all of our normal production concerns.

we are specifying volumes for our specific data, we are specifying our secret and notice down at the bottom we have the external secret because we are going to have put the secret in already via the command
line like we did in the earlier assignment.

docker-compose.prod.yml
-----------------------

version: "3.1"

services:

   drupal:
     ports:
       - "8080:80"
     volumes:
       - drupal-modules:/var/www/html/modules
       - drupal-profiles:/var/www/html/profiles
       - drupal-sites:/var/www/html/sites
       - drupal-themes:var/www/html/themes
    
    postgres:
       environment:
          - POSTGRES_PASSWORD_FILE=/run/secrets/psql-pw
       secrets:
          - psql-pw
       volumes:
          - drupal-data:/var/lib/postgresql/data

volumes:
   drupal-data:
   drupal-modules:
   drupal-profiles:
   drupal-sites:
   drupal-themes:

secrets:
  psql-pw:
     external: true




the point here is that all three of these configs are different in someway but they all relate to the core config or the base config which just defines the two service3s and their images,
you can see below where it is a reminder  of what it looks like 


 
version: "3.1"

services:

   drupal:
     image: bretfisher/custom-drupal:latest
    
    postgres:
     image: postgres:9.6


 
so lets exit this and go to run some commands....when you look at the directory that contains the above files you will see that you have the base file and then three override files and again 
remember that the override.yml file is the default so if you do docker-compose up what it will actually do here is use the docker-compose.yml first and then it will overlay the "...override.yml " file on top.






mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up
ERROR: The Compose file './docker-compose.yml' is invalid because:
Invalid top-level property "postgres". Valid top-level sections for this Compose file are: services, secrets, version, networks, volumes, and extensions starting with "x-".

You might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g "2.2" or "3.3") and place your service definitions
under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.
For more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/





i want to put a -d in tat command above so we can take a quick look after it started " docker compose up -d "  then we inspect that drupal image but first we must list the images  


----------------------------------
Dockerfile
docker-compose.override.yml
docker-compose.prod.yml
docker-compose.test.yml
docker-compose.yml
psql-fake-password.txt
themes
----------------------------------



well lets erase some things........


mike@mike-VirtualBox:~/Desktop/newguy$ 

sudo docker container ls 
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS     NAMES
4d80f1428005   nginx:latest   "/docker-entrypoint.…"   12 minutes ago   Up 12 minutes   80/tcp    mapas.1.dmcfk4wakyisbdo9gf2bq1i9g
dd051f11a282   nginx:alpine   "/docker-entrypoint.…"   2 hours ago      Up 2 hours      80/tcp    myappp.1.df4ad7lxyk8xss0ztyr4qgxwx
mike@mike-VirtualBox:~/Desktop/newguy$ 


mike@mike-VirtualBox:~/Desktop/newguy$ 

sudo docker rm -f 4d80f1428005 dd051f11a282
4d80f1428005
dd051f11a282
mike@mike-VirtualBox:~/Desktop/newguy$ 

mike@mike-VirtualBox:~/Desktop/newguy$ 

sudo docker service ls
ID             NAME      MODE         REPLICAS   IMAGE             PORTS
zainugmyv609   mapas     replicated   1/1        nginx:latest      
ce329ganf7rt   myappp    replicated   1/1        nginx:alpine      
crp3mdp2hwxa   psql      replicated   0/1        postgres:latest   
e832eok2jwz8   psqll     replicated   1/1        postgres:9.6      
mike@mike-VirtualBox:~/Desktop/newguy$ 

mike@mike-VirtualBox:~/Desktop/newguy$ 

sudo docker service rm zainugmyv609 ce329ganf7rt crp3mdp2hwxa e832eok2jwz8
zainugmyv609
ce329ganf7rt
crp3mdp2hwxa
e832eok2jwz8
mike@mike-VirtualBox:~/Desktop/newguy$


mike@mike-VirtualBox:~/Desktop/newguy$ 

sudo docker-compose down
Removing queue ... done
Removing network newguy_default
mike@mike-VirtualBox:~/Desktop/newguy$ 


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker swarm leave --force
Node left the swarm.
mike@mike-VirtualBox:~/Desktop/newguy$ 


      sudo docker service create --name psqll --secret xfile -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/texty -e POSTGRES_USER_FILE=/home/mike/Desktop/newguy/texty postgres:9.6

      sudo docker secret create xfile psql-fake-password.txt

      sudo docker service ps psql

      sudo docker container ls

      sudo docker exec -it  psql.1.deiqcmi6niq5qc8uixusq915n bash

      sudo docker stack deploy -c docker-compose.yml mydb

      sudo docker secret ls

      sudo docker-compose up -d



docker-compose.yml.....wrong
---------------------------
version: "3.1"
services:
   psql:
     container_name: queue
     image: postgres
   drupal: 
     image: bretfisher/custom-drupal:latest  
postgres:
     image: postgres:9.6


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up
ERROR: The Compose file './docker-compose.yml' is invalid because:
Invalid top-level property "postgres". Valid top-level sections for this Compose file are: services, secrets, version, networks, volumes, and extensions starting with "x-".

You might be seeing this error because you're using the wrong Compose file version. Either specify a supported version (e.g "2.2" or "3.3") and place your service definitions under the `services` key, or omit the `version` key and place your service definitions at the root of the file to use version 1.
For more on the Compose file format versions, see https://docs.docker.com/compose/compose-file/
mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up
Creating network "newguy_default" with the default driver
Pulling drupal (bretfisher/custom-drupal:latest)...
ERROR: pull access denied for bretfisher/custom-drupal, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up





docker-compose.yml.....right
---------------------------
version: "3.1"



services:

   psql:

     container_name: queue

     image: postgres

   drupal: 

     image: drupal:latest  

   postgres:

     image: postgres:9.6


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up
Pulling drupal (drupal:latest)...
latest: Pulling from library/drupal
42c077c10790: Already exists
8934009a9160: Pull complete
5357ac116991: Pull complete
54ae63894b5a: Pull complete
772088206f85: Pull complete
3b81c5474649: Pull complete
c62a528527ae: Pull complete
a8da7928e679: Pull complete
caa0c876b41f: Pull complete
bf79d6223250: Pull complete
7ff0e31f4907: Pull complete
b97ccd5e9d41: Pull complete
dfd678d49771: Pull complete
8d484bba0b8a: Pull complete
5e3b1051578f: Pull complete
e4d2768b1274: Pull complete
6b3a79995de4: Pull complete
638c7ac7a58d: Pull complete
Digest: sha256:99acd8c2f1e7af6b4000751af117f9c624d43f70497f7150bc3c8416c6fd2e32
Status: Downloaded newer image for drupal:latest
Pulling postgres (postgres:9.6)...
9.6: Pulling from library/postgres
Digest: sha256:caddd35b05cdd56c614ab1f674e63be778e0abdf54e71a7507ff3e28d4902698
Status: Downloaded newer image for postgres:9.6
Creating queue ... 
Creating newguy_drupal_1 ... 
Creating newguy_postgres_1 ... 
Creating queue
Creating newguy_drupal_1
Creating newguy_postgres_1 ... done
Attaching to newguy_drupal_1, queue, newguy_postgres_1
drupal_1    | AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.20.0.2. Set the 'ServerName' directive globally to suppress this message
drupal_1    | AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.20.0.2. Set the 'ServerName' directive globally to suppress this message
drupal_1    | [Thu Jun 16 08:54:04.466052 2022] [mpm_prefork:notice] [pid 1] AH00163: Apache/2.4.53 (Debian) PHP/8.0.20 configured -- resuming normal operations
drupal_1    | [Thu Jun 16 08:54:04.477697 2022] [core:notice] [pid 1] AH00094: Command line: 'apache2 -D FOREGROUND'

queue       | Error: Database is uninitialized and superuser password is not specified.
queue       |        You must specify POSTGRES_PASSWORD to a non-empty value for the
queue       |        superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".
queue       | 
queue       |        You may also use "POSTGRES_HOST_AUTH_METHOD=trust" to allow all
queue       |        connections without a password. This is *not* recommended.
queue       | 
queue       |        See PostgreSQL documentation about "trust":
queue       |        https://www.postgresql.org/docs/current/auth-trust.html


postgres_1  | Error: Database is uninitialized and superuser password is not specified.
postgres_1  |        You must specify POSTGRES_PASSWORD to a non-empty value for the
postgres_1  |        superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".
postgres_1  | 
postgres_1  |        You may also use "POSTGRES_HOST_AUTH_METHOD=trust" to allow all
postgres_1  |        connections without a password. This is *not* recommended.
postgres_1  | 
postgres_1  |        See PostgreSQL documentation about "trust":
postgres_1  |        https://www.postgresql.org/docs/current/auth-trust.html

queue exited with code 1
newguy_postgres_1 exited with code 1................................................

and stacks here...and when i try to list the containers it has nothing to show so i think i must
create the service before that but with  sudo docker-compose up -d it creates the container but not the service 
mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up -d

Starting newguy_drupal_1 ... 
Starting queue ... 
Starting newguy_drupal_1
Starting newguy_postgres_1 ... 
Starting queue
Starting newguy_postgres_1
Starting newguy_drupal_1 ... done
Starting newguy_postgres_1 ... done
Starting queue ... done


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker container ls
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS          PORTS     NAMES
c64d0de19182   drupal:latest   "docker-php-entrypoi…"   9 minutes ago   Up 18 seconds   80/tcp    newguy_drupal_1



      sudo docker secret create xfile psqlfakepassword.txt

      sudo docker service create --name mapas --secret xfile -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/psqlfakepassword nginx

      sudo docker service ps psql

      sudo docker container ls

      sudo docker exec -it   mydb_drupal.1.3gb4a90v8w2jp0fx59tttsgch  bash

       sudo docker exec -it   mapas.1.ipec3jk75fd7cpknvut3urw15 bash

      sudo docker stack deploy -c docker-compose.yml mydb

      sudo docker secret ls

      sudo docker-compose up -d





mike@mike-VirtualBox:~/Desktop/newguy$     sudo docker service create --name mapas --secret xfile -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/psql-fake-password nginx
secret not found: xfile
mike@mike-VirtualBox:~/Desktop/newguy$     sudo docker secret create xfile psql-fake-password.txt
pq3rqcwggz2xgwo9crcn77yf9
mike@mike-VirtualBox:~/Desktop/newguy$     sudo docker service create --name mapas --secret xfile -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/psql-fake-password nginx
df4zuc4jczayeekus418ed03i
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service converged 
mike@mike-VirtualBox:~/Desktop/newguy$     sudo docker-compose up
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Starting queue ... 
Starting newguy_drupal_1 ... 
Starting queue
Starting newguy_postgres_1 ... 
Starting newguy_drupal_1
Starting newguy_postgres_1 ... done
Attaching to queue, newguy_drupal_1, newguy_postgres_1
queue       | Error: Database is uninitialized and superuser password is not specified.
queue       |        You must specify POSTGRES_PASSWORD to a non-empty value for the
queue       |        superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".
queue       | 
queue       |        You may also use "POSTGRES_HOST_AUTH_METHOD=trust" to allow all
queue       |        connections without a password. This is *not* recommended.
queue       | 
queue       |        See PostgreSQL documentation about "trust":
queue       |        https://www.postgresql.org/docs/current/auth-trust.html
queue exited with code 1
drupal_1    | AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.21.0.3. Set the 'ServerName' directive globally to suppress this message
drupal_1    | AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.21.0.3. Set the 'ServerName' directive globally to suppress this message
drupal_1    | [Thu Jun 16 09:26:00.956641 2022] [mpm_prefork:notice] [pid 1] AH00163: Apache/2.4.53 (Debian) PHP/8.0.20 configured -- resuming normal operations
drupal_1    | [Thu Jun 16 09:26:00.956795 2022] [core:notice] [pid 1] AH00094: Command line: 'apache2 -D FOREGROUND'
postgres_1  | Error: Database is uninitialized and superuser password is not specified.
postgres_1  |        You must specify POSTGRES_PASSWORD to a non-empty value for the
postgres_1  |        superuser. For example, "-e POSTGRES_PASSWORD=password" on "docker run".
postgres_1  | 
postgres_1  |        You may also use "POSTGRES_HOST_AUTH_METHOD=trust" to allow all
postgres_1  |        connections without a password. This is *not* recommended.
postgres_1  | 
postgres_1  |        See PostgreSQL documentation about "trust":
postgres_1  |        https://www.postgresql.org/docs/current/auth-trust.html
newguy_postgres_1 exited with code 1
^CGracefully stopping... (press Ctrl+C again to force)
Stopping newguy_drupal_1 ... done
mike@mike-VirtualBox:~/Desktop/newguy$    sudo docker-compose up -d
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Starting newguy_drupal_1 ... 
Starting queue ... 
Starting newguy_drupal_1
Starting queue
Starting newguy_postgres_1 ... 
Starting newguy_postgres_1 ... done



mike@mike-VirtualBox:~/Desktop/newguy$     sudo docker container ls
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS          PORTS     NAMES
d71156af793a   nginx:latest    "/docker-entrypoint.…"   2 minutes ago   Up 2 minutes    80/tcp    mapas.1.uas7velkzhroo6s1yjap2166o
9e7c47be5d20   drupal:latest   "docker-php-entrypoi…"   5 minutes ago   Up 18 seconds   80/tcp    newguy_drupal_1.......................we are interested on this container
mike@mike-VirtualBox:~/Desktop/newguy$     sudo docker service  ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
df4zuc4jczay   mapas     replicated   1/1        nginx:latest   



so to sum up we performed some changes to the docker-compose.yml file from this 


version: "3.1"
services:
  drupal:
     image: bretfisher/custom-drupal:latest
postgres:
     image: postgres:9.6


to this 


version: "3.1"
services:
   psql:
     container_name: queue
     image: postgres
   drupal: 
     image: drupal:latest  
   postgres:
     image: postgres:9.6

and now we are ready to combine it with the docker-compose.override.yml that we get it ouit from te folder " /home/mike/Desktop/newguy " as it casued a lot of problems....lets go 


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up -d
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Creating volume "newguy_drupal-modules" with default driver
Creating volume "newguy_drupal-sites" with default driver
Creating volume "newguy_drupal-profiles" with default driver
Creating volume "newguy_drupal-data" with default driver
Creating volume "newguy_drupal-themes" with default driver
Recreating newguy_postgres_1 ... 
Recreating newguy_drupal_1 ... 
Recreating newguy_postgres_1
Starting queue ... 
Recreating newguy_drupal_1
Starting queue
WARNING: Service "postgres" is using volume "/var/lib/postgresql/data" from the previous container. 
Host mapping "newguy_drupal-data" has no effect. Remove the existing containers (with `docker-compose rmRecreating newguy_drupal_1 ... error

ERROR: for newguy_drupal_1  Cannot create container for service drupal: invalid volume specification: 
'/home/mike/Desktop/newguy/themes:var/www/html/themes:rw': invalid mount config for type "bind": invalid mount path: 'var/www/html/themes' mount path must be absolute

ERROR: for drupal  Cannot create container for service drupal: invalid volume specification: 
'/home/mike/Desktop/newguy/themes:var/www/html/themes:rw': invalid mount config for type "bind": invalid mount path: 'var/www/html/themes' mount path must be absolute
ERROR: Encountered errors while bringing up the project.
mike@mike-VirtualBox:~/Desktop/newguy$ 


now we are goingn to delete the service that we created above 




mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up -d
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Starting newguy_postgres_1 ... 
Recreating 9e7c47be5d20_9e7c47be5d20_newguy_drupal_1 ... 
Starting newguy_postgres_1
Starting queue ... 
Recreating 9e7c47be5d20_9e7c47be5d20_newguy_drupal_1
Recreating 9e7c47be5d20_9e7c47be5d20_newguy_drupal_1 ... error

ERROR: for 9e7c47be5d20_9e7c47be5d20_newguy_drupal_1  
Cannot create container for service drupal: 
invalid volume specification: '/home/mike/Desktop/newguy/themes:var/www/html/themes:rw': 
invalid mount conStarting queue ... done

ERROR: for drupal  Cannot create container for service drupal: invalid volume specification: '/home/mike/Desktop/newguy/themes:var/www/html/themes:rw': 
invalid mount config for type "bind": invalid mount path: 'var/www/html/themes' mount path must be absolute
ERROR: Encountered errors while bringing up the project.
mike@mike-VirtualBox:~/Desktop/newguy$ 


so we google the following error.......
Cannot create container for service drupal: invalid volume specification:

and we get an answer that we must change the line " - ./themes:var/www/html/themes " in docker-compose.ovveride.yml the with this " - ./themes/:/var/www/html/themes "

and theeeeeen yes.....................

mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose up -d
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Recreating newguy_postgres_1 ... 
Starting newguy_drupal_1 ... 
Recreating newguy_postgres_1
Recreating newguy_postgres_1 ... done
mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker container ls
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS          PORTS                                   NAMES
ea5a4386ec78   drupal:latest   "docker-php-entrypoi…"   8 minutes ago   Up 22 seconds   0.0.0.0:8080->80/tcp, :::8080->80/tcp   newguy_drupal_1




yes but we use the docker-compose up -d and we didn't make the override.....


all this time is spent my time to something that has no meaning because the acttual command is docker-compose up -d and the container with the secret is created with the 
service command...take a look below and cu tommorow here....



so we are here today to run the docker-compose up  command in order to use the docker-compose.yml first and then to overlay the docker-compose.ovveride.yml on top......
i want to put -d on this command and make it like " docker-compose up -d " with the view to take a quick look after it started.....


      sudo docker secret create xfiles texty.txt

      sudo docker service create --name alien --secret xfiles -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/texty nginx

      sudo docker-compose up -d 




and then we do a docker inspect on the drupal image but before we do docker conatiner ls so as to know where to do that inspect....


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker container ls
CONTAINER ID   IMAGE           COMMAND                  CREATED         STATUS          PORTS                                   NAMES
e503002052e2   nginx:latest    "/docker-entrypoint.…"   2 minutes ago   Up 2 minutes    80/tcp                                  alien.1.fg5umdk1d1zg75jpahhqt6yvs
adfb409e6a4e   newguy_drupal   "httpd-foreground"       19 hours ago    Up 14 seconds   0.0.0.0:8080->80/tcp, :::8080->80/tcp   newguy_drupal_1
mike@mike-VirtualBox:~/Desktop/newguy$ 



so we do docker inspect on newguy_drupal_1........and i wanted to show you that in here it has all the mounts listed so we know that it took the override  file because the override file 
was where we defined all of these mounts....so we know that it picked that upand obviously if it didn't pick up the base one it wouldn't even know what images to use so it would actually 
be complaining to us and saying that the compose file was incomplete.......

"Mounts": [
            {
                "Type": "bind",
                "Source": "/home/mike/Desktop/newguy/texty.txt",
                "Destination": "/run/secrets/texty",
                "Mode": "ro",
                "RW": false,
                "Propagation": "rprivate"
            },
            {
                "Type": "volume",
                "Name": "newguy_drupal-modules",
                "Source": "/var/lib/docker/volumes/newguy_drupal-modules/_data",
                "Destination": "/var/www/html/modules",
                "Driver": "local",
                "Mode": "rw",
                "RW": true,
                "Propagation": ""
            },
            {
                "Type": "volume",
                "Name": "newguy_drupal-profiles",
                "Source": "/var/lib/docker/volumes/newguy_drupal-profiles/_data",
                "Destination": "/var/www/html/profiles",
                "Driver": "local",
                "Mode": "rw",
                "RW": true,
                "Propagation": ""
            },
            {
                "Type": "volume",
                "Name": "newguy_drupal-sites",
                "Source": "/var/lib/docker/volumes/newguy_drupal-sites/_data",
                "Destination": "/var/www/html/sites",
                "Driver": "local",
                "Mode": "rw",
                "RW": true,
                "Propagation": ""
            },
            {
                "Type": "bind",
                "Source": "/home/mike/Desktop/newguy/themes",
                "Destination": "/var/www/html/themes",
                "Mode": "rw",
                "RW": true,
                "Propagation": "rprivate"
            }
        ],

so that one worked..... lets do a down really quick......docker-compose down 


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose down
[sudo] password for mike: 
Stopping newguy_drupal_1 ... done
Removing newguy_postgres_1 ... done
Removing newguy_drupal_1   ... done
Removing network newguy_default
mike@mike-VirtualBox:~/Desktop/newguy$


so if we were going to actually do the command we needed for our CI solution what we would have to do on our ci solution was to make sure that Docker compose was there and installed and available 
so that we could do docker compose commands.....then specify the -f  flag and then the order of the f's is that the base file always needs to be first e.g. 
first we put the docker-compose.yml and then the  docker-compose.test.yml

docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d
WARNING: The Docker Engine you're using is running in swarm mode.

Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node.

To deploy your application across the swarm, use `docker stack deploy`.

Creating network "newguy_default" with the default driver
Creating newguy_drupal_1 ... 
Creating newguy_postgres_1 ... 
Creating newguy_drupal_1
Creating newguy_drupal_1 ... done
mike@mike-VirtualBox:~/Desktop/newguy$ 





and if we did this with an up at the end and a - d and then i went and inspect that same drupal you will notice that there is no bind mounts based on the video  they are completely missing
because in the test file we didn't specify those we didn't actually need drupal to save information because it was going to be thrown away at the end  of our CI run 

but in my case because i am spiderman  i have one  



        "Mounts": [
            {
                "Type": "bind",
                "Source": "/home/mike/Desktop/newguy/texty.txt",
                "Destination": "/run/secrets/texty",
                "Mode": "ro",
                "RW": false,
                "Propagation": "rprivate"
            }
        ],

and then third we have the production config, now the production config is going to be a little but different as instead of the up command i put a config and if you combine it with the --help 
for the config you will see that it has two options and neither one of  those are really relevant in this situation, 


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose -f docker-compose.yml -f docker-compose.prod.yml config --help
[sudo] password for mike: 
Validate and view the Compose file.

Usage: config [options]

Options:
    --resolve-image-digests  Pin image tags to digests.
    -q, --quiet              Only validate the configuration, don't print
                             anything.
    --services               Print the service names, one per line.
    --volumes                Print the volume names, one per line.
mike@mike-VirtualBox:~/Desktop/newguy$


what we want to do is just run config by itself, and what its going to actually do is look at both files and push them together into a single compose file equivalent 




mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose -f docker-compose.yml -f docker-compose.prod.yml config
ERROR: yaml.parser.ParserError: while parsing a block mapping
  in "./docker-compose.prod.yml", line 5, column 4
expected <block end>, but found '<block mapping start>'
  in "./docker-compose.prod.yml", line 15, column 5


here i alligned the postgres key with the drupal 



mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose -f docker-compose.yml -f docker-compose.prod.yml config
ERROR: The Compose file is invalid because:
Service drupal has neither an image nor a build context specified. At least one must be provided.

here i put the " image: drupal " below the drupal key......


mike@mike-VirtualBox:~/Desktop/newguy$ sudo docker-compose -f docker-compose.yml -f docker-compose.prod.yml config
secrets:
  texty:
    external: true
services:
  drupal:
    image: drupal
    ports:
    - 80:80/tcp
    volumes:
    - drupal-modules:/var/www/html/modules:rw
    - drupal-profiles:/var/www/html/profiles:rw
    - drupal-sites:/var/www/html/sites:rw
    - drupal-themes:var/www/html/themes:rw
  postgres:
    environment:
      POSTGRES_PASSWORD_FILE: /home/mike/Desktop/newguy/texty
    image: postgres:9.6
    secrets:
    - source: texty
    volumes:
    - drupal-data:/var/lib/postgresql/data:rw
version: '3.1'
volumes:
  drupal-data: {}
  drupal-modules: {}
  drupal-profiles: {}
  drupal-sites: {}
  drupal-themes: {}

mike@mike-VirtualBox:~/Desktop/newguy$ 


and  so what we could do here is just run this command somewhere in our CI solution, and then have an output to a file maybe with you know
the following command.....

docker-compose -f docker-compose.yml -f docker-compose.prod.yml config > output.yml

that output file would be the one that we would use officially to create in production to create or update our stack.


however i want to throw in a liitle caviar  here the below stuff are relatively new.....

local docker-compose up development environment
remote docker-compose up CI environment
remote docker stack deploy production environment 



we know that secrets and swarm stacks are relatively new, they are a couple of months old as of this recording and so there are couple of rough edges.....
note: docker-compose -f a.yml -f b.yml config......mostly works 

we just ran that config command and you'll actually notice that the secrets weren't listed in there...that'a bug currently i am actually working with the docker team to see if we can squash that bug 
so by the time you read this it may be already been fixed and make sure you inspect that output of the config line before you go to deploying in production 


and secondly the compose extends option (note: compose extends: doesn't work yet in stacks ) which i did not discuss here is another way to override these compose files where you actually use the 
override file and you actually define an extends section in there.....its a little bit more declarative so it's easier to understand  but know that the extends option doesn't yet work in swarm stacks,
so i didn't mention it here because it doesn't really give you the full app lifecycle that we were hoping for, but id do expect them at some point to do something about that...like either add it 
into swarm or create a bettwr workflow because that's really the idea we are tru=ying to get to with all of these tools is to have a complete and easy lifecycle from development all the way through test 
into production with the same set of configurations, in the same set of images.....

so i hope this got you thinking about how you might make your apps this way and how you might extend your own compose files for complex scenarios.....because nothing puts a smile on my face 
faster with docker than when i see someone run a one line command to do something that previously took multiple script to execute 

docker-compose -f docker-compose.yml -f docker-compose.prod.yml config --help

      sudo docker secret create xfiles texty.txt

      sudo docker service create --name alien --secret xfiles -e POSTGRES_PASSWORD_FILE=/home/mike/Desktop/newguy/texty nginx

      sudo docker-compose up -d 

      sudo docker service ps psql

      sudo docker container ls

      sudo docker exec -it   mydb_drupal.1.3gb4a90v8w2jp0fx59tttsgch  bash

      sudo docker exec -it   mapas.1.ipec3jk75fd7cpknvut3urw15 bash

      sudo docker stack deploy -c docker-compose.yml mydb

      sudo docker secret ls

      sudo docker-compose up -d


23--------------------------------------------------------full app lifecycle: dev build and deploy with a single compose design-----------------------------------------------------------------------

24--------------------------------------------------------service updates: changing things in flight--------------------------------------------------------------------------------------------------

service updates you ve probably assumed all along that there some way to update your services even though we haven't been focusing on that yet...but lets talk about it because
updates has a whole lot of stuff going on under the covers.
swarm's update is centered around a rolling update pattern for your replicas which means if you have a service with more than one replica and hopefully you do....
it's going to roll through them by default one at a time updating each container by replacing it with the new settings that you're putting in the update command.
a lot of people will say that orchestrators prevent downtime in updates but i am not going to say that this prevents downtime i am going to say it limits 
downtime.
because preventing downtime in any system is the job pf testing.
you really need to start testing......how you do your updates and determining...
does it impact ny users ?
does updating a database impact my webapplication ?......it probably does

each application that you have a service for is going to update and impact the other things around it differently,
that's not the job of an orchestrator as an orchestrator can't determine that this one is a database protocol and this one is a REST application protocol that's easy 
so those are going to be different complicated things that you need to deal with, so in the case of service updates, 

if it's just a rest api or  a web frontend that's not doing anything 
fancy like web sockets or long polling if its something very simple like that or a static website you will probably have an easy update and it won't be a problem,
but other services like databases or persistent storage or anything that requires a persistent connection, those are always going to be a challenge no matter what you are trying to update 
so test early and test often......

now like i said before this will definately replace containers in most updates, unless you're updating a label or some other metadata with the service, 
it's going to roll through and change out each container with a tottaly new one........so just be prepared for that, 
it has many options, in fact the last i counted there was at least 77 options for the update command, but just about everything you want to do can be tweaked.
so a lot of the options in the update command are actually create options that just have a "-rm" and a "-add" on the end of them.
because if its an option  that can be used for multiple values so lets say a port to publish or an environment variable, those you can use many of them right, so you need to be able to tell the update 
command which ones you are adding and which ones you are removing and we will see those in a minute......
this also included rollback and health check options, so you should look at the options for those and see if their defaulty values aren't ideal for your application and test different settings 
to see if it makes an update easier for you and your system.



....also has scale & rollback subcommand for quicker access
    docker service scale web=4 and docker service rollback web
you also will see that we have scale and rollback options in here that are their own commands now. 
they useed to be options that you had to specify with the --rollback or --scale, but so many people have been using those so frequently that docker is now making them sort of first class citizens 
in the command line.
and they might be adding more of those in the future.

and lastly before we get to some examples if you are doing stacks, a stack deploy to the same stack is  an update, in fact there is no seperate option for stack updates, you just do a stck deploy 
with the same file that's been edited....then it will work with the service commands and the networks and every other thing that it does...it will work with them to make sure if any changes are 
necessary that they get applied....
so lets look at some quick examples and then we will get to the command line.......



swarm update examples 

just update the image used to a newer version
......docker service update --image myapp:1.2.1 <servicename>

 the above example is the most common example that you will be using which is to change the image of a service, every time you update your app and you build a new image 
 you are going to have to do a service update command with the image name and then the service name and so in this case maybe i had my application with a tag of 1.2.0
 and then in this case i am now applying a 1.2.1. image and the service will determine .....a yes that's a different image that i have running in my service and we will go and update them
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 on the next onw we will show how you can do multiple things at once inside a single update command, you can add an environment variable with the env-add and then you can remove a port 
 with the publish-rm 
 we can also be adding and removing environment variables and publish ports in the same update command as much as we want 

adding an environment variable and remove a port
....docker service update --env-add NODE_ENV=production --publish-rm 8080
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
on this last one this is showing how we can use these new scale and rollback commands on multiple services at the same time.
which is one of the advantages of using them over the update command is that they can apply to multiple services, so in this case i am actually going to be changing the number of replicas of the web and the 
API services at the same time 

change number of replicas of two services 
docker service scale web=8 api=6
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

and like i said long time ago in the swarm updates you don't have a different deploy command, its the same docker stack deploy with the file that you have edited and its job is to work with 
all of the other parts of the API to determine if there is any changes needed and then roll those out with a service update 
in other words
it is the same command, just edit the yaml file, then 
docker stack deploy -c file.yml <stackname>


alright lets start by actually creating a service so that we can manipulate it with some update commands 

sudo docker service create -p 8088:80 --name web nginx:1.13.7

..........its nice to have the detach option with our service create and update commands we can actually see this happen synchronously in real time.
so this will be good for update commands to see how updates actually roll out via the command line 

and now if we do 

sudo docker service ls we will see that service running see also that it has 1/1/ replicas....so its good to go 
so now lets scale our service up so we can have some more replicas to work.....with "docker service scale web=5"



mike@mike-VirtualBox:~$ sudo docker service scale web=5
web scaled to 5
overall progress: 5 out of 5 tasks 
1/5: running   [==================================================>] 
2/5: running   [==================================================>] 
3/5: running   [==================================================>] 
4/5: running   [==================================================>] 
5/5: running   [==================================================>]
verify: Service converged 
mike@mike-VirtualBox:~$ 


and you just saw that one of those was already running , and you just saw that one of those was already running and the other 4 had to start.
it is feasible for the image to be DOWNLOADED and on other nodes while its downloading and its services can be on pending states.



okay so now lets do a rolling update by changing the image of that nginx  

sudo docker service update --image nginx:1.13.6 web.......and you see here that i've picked an older image, now
docker doesn't care about what the image actually is, it could be a completely different application for all it cares, 
it just knows that in this service i am changing it to a different image.......


sudo docker service update --image nginx:1.13.6 web 



and remember by default its going to go through here one at a time, it will first remove it, create a new one and then when that's one's good to go and it looks healty...
it will start in the next one, 

in the next example we are going to change the published port, but you can't change the port, you actually have to add and remove them at the same time, 
so in this case because we first published it with an 8088 we need to do a 

docker service update --publish-rm 8088 --publish-add 9090:80 web 


and you see that i don't need to specify the value, i just need to specify the key and then the value which in this case is 80, you will see that 
it will do the same update process as before .........



and the last example i want to talk about is kind of a tip, because you  will often have a challenge with something called rebalancing.
or if you change the number of nodes or if you move a lot of things around.

if you have a lot of containers in your swarm you may find out that they are not really evened out...you ve got some nodes that are pretty light on how many 
containers are running and other ones that have a lot of things changing, swarm will not move things around to keep everything balanced in terms of the number 
of resources used. but what you can do is you can force an update of a service even without changing anything in that service.
then it will reissue tasks and in that case it will pick the least used nodes which is a form of rebalancing, so a lot of times in a smaller swarm when i move 
something big or add a bunch of nodes i suddenly have a bunch of empty servers doing nothing and i need to get work on them, so what i do  is take one or two of  my services 
and i will do a 
sudo docker service update --force web

.......and in this case it's going to roll through and completely replace the tasks, of course it will use the schedules default of looking for nodes 
with the least number of containers and the least number of resources used 
and 
that's kind of a trick to get around an "uneven" amount of work on your nodes  


and remember to cleanup by removing the service that we created in this lecture...........docker service rm web

24--------------------------------------------------------service updates: changing things in flight--------------------------------------------------------------------------------------------------


25--------------------------------------------------------healthchecks in dockerfiles--------------------------------------------------------------------------------------------------

the docker health check command....

supported in dockerfile, compose yaml, docker run and swarm services
----------------------------------------------------------------------
it was a new feature addeed in 1.12 which came out in the mid 2016, the same time that swarm kit and swarm mode were available in docker, it was added really as a part of that toolkit 
but it still works in all different files like the dockerfile the compose file, the docker run command uses it, the stack files support it the service update and the service create command support it 
its everywhere 

docker engine will exec's the command in the container (e.g. curl localhost)
----------------------------------------------------------------------------
i higly recommend that when you are on production you do engage in testing options for this health check command...its going to work right out of the box with an exec, 
its going to execute that command inside the container just like if you were running your own exec command.
so its not running it from outside the container...its just running it inside which means that even simple workers that don't have exposed ports, you can run a simple command 
in them to validate whether they are returning good data or whatever.

it expects exit (0) --> ok or exit 1 -->error
----------------------------------------------
its a simple execution of a command which means it gets a simple return....it expects a 0 or 1 so in linux and windows you have exit codes from commands and 0 is a good thing 
it means everything was fine and anything other than 0 is going to be an error in most applications  but in docker we need that application to exit a 1 specifically...
so we will show in a minute how you do that, 
three container states: starting,healthy,unhealty 

there is only three states to a health check in docker, it starts out with starting.
starting is the first 30 seconds by default where it hasn't run a healthcheck command yet.
then its going to run one and if that returns a zero it will start with the healty, it will change to the healty option.
it will take that command and it will run it every 30 seconds be default again.
if it ever receives it an unhealthy return with an exit 1, then it marks it as an unhealty container.
now we have options for controlling all of this including retries we will see that in a  minute....
this is a much better option than we have had in the past because docker until now was just making sure the application was still running 
it didn't have any insight into whether that application was doing what it was supposed to....
and now we can do that inside the docker container itself.....but this isn't a replacement for your third party monitoring solution, this isn't going to give 
you graphs or status over time or any sort of third party tooling that you would expect out of a monitoring solution, this is about docker understanding if the container 
itself has a basic level of healthy. 
so in an nginx it might return a localhost of the root index file.
a return of 200 or 300 is fine and gives it an exit code of 0 and it considers it healthy...thats not a superadvanced uh you know monitoring tool 
but if it did return 404 or 500 error, it will then consider it unhealty and we can do something about that....

so where we are going to see this docker heqalthcheck in the gui ?
so the first place is in container ls (sudo docker container ls), it will just see it has this new option
its in the middle, we will see in a second where i will show us one of the three states if the health check is running and that's how we actually know that there is a healtcheck 
that's the easiest way at least to know...

healtcheck status shows up in docker container ls,check last 5 healtchecks with docker container inspect 
--------------------------------------------------------------------------------------------------------
we will see the history the last five of that healthcheck  to show up in the inspect for that container and we can see some basic trend over time there.

docker run does nothing with healtchecks
-----------------------------------------

but the docker run command does not take action on an unhealthy container, once the healthcheck considers a container unhealthy docker run is just going to indicate that in the ls command 
and in the inspect, but its not going to take action,

service will replace tasks if they fail healtcheck
---------------------------------------------------
and that's where we expect the swarm services to tak action, so the stack and services will actually replace that container with a new task
or a new host possibly depending on the scheduler, 

service updates wait for them before continuing
------------------------------------------------
and even in the updates command we see a little extra bonus by using the healthchecks because the updates will consider the healtcheck as a part 
of the rediness for that container before it goes and changes the next one....so if a container comes up bit it doesn't pass its health check then the service update won't go to the next one 
or it will take action based on the changes you give it...  


so lets look at a few examples before we go to the command line 
----------------------------------------------------------------------------
docker run \
--health-cmd="curl -f localhost:9200/__cluster/health || false" \ 
--health-interval=5s \
--health-retries=3 \
--health-timeout=2s \
--health-start-period=15s \
elasticsearch:2
----------------------------------------------------------------------------

this one that we are using on docker run and this allows us to use an existing image that doesn't have a health check in it and we are adding the health check in at runtime  and in this case 
we are using the elastic search image you can see the command is a cURL localhost 9200, which is the port that the elasticsearch is running on inside the container not the published port, but inside the container

for elastic search there is an actual health url, so we can use that here, you will notice the two pipes with the false at the end of that command and that's gonna be pretty common if you are using something like 
curl or another tool that will send out an error code that's other than 1, remember when i mentioned that while ago.....we need to exit with 1 if there is a problem because that's the one problem 
because that's the one  error code that docker is going to do something about so we need to make sure that in this case a shell will always return the false 1 exit code....
if there is anything coming out of that command other than 0, so its a nice way to get around that problem and it just so happens with curl, curl will give other potential error codes and we don't want it to 
do that.



----------------------------------------------------
--------------------------------
options for healtcheck command
--------------------------------

--interval=DURATION (default: 30s)
--timeout=DURATION (default: 30s)
--start-period=DURATION (default: 0s)(17.09+)
--retries=N (default: 3)
----------------------------------------------------

and in the actual docker files we can add the same command,  the format is a little bit different but you see that we have these options here, 
we have the interval, the timeout, the start period (which is new), and retries.....
so the interval is what you would think it is, it's by default every 30 seconds, how often it's going to run this health check.
the timeout is how long it's going to wait before it errors out and returns a bad code  if maybe the app is slow.
the start period is a new feature that allows us now in 17.09 and newer to give a longer wait period than the first 30 seconds of the duration before  it will always just wait
the long....the interval time before it started the healtcheck but maybe you have a java app or database or something that takes a lot longer to start, maybe it takes five minutes.
so you could add that start period in there it will still do healtchecks.
but what it will do is it wont alarm on an unhealthy check until that time has elapsed  so if you set two minutes in there even though it's health checking every 30 seconds it's going to only 
consider it unhealthy once it's past that two minute mark.
the last one ther retries means that we will try this health x number of times before we consider it unhealthy, and that gives maybe a potentially unstable app a chance to come back with a healthy and recover on 
its own before we consider this a truly unhealthy container 

----------------------------------------------------
basic command using defaults options 
HEALTCHECK curl -f http://localhost/ || false
----------------------------------------------------

the basic healthcheck command you would use in a dockerfile is called HEALTHCHECK (all capital letters there).
the same format exists where if we're just doing a simple cURL of the localhost because maybe it's a PHP app or something.
we can do that 

----------------------------------------------------------------------------------------------
custom options with the command
HEALTHCHECK --TIMEOUT=2s --interval=3s --retries=3 \ 
CMD curl -f http://localhost/ || exit 1
----------------------------------------------------------------------------------------------

and this is how you add those options in to a dockerfile so you would see how i add the timeout interval and the retries before the command itself, 
the first one ther for the basic command, notice i don't have to put the CMD flag if i am just giving it the command to run.
but if i want to show options if i want to give it custom options out of the box with the timeout and so on, then i have to specify which one is the command.
now these aren't two different lines.
Notice the back slash on the end of the first line there, so don't get that confused 




--------------------------------------------------------------
static website running in nginx just test default url

from nginx:1.13

HEALTHCHECK --interval=30s --timeout=3s \
CMD curl -f http://localhost/ || exit 1
---------------------------------------------------------------

and here we have a simple example of what it might be like if you had a static application running inside an nginx server, you could set the interval
and the time out from your Dockerfile and you would just have it simply do a cURL command on the localhost and if it return a 200 or 300
it considers that fine, if it returns a 4 or 4 ir something else, it considers that an error, 
and you notice here that i have an exit 1 which is the same thing as a false. 
i did that just to show you that ceratin examples on the internet will have a false.
ceratin examples will have an exit 1 they both do the same thing.


-----------------------------------------------------------------------------
healtcheck in php nginx dockerfile
php-fpm running behind nginx, test the nginx and fpm status URLs

From you nginx php fpm combo image

#don't do this if phph-fpm is another container
#must enable php-fpm ping/status in pool.ini
#must forward /ping and /status urls from nginx to php-fpm 

HEALTHCHECK --interval=5s --timeout=3s \
CMD curl -f http://localhost/ping  || exit 1
-----------------------------------------------------------------------------



here is a  little bit more advanced example, in this case we are using a php app that's combined with nginx 
and what i've done is in the resources you'll find a link to this php example, i've added in a custom  nginx config file that uses nginx and php-fpm status URLs 
so bith of these applications have their own status page and sort of a healtcheck ping url and you can use those in your apps if you are using php or nginx.
there are two different URLs but you can use both of them inside the same healtcheck.
in this case we are just using one of them and we are throwing in the localhost/ping which is actually a php-fpm status command, but you have to enable that inside your php-fpm.
Again in the resources of this lecture there is a link to a php docker good defaults (https://github.com/BretFisher/php-docker-good-defaults),
you can go check that out on a github where i've shown in this example in a little bit more detail.


-----------------------------------------------------------------------------
healthcheck in postgres dockerfile

use a postgreSQL utility to test for ready state


FROM postgres

#specify real user with -U to prevent errors in log

HEALTCHECK --interval=5s --timeout=3s \
CMD pg_isready -U postgres || exit 1
-----------------------------------------------------------------------------


next we have a postgres example so in the dockerfile i can use a different URL.
so here we have a postgres application where in the healthcheck command i am using a command of " pg_isready ".

Now with different apps, there is different tools. with postgres, it comes with a built-in tool, that's a very simple testing of a connection to a postgres server 
so it doesn't validate that you have good data or that your database is mounted properly.
it's simply goin to say....does this database server allow connections? yes or no 
so thats a neat one that you can do out of the box   

-----------------------------------------------------------------------------

healtcheck in compose/stack files

version: "2.1" (minimum for healthchecks)
services:
  web:
   image:nginx
   healtcheck:
     test:["CMD", "curl", "-f", "http://localhost"]
     interval:1m30s
     timeout:10s
     retries:3
     start_period:1m #version 3.4 minimum 
-----------------------------------------------------------------------------


and here is what it would look like in a compose or stack file  very similar you will notice that at the start period dow there requires a different version 
so since the healtcheck command came out in 1.12 it was actually supported in 2.1. of this compose file.
but if you are going to use the start period that means you have to update your compose file to version 3.4 in order t support that.
because the start period came out over a year later after the healthcheck command did.....

so lets start with some simple run commands.....
so what we are going to do here is we are going to start postgres database server without the healtcheck because by default it doesn't come with one 

and then we are going to run it again with a manual healtcheck command that will add at the command line and we will see the difference 

so firts without healtcheck
sudo docker container run --name p1 -d postgres........so here we are going to call the first one p1 and we will run it in detach mode from the official postgres image   
                                                        and if we do docker container ls we will see that there is nothing indicating a healtcheck here 

in fact when i do docker containe ls i am not able to see a thing.....

when we do the same command here with the "health" command here and this time we are going to use4 the " pg_isready " 
which we talked about earlier to test  that the connections are available on this postgres server, we are going to tell it that the 
 user we need is the postgres user, we don't actually need to give it a password, its not going to try to log in and its going to try to validate,
 we will use the postgres image 


sudo docker container run --name p8 -d --health-cmd = "pg_isready -U postgres || exit 1" postgres


and now if we made a container ls we see that the health is starting,
so now we get this additional feature in our status of our ls command...it will stay in the starting state for the default 30 seconds until it runs the healthcheck command for the first time.
and now that we've waited over 30 seconds you will see that it's changed to status of healthy 

now if we do " docker container inspect p2 " we will see at the very top of that we have this healthy and it shows at the output that is accepting connections 
status even though that  mine is unhealthy



now we are going to do some service commands to that same database in that same test healtcheck, what we will see here when we do this is that there are three different states that a service goes through
on starting up its preparing which is usually means its downloading the image its starting which means its executing the container and bringing it up.
then it's running and without the healtcheck command the starting and running are very quick, they are almost instantaneous and we will see that here 
with the command 
docker service create --name p1 postgres 

and once its done preparing by downloading the image you will see that it goes immediately from starting to running because there is no healtcheck. 
it doesn't have anything else to do other that start the container and say yeah the binary is running......

but if we do that same command docker service create and call it p2 like before and give it that same command and we start the se4rvice with that health command  built in.....

docker service create --name p2 --health-cmd="pg_isready -U postgres || exit 1" postgres 




what we will see is that it will go from preparing to starting and it will sit at the starting state for the default 30 seconds until the first healtcheck runs  
and this now the docker service expecting a healthy state before it considers this service fully running.
after the 30 seconds is over it will shift to the running state, 
then we get the latest little verify there, just to make sure that it's considereed stable and then we are done  
so you can already see out of the box that the services as well as service updates we are going to get this extra bonus of health concept if we use these commands whenever we can 



mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker service ls
ID             NAME      MODE         REPLICAS   IMAGE             PORTS
78zlg7z71ymp   p1        replicated   1/1        nginx:latest      
g7pignaf3g3g   p2        replicated   0/1        postgres:latest   
ndtsx8abfdo9   p3        replicated   0/1        nginx:latest      
ueg83nku2n64   p4        replicated   0/1        nginx:latest      
mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker service ps p4
ID             NAME       IMAGE          NODE              DESIRED STATE   CURRENT STATE             ERROR     PORTS
wtxsu78jprhh   p4.1       nginx:latest   mike-VirtualBox   Running         Starting 32 seconds ago             
fbis8eemo8h6    \_ p4.1   nginx:latest   mike-VirtualBox   Shutdown        Complete 37 seconds ago             





nice all the errors came out from that i use postgres and not nginx so now we are ready to and of course we need to create the service before we create the container 
so 

sudo docker service create --name p1 nginx  or    sudo docker service create --name p2 --health-cmd="pg_isready -U nginx || exit 1" nginx

and then 

sudo docker container run --name p1 -d nginx or   sudo docker container run --name p8 -d --health-cmd = "pg_isready -U nginx || exit 1" nginx


mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker service create --name p1 nginx
hd9xexklfh5eoqc68w28qjkry
overall progress: 1 out of 1 tasks 
1/1: running   [==================================================>] 
verify: Service converged 
mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker container run --name p1 -d nginx
Unable to find image 'nginx:latest' locally
latest: Pulling from library/nginx
Digest: sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514
Status: Downloaded newer image for nginx:latest
docker: Error response from daemon: Conflict. The container name "/p1" is already in use by container "fa830494edd58d9753bf89eafaf13b5a0c893655c4d1bf68d4655b03cc9adca7". You have to remove (or rename) that container to be able to reuse that name.
See 'docker run --help'.
mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker container run --name p1 -d nginx


and of course the containers that are based to nothing become dangling containers and its impossible to deleted with the original way and thus deleted at the time they showed up 

so 

mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker container rm -f fa830494edd58d9753bf89eafaf13b5a0c893655c4d1bf68d4655b03cc9adca7
fa830494edd58d9753bf89eafaf13b5a0c893655c4d1bf68d4655b03cc9adca7
mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker container run --name p1 -d nginx
193dd002dd5d6dc45e91dad39f3023cc52ece404158a19f5edfd7468937f824b
mike@mike-VirtualBox:~/Desktop/somewhere$ 



mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker container ls
CONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS          PORTS     NAMES
193dd002dd5d   nginx          "/docker-entrypoint.…"   22 seconds ago   Up 21 seconds   80/tcp    p1
9cb994d38956   nginx:latest   "/docker-entrypoint.…"   4 minutes ago    Up 4 minutes    80/tcp    p1.1.hse2rh8j6ex8mzs34juvs98m1
mike@mike-VirtualBox:~/Desktop/somewhere$ sudo docker service ls
ID             NAME      MODE         REPLICAS   IMAGE          PORTS
hd9xexklfh5e   p1        replicated   1/1        nginx:latest   
mike@mike-VirtualBox:~/Desktop/somewhere$ 

25--------------------------------------------------------healthchecks in dockerfiles-----------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                           section 5                                                        
                                                                      swarm app lifecycle
------------------------------------------------------------------------------------------------------------------------------------------------------------------------





                                                                            section 6
                                                              controlling container placement in swarm
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
26--------------------------------------------------------section overview: 5 ways to control container placement-------------------------------------------------------
so here we are going to focus on the swarm tasks or the containers,  how they get placed, how do you control which nodes are actually put on...
because up untill now we've just been deploying them and the go wherever they decide to go , 
but there is actually a lot of built in the swarm onn how you can control where those go based on the scheduler, 
before we get started let me just tell you the quick requirements for this whole section, you actually need to have that three node swarm.
you can have more than three nodes if you want to play aroun d with more....but you need a 3-node swarm to get started.
in that specific swarm you need one manager and two workers.
Really what you just need is.......you need some machines that aren't managers so in my set up i am going to nhave one manager and tow workers so that i can control the difference between 
the two.
and you want to clean everything out....so in previous sections you probably did some cool stuff  and you might have some leftover services or containers 
so you really want to go through and clean out all your stacks, your services, your containers, your volumes and your networks except for the one that you set up at the beginning of this course,
the visualizer.....the visualizer is a part of this course, ongoing, so if you accidentaly deleted the service for it you can just run back to that visualizer lecture in the 
previous sections and just recreate that real quick with a cut and paste.

|-------------------------------|
|controlling container placement|
|-------------------------------|

so here lets learn how containers get placed through the orchestrator...so there is actually this engine that's happening in the background and it's making decisions all the time,
every second multiple times a second, around do you need more tasks, does the service that you've asked for have all the tasks that you've asked ?
the number of replicas,. so if you have five replicas that you've asked for a service its always making sure that those five are running and the are running where you've asked them to run.
now by default up until this point in the course yu haven't worried about where they where running, we've always just started the service, or the stack and it created the services and the tasks that belong 
to those wherver they could fit, and that's an actual default that we can change a lot of it.
so the default is to spread the tasks out, the containers are the same thing as a task, there's a 1-to-1 relationship there, 
so when you're setting up a service and you pick five replicas let's say those five replicas are going to be spread out on so mnay nodes as it can spread them out on by default, and it will also
try to use the least used nodes as well.
But it will make a priority around dispersing your tasks in the service because it assumes you want fault tolerance and high availability so it;s going to spread those out to as 
many nodes as possible.....but there is many ways we can use to control where these tasks place their containers.
and a lot of these options can actually be used together for really complex requirements.
so in this sections we are going to go through five major areas that you can control how a container gets placed on a node.

and the first one there is what i will call service constraints, 
its a combination of features where it uses metadata on the nodes and then when you create a service, you actually apply a constraint to it saying it has to have a specific 
label in order to run that service task...right? ....." #1 node labels plus service constraints (<key>=<value>)"

the second option  there are service modes and you might have even seen that in the interface when you  do a docker service ls, you actually get to see the service mode which up untill now has 
always been replicated, but there is another mode that we will talk about in a little bit called global......" #2 service modes (replicated|global) "

the third option here is placement preferences which is a pretty new feature.
placement prefeences is a soft requirement, meaning that if it can't fulfill this requirement it will still go ahead and create the task  somewhere,
it just  won't be put where you would prefer it and the placemet preferences are really good for spreading out maybe across racks or data centers or availability zones.
and we will talk about that in a minute......"#3  (new in 17.04+) placement preferences (spread)"

the fourth option is node availability, so you can actually control on a node by nodel level whether it is even available for new tasks to be scheduled on it  and we talk about 
the options there......"#4 node availability (active|pause|drain)"


and then lastly we have resource requirements, so you can actually tell a service that it needs a certain amount of cpu or memory, and the schedulere will make sure that whatever node you are putting
it on has that cpu or memory available and if it doesn't it won't schedule it there..."#5  resource requirements (cpu|memory)"
26--------------------------------------------------------section overview: 5 ways to control container placement-------------------------------------------------------

27-------------------------------------------------------service constraints-------------------------------------------------------
   
|---------------------|
| service constraints |
|---------------------|


   -can filter task placement based on built in or custom labels
   okay this lecture is going to talk all about service constraints which is one of the ways we control service task assignment to which node. 
   and its really a simple key value matching algorithm.

   -can be added at create time, or add/remove at update time
   it can actually use labels that are either built in to swarm and the docker engine or custom that you  apply at any time during your service life cycle.
   for assigning the constraints you can actually do that at service creation time but you can also do it at the time of update, so when you are changing something about your service, 
   you can also change the constraints and we do that by removing existing ones and adding new ones.

   -creates a hard requirement, placement fails if not matched
    this is actually a hard requirement which means if you don't get a match, if you set a constraint on a service that can't find a node that  matches it then it will not deploy that task, it will
    basically have the service sitting in a pending statre forever.
    
    -supports multiple constraints
     you can assign multiple constraints to the same service, and that allows you to set up some pretty complex scenarios, and this one of the key ways we can create large swarms with diverse infrastructure 
     and still manage it in one swarm with different services going to different places.

     -support either a <key> or a <key>=<value> pair   
     and when we create that constraint on the service we can ask it to just look for a key and whether or not that metadata key exists on the node or a specific value on that key.
     and we will see some examples in a minute.

     
-can match with == or !=
you can use an equal assignment or a not assignment so true or false, you can force those in the assignment just like you do with most programming languages and the labels are 
coming from two different areas....we talked about that they come from either built in labels or from custom labels that you create.
but those labels actually  themselves  have two differ4ent groupings, and one of them is a node label, which is actually stored in the raft database itself,  
and there is the engine labels which are created in the daemon JSON file on each node where you actually go into the docker engine config and you assign the engine labels inside there,
we will talk a little bit about why those are different, and why you may want to use one or the other on and off here in this example.
but really the gist there is that engine labels are little bit harder, right ? you have got to set them, you have to go in and actyually change the config and restart the docker daemon.

-labels can be node.labels or engine.labels
--node.labels only added via manager to raft log
--engine.labels added in daemon.json to any node {"labels": ["dmz=true"]}
--default to using node.labels, use engine.labels for autoscaling hardware, os

so i usualluy prefer node labels unless i maybe have some scenario where its not easy for me to jump on a manager and create a node label. 
because a node label command is a swarm command, it has to be done a manager node and i have to be someone who has permissions on there, so  
maybe if you are doing autoscaling of your hardware in aws and you want to create some labels for that specific node, like maybe it's in the DMZ and you want to assign it a DMZ label 
well those might be good to put in the engine config, because you can really do automation pretty easily  with that when you are setting up instances in the cloud.
but node labels are really great for an administrator typing those in and defining key parameters about specific infrastracture that we will talk about here in a few minutes. 


|----------------------------|
| service constraint example |
|----------------------------|

so a couple of basic examples on what we are doing here.....


-place only on a manager (2 options for same result)
  --docker service create  --constraint=node.role == manager nginx
  --docker service create --constraint=node.role != manager nginx

the first one here, actually these two lines result in the same outcome.......so you can see where i am doing a docker service create and i am using the node.role constraint and 
that label is...ships out of the box, and the way we know that is that it doesn't say the word labels in it.....so anything that's node.labels or engine.labels is something that we use custom
that we've created custom by a human, but the .something else in this case the role value or key rather the node.role key is built in out of the box  value, 
and so here........(docker service create  --constraint=node.role == manager nginx) we are asking it to only deploy 
the first line onto a node that is a manager, and on the second  line we are basically say the same thing, we are saying deploy this service task to a node that is not  a worker 

there are only two types in a swarm...managers and workers, so these two results in the same effect, but it shows you the example of how you can use both of those 


-add label to node2 for dmz=true, and custom to it
 --docker node update --label-add=dmz=true node2
 --docker service create --constraint=node.labels.dmz==true nginx

  here in the second example we have us adding a label and you will notice that we do a docker node update, that's how you control labels on a node, you can actually use the label-add or the label dash-rm. 
  you will see in that first line that i am adding a dmz=true requirement to node2, or i am adding the label for that, should i say
  and in the second line i am doing a docker service create and i am telling it that the constraint is that this nginx service has to run on a node that has the label dmz and that that label is marked true 
    
  so lets actually see this in action and play around with some of the options e.g we are going to our cmd on windows, so we are on our swarm 

  as we have delete everything and recreate them from the start

  



C:\Users\michael.kourbelis>docker swarm init --advertise-addr  192.168.99.117
Swarm initialized: current node (3ubg17u15nc3duc28f5oyqsd6) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-4omlhjm2yqxq4rhsf1o9y0tdhicy7yio3omlx9h94eru1pxh5s-aldd95uqjsd3reprh4rqupylu 192.168.99.135:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.


C:\Users\michael.kourbelis>docker-machine create i
C:\Users\michael.kourbelis>docker-machine create ii
C:\Users\michael.kourbelis>docker-machine create iii

C:\Users\michael.kourbelis>docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
3ubg17u15nc3duc28f5oyqsd6 *   default    Ready     Active         Leader           19.03.12


this is because i don't have them add to the swarm............................................



C:\Users\michael.kourbelis>docker-machine ssh i
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@i:~$ docker swarm join --token SWMTKN-1-4omlhjm2yqxq4rhsf1o9y0tdhicy7yio3omlx9h94eru1pxh5s-aldd95uqjsd3reprh4rqupylu 192.168.99.135:2377
This node joined a swarm as a worker.
docker@i:~$ exit
logout

C:\Users\michael.kourbelis>docker-machine ssh ii
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@ii:~$ docker swarm join --token SWMTKN-1-4omlhjm2yqxq4rhsf1o9y0tdhicy7yio3omlx9h94eru1pxh5s-aldd95uqjsd3reprh4rqupylu 192.168.99.135:2377
This node joined a swarm as a worker.
docker@ii:~$ exit
logout

C:\Users\michael.kourbelis>docker-machine ssh iii
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@iii:~$ docker swarm join --token SWMTKN-1-4omlhjm2yqxq4rhsf1o9y0tdhicy7yio3omlx9h94eru1pxh5s-aldd95uqjsd3reprh4rqupylu 192.168.99.135:2377
This node joined a swarm as a worker.
docker@iii:~$ exit
logout

C:\Users\michael.kourbelis>docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
3ubg17u15nc3duc28f5oyqsd6 *   default    Ready     Active         Leader           19.03.12
3jolccxxxvzw28lkzkhmfh2pe     i          Ready     Active                          19.03.12
g14k1qj2mt7i94gm9wt4p0pgd     ii         Ready     Active                          19.03.12
a2f14pc5r70b0sx4k04rt80f1     iii        Ready     Active                          19.03.12

C:\Users\michael.kourbelis>


so now we can see that we have four nodes in our swarm and one of them is the leader-manager
and then also i am running the visualizer, so if i go to my browser and hop over to my manager node on port 8080
i can see that 


 the ip of the swarm  you
are going to see the visualizer that shows that you ve got the three nodes 
and the visualizer is running on the manager node on port 8080 we see the three nodes 
and the visualizer is running on the manager node as it should 

so i  our first example letá actually use a  built-in label like we talked about, the node.role, to force nginx 
to run on a worker 


......docker service create --name app1 --constraint node.role==worker nginx........


and now if we go back to our visualizer we will see that our app1 is running on the worker 2

now if i didn't specify that constraint i would still probably run on a worker because swarm prefers to spread out workloads and not assign 
everything to the same box.
and since visualizer was already running a node 1 it probably wasn't going to assign app1 to the manager, 
but i can force it over there......

so lets do a service update command where we actually remove this constraint and add a new one, now you can't change an existing constraint 
you can only add and remove and as you see we make a remove with rm and then add app1 to the manager node 
then if we go back to our visualizer we see that app1 runs on the manager node 

....docker service update --constraint-rm node.role==worker --constraint-add node.role==manager app1 



and we will see in a minute that there are a lot of built in options.
i can actually specify the node name or the  node id, if i wanted to hardcoded the specific node, but for now LETS  leave THAT  there 
and we are now going to add a label a custom label to one of our nodes and we do that with a docker node update 
and we do that with a docker node update....
and so in the same way as before, with the service update command we can only add and remove so we add

docker node update --label-add dmz=true (and we assign that to node2) node2

and when we do that we see in the visualizer we have this nice little graphic that shows the custo labels that we have assigned, so you will see that 
thw dmz = true, so lets imagine that we have a workload that has to run dmz , maybe its an nginx reverse proxy that needs to run in a specific 
virtual network or maybe its an aws and it needs to run in a specific network there, so lets do docker service create, we will just call it 
dmz nginx 

....docker service create....
and we will just call it dmz-nginx and we will assign it a constraint.... 
--name dmz-nginx --constraint node.labels.dmz==true --replicas 2 nginx


docker service create --name dmz-nginx --constraint node.labels.dmz==true --replicas 2 nginx 

and remember that anything that has the word labels in it goes in an array of custom labels that we have assigned to this service, 
so anything i do beyond this, is going to be whatever format i want to use for my proper labeling. 
And we will actually have a lecture later on labeling all the different things you can label in a swarm and the 
proper way to label those and some of the best practices 
but for now we are just going to say.....dmz==true.....
so in the exact command above what we are saying is that we want to make sure that not only is the dmz label there, but it also has a value of true 
and i want add 2 replicas, and we are going to run nginx again at the end, you can  see that both of these replicas are running on the same 
worker that has dmz equals to true.....and that's against the default setup right?

because if we didn't have that constraint in place it would naturally want to make sure that it spreads out those tasks and it assigns a container
per node versus what we just set up here which was to force them on the same 


|-----------------------------------|
|service constraints in stack files |
|-----------------------------------|

version: "3.1"
services:
  db:
    image: mysql:5.7
    deploy:
      placement:
        constraints:
          - node.labels.disk == ssd


what we have here is a stack file and you will see that it's very similar to the service create and service UPDATE COMMANDS, 
so the above is an example of maybe i have  a service called db, and its running a mysql image and i have a specific infrastructure 
that i have added ssds to, so what i've done is i've gone in and i've added a label to the specific node that i basically gave it a label 
of disk equals ssd 


so what i am telling this stack file is that only deploy that db service onto a node that has the disk label and its value is ssd 
so it works the same way as the service commands, 
because again the stack themselves are really just in the background creating  service commands for you.....so we would expect them to function the same way.


|-----------------------------------|
|service constraints:build-in labels|
|-----------------------------------|

node.id (listed in docker node ls)
node.hostname (listed in docker node ls)
node.ip (e.g. 172.168.99.0/24)
node.role (manager|worker)
node.platform.os (linux|windows|etc)
node.platform.arch (x86_64|arm64|386|etc.)
node.labels (empty by default)
*might be others out there but not well documented


above its a handly list of some of the built in labels, these are all node labels that come out of the box and you can see down at the bottom like the node.labels 
is empty by default because that requires you to add custom ones in there, but you can see that we have 
like the id of the node the host name of the node, you can actually get the ip of the node, the role and then what platform OS is running 
and the architecture  whether its 32 bit or 64 bit....an arm that sort of thing 
there is might be other labels out there or engine labels out of the box in specific situations, but a lot of this isn't well documented,
so if you need to get into this stuff you are going to have to maybe experiment with labels and see if they work and maybe dig into the code a little bit 
to figure out what might be hiding in there as the platform matures 


and just rememebr use the following commands to clean up after this lecture in order to start fresh 

---remove the service and labels we created 
.....docker service rm app1 dmz-nginx
.....docker node update --label-rm dmz node2
27-------------------------------------------------------service constraints-------------------------------------------------------

28-------------------------------------------------------service mode-------------------------------------------------------

service modes....notice it in docker service ls 
-----------------------------------------------
okay now that we have worked with constraints a little bit lets look at the next option for controlling container placement, 

default to replicated, but global is an option
-----------------------------------------------
and that is sevice mode, service mode has actually been there all along, you've been using the default, which is a replicated mode.
you can see that with docker service ls command...you actually see a column there that tells you, and its always saiys "replicated".

global=one task per node 
------------------------
but there is another option, the global option and we are going to play with that here, it basically is the opposite of what replicated mode is.
replicated mode has always been where you default out of the box with docker service create.
it will make one replica or one task or one container....same thing.
when you say replicas 10 it will create 10 containers and spread those out on as many nodes as it can in your swarm....and global is the opposite of that  
global says there are no replicas, you can't specify replica, but instead i am always going to put one container on every node, hence the world global....
so its actually very useful to use that in several different situations....but you can do this at service create time...
unfortunately the feature right now does not allow you to use a service update to change your mode.
so make sure you know what mode you want when you put a service into production. it's really good there for security tools or monitoring tools 
or maybe a web proxy that you want on every single node in your swarm or a backup option.
Really i look at this as global mode is when it's replacing the idea that i would usually have, before containers, of an agent that runs on every system,
and lots of times the most common example might be a pci compliance tool that needs to run on every one of your nodes because you run credit card transactions.
so in order to run those in a container without having to specify the replicas to match the magical amount of service nodes....so for example maybe you have 10 nodes 
in your swarm and up until now you ve been specifying replicas equals 10 because you needed that on every single node.

well one thats not going to guarantee that they are always going to have one on every single node.
and if you change the number of nodes it could either miss a node or have two on one node....its not perfect right;
so when you need to guarantee one on each node......that's when you use global.

in 1.13 we got a bonus with global mode, where it can actually be combined with constraints and this means that you can actually tell it, i want one on 
each node but only with a small subset or a specific subset, on my swarm that identifies what this constraint...and we will see that in a second 
so real quick before we go to the command line and play with this, just a couple of basic examples


the first one here is the simplest way to use it....place one task on each node in swarm 
.....docker service create --mode=global nginx

here as you notice i changed the mode from the default replicated to global and i am going to use the nginx and its basically going to put an nginx container
on every single node in the swarm 




the second option combines that with an additional constraint...so this is using the worker role constraint, which means that this will only work 
or this will only apply to nodes that are in worker and not to a manager node, so when i look at this in a second, you will see that it doesn;t put a 
container on a manager node....it's very handy, 
.....place one task on each worker in swarm.....
docker service create --mode=global --constraint=node.role==worker nginx


okay lets see how these work on the command line, before we do real quick, just to make sure check out your visualizer if you don't have it up already 
and make sure that you have everything cleaned up from the previous section..... which make it easier for us showing this off 

so for the first one here let's do
docker service create --mode=global --name tets1 nginx


okay now if we go back to our visualuizer you will see that it's given me three exactly and they are coming up one on each node, 

mow lets delete "test1" (docker service rm test1) and recreate it with a constraint....we are going to use a constraint to put it only on workers, like 
we showed in the example. 
so 
docker service create --mode=global --name test1 --constraint=node.role==worker nginx


now when i flip over to the visualizer what we should see is it's only running on node2 and node3 because in my swarm i only have one manager set up.
notice that at thhe top it says manager on node1 and worker on node2,3 and there we go...so its created exactly two, 
now if i added another node to this swarm this container or in any global container that would match it, would automatically be created on that node,
so that's a really handy thing when you are maybe auto scalling your nodes with...you know maybe a aws, asg or something like that....
if you have automation that does create vm's and add them to swarm this is a great way to make sure that it's running the utilities and tools, like backup tools or proxy
or whatever you need there, so that's when i use it.....so below is an example of using the global mode in a stacked file. 
its actually really simple, because we arew just creating a service and underneath that service name, under the deploy option which is all the swarm options right there that's swarm specific,
we just change the mode, which it always defaults to replicated to global   and if we wanted to add constarints in there, we would just add them like we would in the previous lecture 
and those who would  work together inside that same service 

version: "3.1"
services:
  web:
    image:nginx
    deploy:
    mode:global


    and of course use this command  (docker service rm test1) to clean up after this lecture so that you can start fresh on the next one.

28-------------------------------------------------------service Mode-------------------------------------------------------


29-------------------------------------------------------placement preferences-------------------------------------------------------

okay the next way we can control container placement is with placement preferences, mow in order to do this lecture you need to understand constarints.
so if you skipped over the constraints lecture a little bit ago, go back to that one, because that will help make this one make more sense.
so it actually was a feature...a recent feature in the 17.04 or newe that is the first time we ve seen a soft requirement, and a soft requirement is 
something where if it doesn't get matched, the containers still run...they still actually get put on nodes and executed.
And so it allows for a few unique scenarios that weren't easily duable when you had hard constraints and you wanted to move things around a swarm, but you really mostly cared that they just ran. right ?
so in this the only strategy we have for this os spread, so you could potentially see in the future where we might have different placement preference options 
but really for now its just spread. and that will take a label, any label you choose and it will make sure that when you create the service that...that service will be spread out amongst all the different 
values of that label.
so in this case, if you could imagine maybe doing like, datacenter 1 or datacenter 2 as labels....and then when you create a placement preference it will decide how to make sure that both datacenter 
1 and 2 have nodes running those containers.  
so you will see in a minute when we go through some examples how this makes sense.....
so this is usually used though for on AWS availability zones or maybe regions or datacenter, or racks or subnets.
things where you want to spread workload out amongst various types of infrastructure and you want to ensuder that enough replicas are running in each or in all of the different options..right?
so this actually does work on service create or update.
it actually allows for multiple preferences to  be applied which means that you can have sort of nested or multi-layer preference, where maybe the top prefeerence is the availability zone, and the next 
preference below it is the subnet and we will see how that works in a minute 




this won't actually cause service tasks to be redeployed if you do change labels on nodes and lastly here this actually a little  nuance that you will get into when you start playing with this 
in the real world....but you will notice that just because you deploy certain nodes with labels like, let's say datacenter 1 or 2, if you have other nodes that don't have that label 
in other words they are missing the label than that is actually considered a no label and will also get spread out on the placement preference, if you don't realize it, you may not realize it, 
you need to either one label all your infrastracture with that label....either it's you know its either datacenter 1 or datacenter 2 and that's every node.
or you can combine this with constraints to make sure that this placement preference is only applied to the nodes that you really want it to applied to....

and we will see that of course in a minute akay so lets look at some examples real quick before we actually go to the command line....
so of course in order to use placement preferences you have to have labels on your nodes, so lets say you have three nodes, and each one is actually in an amazon availability zone, 
and an availability zone is kind of like a datacenter within the same city.
so maybe you would have three datacenters and they call them availability zones and the have  aws, so in this case you would first label your nodes and each availability zone with a specific node zone 
number like 1,2, and 3 that's a simple example 


-----------------------------------------------
label all your aws nodes with availability zone
-----------------------------------------------
docker node update --label-add==azone=1 nodeX
docker node update --label-add==azone=2 nodeY
docker node update --label-add==azone=3 nodeX


and then you would actually use a service create command with this placement preference option.
and you would say okay spread this out over this particular label.
and then in this case if i created three replicas it would make sure that there was always one replica in each of the three zones ....

now if you have three servers and you are doing it across three replicas prefernce didn't really make any difference because that's how would work anyway in our 
simple little test environment  
but you can imagine when you had 100 nodes and you had those split between   different zones how this would really help to ensure that you didn't get too many in 
one zone or the other.


-------------------------------------------------------------
make sure your service is deployed to all availability zones 
-------------------------------------------------------------
docker service create --placement-pref spread=node.labels.azone --replicas 3 nginx

 
you can also add and remove these 

----------------------------------------------
use service update to add/remove preferences 
----------------------------------------------
--placement-pref-add spread=node.labels.subnet
--placement-pref-rm spread=node.labels.subnet


and then there is the multi layer preference examplel and for this i am actually going to go to the docker docs website because they have a really great scenario 
and a visual graphic that i think works greate.....and i am not going to try to improve on that, so lets hop over to their website....https://docks.docker.com/engine/swarm/services/#placement-preferences

multi-layer preferences 


ok, so lets here i am on the docks.docker.com website and i am looking at the swarm services page and by scroll down a long way, because it's actually a pretty long page, and you will find the areas
below where it talks about placement preferences.....and then we get into this area where we talking about multi-level placement prefernces, 




 docker service create \
  --replicas 9 \
  --name redis_2 \
  --placement-pref 'spread=node.labels.datacenter' \
  redis:3.0.6






so in this you can see where they've placed two different labels as different preferences in the same service create command.
and then there is a nice graphic here that explains how it will do them in the order it sees them.
so t5he first look at the first one which is actually a datacenter scernario, so maybe you have a datacenter a and datacenter b which groups the containers of datacenter a and b 
into two groups and it will assign them to those nodes....and then it will, before it actually executes the task on the specific nodes, it will actually group those into the the racks there in and so it 
allows a nuanced set of preferences that you can really get  very granular.
I thought it was a greate one, so instead of me making up my own, i just thought i would show you that one 
okey so lets do some stuff at the command line and like before we jump on our visualizer real quick to make sure that you have cleaned out previous lecture stuff like, services and other labels 
and we can starft fresh.
so over at your CLI first we want to actually assign some labels to our nodes so that we can use this for  the preference,
so lets do a 

docker node update --label-add azone=1 node1

so we are going to use that availability zone  example. ande i want to assign this to node1 (docker-machine)
and then i want to hit the up arrow and do azone 2 for 
and then we write that command again like.....

docker node update --label-add azone=2 node2

and for that example we are going to use two availability zones just for it.....to keep it simple so for node3 also we use the azone=2....

docker node update --label-add azone=2 node3


and of course this label has no meaning, really in the real world, it doesn't mean anything until we apply the preference 

so now we go and do a 

docker service create --placement-pref=spread=node.labels.azone --replicas=2 --name webapp1 nginx


-pref=spread....which is again the only preference option we have at this point 
and then node labels.azone  

okay now if we go to the visualizer, what you should see is that webapp will ensure that that service will ensure that it has one of the two replicas always running on node1 
and that fills the availability  zone 1 requirement, and then yours may be different than mine , it may have launched the second replica on node 2 or node 3.......

it doesn't actually care which one it runs them on, because either one will fulfill the requirment of making sure that containers are running in both zones....that's the goal,

akaiy so lest do that again with a new service, and you will just hit the up arrow and back it up a little bit and we are just going to change just to "webapp2"

docker service create --placement-pref=spread=node.labels.azone --replicas=2 --name webapp2 nginx

so its going to be anoter service, exactly the same way and we are going to do it just by naming a different thing, 
what this is going to show is , if we jump back to our visualizer, is you will see now that node2 and node3 are running one container each 
and yours may be flip-flopped from mine, you might not have webapp1 technically running on node2 but those should have one container each, 
and then node1 is running three containers.
so you can see that this is working normally in a normal swarm without any of this prefeence going on, we wouldn't have three running on node1...we would have it more evenly distributed.
but also i want to show you here that this doesn't mean that, that doesn't still takes place.

now you can't use placement preferences in global mode.so you are using it in replicated mode....
hence us using ---replicate equals 2  
but that doesn't mean that the default mode of replicated doesn't still take effect when you are using  placement preferences...because you can see here that node 2 and 3 are spreading the work out 
so once the placement preference has satisfied the requirement of saying hey i just need to run this thing in availability zone 2 and i don't really care which node.....
then inside that preference it will msake sure that it runs evenly out,
so the nice thing here is if you have a huge group of servers, it's going to make sure that its not  goig to ignore the fact it need to spread that workout.....


and just to prove that point again, we are going to actually remove the placement preference from web app1 and see how swarm will actually rebalance that work, 
so lets do a docker service update and we are going to do a placement preference remove.....

docker service update --placement-pref-rm spread=node.labels.azone webapp1

"........update removing preferences no longer causes task re-creation unless you use --force........"

okay so its going to remove that placement preference...and what we should see happen is that it's going to  start balancing that work back out  across the three nodes
so its doing the best it can....okey there is an odd number of container here, but it's take some of the work off node 1 and spread it out so that, now that we don't have 
that placement preference anymore, it's got more flexibility where i can assign it....

now lets do something fun here and just do a what would happen if we scaled up with this placement prefernce, so if you remember webapp2 is still doing the placement preference,
so lets scale that up and see what happens, 
docker service scale web app2 equals eight......docker service scale webapp2=8



now what you should see is because webapp2 has placement preference still in effect, it's going to try to load half of those 8 on node 1 to match the availability zone 1 requirement and then 
half of  them on availability zone2, and because availability zone 2 has two nodes its going to use both of those to fulfill the requirement..... 
alright and as one last thing, lets actually do something that would break this 
so lets add a constraint that says this webapp2 which is still under placement preferences has to run on a worker node....now in my case remember node2 and node 3 are the worker nodes,
so if i do that, it won't bebable to fulfill the placement preference.
but remember placement preferences is a soft requiremnet....so if it can't be rematched it will still run the containers, so lets do that.......for the webapp2

docker service update --constraint-add.role==worker webapp2


and then what you would see here is that since its not allowed to run anything on node 1, its going to load everything  on node 2 and 3, 
so all 8 containers will run on 2 and 3 and it will split the work out between the two of them, but it can't actually fulfill the placemet, 
in otgher words the constraint overrides the placement preference...right?....so the placement preference is just not satisfied, and that's okat because it's not a hard requirement,
if we needed it to be a hard requirement we should have made it a constraint 



akay and for stack files here is an example of how the spread would be used as the placement prefeerence in a stack file again 
 its pretty simple and it goes under placement just like a constraint does underneath 
the deploy area for a service. 


version: "3.1"
services:
  web:
   image:
   deploy:
     placement:
      preferences:
        spread: node.labels.azone

so you can actually again, use this with constraints at the same time....and they would both just fall underneath the placement area of your stack file.....
and just a reminder be sure that you use the following commmand to clean up after this lecture so that you can start fresh on the next one.

-------remove the service created--------------

docker service rm webapp1 webapp2

docker node update --label-rm=azone node1
docker node update --label-rm=azone node2
docker node update --label-rm=azone node3
29-------------------------------------------------------placement preferences-------------------------------------------------------

30-------------------------------------------------------node availability-------------------------------------------------------

 ok next up in our container placement series here is that node availability does not mean that you broke your server, that its no longer available 
 although that does affect container placement.....this isactually underneat the docker node command, there is actually an ability for you to control whether containers can be 
 executed or run on a particular node, that comes in one of three states,  
 so it only affects new containers and containers existing/running. it doesn't affect other things like networks or volumes or anything like that.
 this is really just about containers and service tasks...
 
 and the first one is active  which is all nodes default to active when you create them, they
 are always active unless you set it in one of the other two modes manually

 pause is a good situation for when, maybe you have a node that is acting funny and you need to do some troubleshooting and you are not really sure if it's healthy.
 so maybe you put it in a pause soa that no new tasks or containers get started up on it....but it doesn't affect tasks, any container still running on that node will stay there, 
 for the time being as long as its paused.
 that's really the only time i would really suggets that is for troubleshooting.


third.....drain is an interesting one wher it will pull off all the tasks and stop the containers and then take those tasks from that service and move them somewhere else....
put thme on another node,  
basically  it means thte node is still talking to swarm, but it's no longer able to run containers, so it will gracefully 
stop those,move tasks over and they will recreate somewhere else.....now this is good for maintainance, so if you know you are going to take a node out of rotation and you want to maybe 
replace it with a different node that maybe has an updated OS or maybe you are going to do OS patching or whatever.....
drain is what you would always do on the node before you removed it from swarm. or you shut it down, or anything like that 
because this allows swarm ro schedule these tasks, you know schedule those tasks gracefully and control it in a more appropriate manner, 
and when you are thinking about these options, remember that these actually affect service updates and recovering or failed tasks that need to come back up, again
so in other words, if yopu pause or lets say you pause a node, and  then you have tasks running on it....and one of those containers fails and they have maybe failed their health check,
so then the service orchestrator has to schedule that task again and its not going to put it back on that node because that node is paused, so it wont be able to run new tasks and so if you thing about it,
this could affect even those paused containers if maybe there is an update  to that service where you do a docker service update, so it going to redeploy all the containers in that task and that would cause \
these paused nodes to not receive any of the new work.

so when you think about it, it doesn't actually guarantee that those containers are always going to keep running on that paused node...so just a reminder on that....


and lastly here this is sort of PSA (public service announcement). that on the internet you see a lot of examples and blogs around how to use node availability for your manager nodes.
what they suggest is that you would drain or pause your managers, in order to only run the containers you want on those nodes and not regular workload....right,

in theory with one of the examples, you can say you drain your managers and then you keep them in drain mode all the time, and that means your workers run all the work, and you can sort of 
keep your managers isolated.
now in the real world, i don't believe that works so i don't recommend it, i don't think you are able to really use it in that manner.

the reason being is that you are usually on a production  cluster.
you are going to need to run admin stuff  an your manager nodes,
so you might need to run monitoring tools or you might need to run something that talks to the docker  socker like portainer, a web gui for swarm or some sort of logging engine that 
needs to talk on the swarm api.


anything that need to run on those nodes, you are going to want to run in containers.
so youi can't drain them because those things need to run somewhere  
and then if you pause them like lets say you were to actually deploy stuff that only runs on managers and then pause the nodes so they don't get any more work.  
 
well the problem there is that any time you would do a service update or one of those containers might fail....it can't redeploy new tasks, new containers to those nodes because they are in pause.
so they can't do new work 
so this sort of idea around using node availability to control managers isn't good idea, i recommend using labels for that.
assign constraints to your swarm that make sure that most things don't run on managers.....
then things that have to run on managers, you would definately apply the label so that they always run on managers and that's really what constraints are about, this node availability thing is really meant 
for maintainance and troubleshooting and that's the only time i recommend using it, and there might be some other edge cases that i just dont know about but that's when i recommend,


so lets look at some examples (node availability examples) real quick before we jump to the command line, 


-prevent node2 from starting new containers...docker node update --availability pause node2
....in this case we just do a docker node update   and set the availability to pause for a node and remember  that will mean no new containers run there, but the existing ones stay running.....
and then you can set it to availability drain on node3 maybe, and then that would actually gracefully stop all the containers there , and then spin up new tasks on other nodes that are not in drained 
-stop containers on node 3 and assign their tasks to other nodes.....docker node update --availability drain node3

now we can jump on the cli and see how easy this is to use....

now lets go and see why our visualizer cannot be displayed even if we nuse te ip of the docker swarm init...so we go to google and search for that (Docker Swarm Visualizer)
.........https://hub.docker.com/r/dockersamples/visualizer


docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer

and we are okay then......................................http://192.168.99.135:8080/





C:\Users\michael.kourbelis>docker swarm init --advertise-addr 192.168.99.135
Swarm initialized: current node (i5cp78p9vbvippnyuwr2koarz) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-2fmonnu9zk69del9cm454qnjxutfhbanqe8xubcofsuibgz0xl-8buybohh70sfy1tgbebhzbm1b 192.168.99.135:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.

C:\Users\michael.kourbelis\Desktop\composee>docker-machine create i
C:\Users\michael.kourbelis\Desktop\composee>docker-machine create ii
C:\Users\michael.kourbelis\Desktop\composee>docker-machine create iii

C:\Users\michael.kourbelis\Desktop\composee>docker-machine ssh i
docker@i:~$  docker swarm join --token SWMTKN-1-5oshrk7khfgn1ch81pzh8d6pfjz3q8wi0lthgeybnenzhttp2m-6huwfxyuzc2vb77mimfx1k630 192.168.99.135:2377

C:\Users\michael.kourbelis\Desktop\composee>docker-machine ssh ii
docker@ii:~$  docker swarm join --token SWMTKN-1-5oshrk7khfgn1ch81pzh8d6pfjz3q8wi0lthgeybnenzhttp2m-6huwfxyuzc2vb77mimfx1k630 192.168.99.135:2377

C:\Users\michael.kourbelis\Desktop\composee>docker-machine ssh iii
docker@iii:~$  docker swarm join --token SWMTKN-1-5oshrk7khfgn1ch81pzh8d6pfjz3q8wi0lthgeybnenzhttp2m-6huwfxyuzc2vb77mimfx1k630 192.168.99.135:2377

















okay now that we make our visualizer running and made it sure lets jump over to the command line  and lets actually create some work so we can see how its affected 
by the availability option, so lets do a 

docker service create --name webapp1 --replicas 4 nginx 

and the we must see that service on every node now we go and give the following command

docker node update --availability=pause ii 

we will see that we don't do anything as node ii doesn't have any new work yet and this doesn't change running containers   

alright lets actually do a docker service update on this service and see how that affects node ii, akay lets give 
the 


docker service update --replicas=8 webapp1

and you see here that it's added a bunch of work specifically 4 more but it hasn't changed node ii

and your might be diferent it might have a few different containers on different nodes, but you will see that it couldn't apply new work to node ii
so it had to put all that work on node i and node iii

now lets a do a docker node update availability and lets put that node ii back to active.........docker node update --availability=active ii


and now you will notice that nothing new got put on node ii as docker swarm doesn't necessarily move containers around to  rebalance work just because new nodes came online or became available.
and you notice that if you actually add more nodes to your swarm the current way that swarm works,  is that it doesn't rebalance automatiically all of the work
so we wouldn't expect node ii to get anything different here but if we did do a.....docker node update --availability=drain iii 
set the availability to drain 
then we would see  new work to get put on node ii
and you will see that it will quickly clean everything off of node iii and reassign all that work on the other two nodes.

you can always see docker node ls....will always give you the availability over here on the right so we can see that the third node is in drain 

C:\Users\michael.kourbelis\Desktop\composee>docker node ls
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
dtcxsvbb39906ikk8mfuje48p *   default    Ready     Active         Leader           19.03.12
ttrsbchvkra7cmp3mo4ktioan     i          Ready     Active                          19.03.12
7bhs79jos9cingd6y7bs9yycw     ii         Ready     Active                          19.03.12
ozdmivcn3ml7j7d1a2q6f1jdm     iii        Ready     Drain                           19.03.12

C:\Users\michael.kourbelis\Desktop\composee>

and now if we do a docker node update availability active for node iii and go back we will see node iii still empty because there is no rebalance 
.......service rebalance.....easiest way to fully rebalance is to update the service in a minor way like add a container label....

and of course remember the lecture cleanup to remove the services and set the availability on the nodes to active

docker node update --availability active iii........active 

docker service rm webapp1...........................remove

30-------------------------------------------------------node availability-------------------------------------------------------

31-------------------------------------------------------service resource requirements-------------------------------------------------------

lets talk about resource requirements...these are another way to control container placement in your swarm, it works a little different than docker run 
(docker run has many options service create has different options).
if you ever used any of the resource controls in docker run there's lots of them....there is actually almost two dozens at this point. there is a lot, 
we dont get any of those in swarm, swarms are actually different.
they are only used on service update and create, and they are only going to be on cpu in memory....so we get less options, but these options actually work differently 
because now we are talking about multiple  nodes and we have different concerns in those multiple nodes.....
for example below on the limiting we see that we are limiting the cpu in memory 

--limit-cpu .5
--limit-memory 256M

so whats going to happen is you will schedule this service....it will then limit it to half of the cpu. thats whatever the OS considers a cpu, it might be a physical  cpu,
a single core, it might be a hyper-threaded core. whatever the OS sees, it will give it half of one of those, and it will give it 256MG of RAM.
so this is pretty similar to what you would do for a virtual machine, it gives it hard limits, and it just makes things slower and less performant. 

---beware of OOME (with or without limits)
but i want to caution you on the memory part because if you get too aggressive on limiting memory inside your containers, especially ones like databases that grow over 
time and start adding indexes to memory as it continues to be used.
if you hit that limit on your memory, your container will be killed and then  rescheduled  and this is obviously not an ideal situation if you are running a database.
not so much of big  deal if you are running nginx and maybe nginx is using more than it should, but you want to be careful and very thoughtful about how you control the limits on your memory.
and definately test them with real-world data to make sure that you are going to see the same memory that you are limiting on.


....minimum free needed to schedule container (swarm keeps track)
--reserve-cpu .5
--reserve-memory 256M
The other option we have us reservations, which i think are a little more common when we are dealing with our own resources on our own servers, 
we don't tend to necessarily want to limit them, because we really want them to run as fast as they can....right, its more about making sure that they have the resources they need and that's 
what the reservations are good for.....now the reservations in swarm do not look at real world data on a continual basis of performance on your containers.....its simply a table
in the raft system and the raft database that logs every time you make a reservation, before it schedules a new container on that node, if you've askeed for new reservations, it will then go to the reservation
list for that node and make sure that there is enough resources there, that you've reserved, to be used for this new container.....so if you have 4 cpus ina  server, and you reserve all four for one service,
and it schedules the container on that node, no other reservations can be put on that node....it want be able to squeezee them in even if that application is using nothing, maybe its idle.
so the reservations are about a promise that you are giving to swarm to make sure that it keeps for the services.....now this isn't actually working at the kernel level of the OS, 
it in't actually preventing CPU cycles from being used on anything else other than that container....this is really more about swarm wide, ensuring the resources you need for a given container that it might 
need eventually, are going to be there when it needs them.......

lets look at some examples 

1
--> reserve cpu and memory 
.....docker service create --reserve-memory 800M --reserve-cpu 1 mysql

the example above is a pretty easy one on service create we are reserving memory and cpu we are saying 'for mysql i want an entire cpu',  which means if there's is no servers that have a cpu available 
because maybe you have made a bunch of other reservations, this will never get deployed to a node...it will never schedule it because it can't find enough room in the reservations table.

2
--> limit cpu and memory
......docker service create --limit-memory 150M --limit-cpu .25 nginx

here we  are limiting the memory and the cpu for nginx, so on the cpu side we are basically saying  nginx only gets a few cycles on the system, and that it can't use more than 150MG of ram, and
 if it does try to use more than 150....it will crash, the updates are exactly the same so we don't have changed rm or add options for the memory or the cpu on these,
 you use the same options when you're using the update command, but it's not necessary intuitive to realize that if you need to remove a reservation completely, that you actually set it to 0...
 which I actually think that means give it none.......but what it really means is....to swarm is to remove this reservation   


3
resource requirements in stack files

version: "3.1"
services:
  database:
    image: mysql
    deploy:
      resources:
        limits:
          cpus:'1'
          memory: 1G
        reservations:
           cpu: '0.5'
           memory:500M

all right now lets be real quick and look at a stack file example, as you probably seen before its underneath the deploy area and its going to have the resources  
subheading and then you will see limits and reservations, so that all kind of makes sense, now if you used to running...docker run and you have used compose files,
there is all these other settings for a compose file, but those won't work in a stack file because it has to be under the deploy area, because like i said before, docker run 
has 20+ different options that are really only good for docker run...and in swarm we have different concerns, and all of those options in docker run haven't been added to swarm yet
so this is really it for us for now, let's do some examples at the command line 





----------------------------------------------------------------------------------------------------------------------------------------------------------------------
C:\Users\michael.kourbelis>docker swarm init --advertise-addr 192.168.99.135
Swarm initialized: current node (i5cp78p9vbvippnyuwr2koarz) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-2fmonnu9zk69del9cm454qnjxutfhbanqe8xubcofsuibgz0xl-8buybohh70sfy1tgbebhzbm1b 192.168.99.135:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.


C:\Users\michael.kourbelis>docker-machine create i
C:\Users\michael.kourbelis>docker-machine create ii
C:\Users\michael.kourbelis>docker-machine create iii

C:\Users\michael.kourbelis>docker-machine ssh i
docker swarm join --token SWMTKN-1-2fmonnu9zk69del9cm454qnjxutfhbanqe8xubcofsuibgz0xl-8buybohh70sfy1tgbebhzbm1b 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh ii
docker swarm join --token SWMTKN-1-2fmonnu9zk69del9cm454qnjxutfhbanqe8xubcofsuibgz0xl-8buybohh70sfy1tgbebhzbm1b 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh iii
docker swarm join --token SWMTKN-1-2fmonnu9zk69del9cm454qnjxutfhbanqe8xubcofsuibgz0xl-8buybohh70sfy1tgbebhzbm1b 192.168.99.135:2377

C:\Users\michael.kourbelis>docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer
3f7f69f199186b061ddc567b32f4e2c1d5901f2a565fa84544c9193ee545a60c

192.168.99.135:8080
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
 

okay now for  these examples your node setup will actually affect the outcome of these commands, so in my case i am using the virtualbox docker machine default which means 
i get a single cpu and a giga ram for each node, when I do these reservations here in a minute, that will affect the outcome,
so if you have a different setup, then you realize that you will have a different outcome and you might  need to change stuff a little bit 

so lets do a docker service create and lets reserve some memory reserve-memory and 800M (you can use a lowe case or upper case) we will cvall it 800 and we just use nginx 



docker service create --reserve-memory 800M --name 800 nginx


so in the bacground docker swarm is going into the raft and seeing if any other services have been reserved, resources, and if they have it has to make sure that this will fit in whatever resources 
 the node has total, so in this case we dont have any other reservations but it will assign it to node 2 or 3 because those are at least used....but if i created a new service, so docker service create 
 and we give it four replicas and we give it a reserved memory of 300.....then whato do you think itys going to happen ?....lets give it a name e.g. 300 and then nginx again 
 what do you think its going to happen is that swarm can't actually assign any of these to that third node in my case because the third node doesn't have 300MB of unreserved RAM
.....but in my case it has so it assign this to all three nodes 



docker service create --replicas 4 --reserve-memory 300M --name 300 nginx



now you notice that has nothing to do with the reality of how much nginx is using because i am sure its using far far less than that.....it's really all about what did i ask swarm to promise to that 
container in case it ever needed it....so you will see that it assigned all the 300's over the other nodes because it doesn't have room on the third 

now lets do a......docker service update --reserve-memory 0 800.....
here we change the reservation to 0 on the service with the name 800
in other words we are going to change that reservation to 0 which removes it, and when we do that it will reschedule the container and redeploy it on a new node,
and in this case its going to put it back on the same node because that's the least used node,
but it doesn't have a reservation anymore and if you remember from before these other containers will not rebalance and move themselves over because swarm doesn't do rebalancing it doesn't want to
mess with any of the services that you didn't specifically ask it to mess with, so just because this node has more free resources doesn't mean that swarm is going to go and change the 300 service and suddenly 
move things around, because we probably don't want that to happen 

imagine yourself having a 100 services, if you added a couple of nodes that would be a whole lot of things suddenly happening in your swarm just because you added a few server for the rebalancing 

now what happens when we reserve more than we can fit ?...so lets do a 
docker service create --reserve-cpu 8 --name 8 nginx

i don't have a single node with 8 cpu's in it....
starting with docker in the fall of 2017 we now get this real-time results out of our service create commands and our service update command...so io this case 
its actually saying, i don't have a node that i can deploy to....i have no suitable nodes....so i am just going to wait and its going to wait forever before it can schedule this 



C:\Users\michael.kourbelis>docker service create --reserve-cpu 8 --name 8 nginx
52irkuj95iwh0mr5g690x558l
overall progress: 0 out of 1 tasks
1/1: no suitable node (insufficient resources on 4 nodes)

then we type ctrl+c to get out of that and then it will still be running in the background right? because it's asynchronnous.......so 
docker service ps 8 

and you will see here that it's saying 
no suitable node is the error and that's pending over at the current status, it's pending and it will stay pending untill the resources become available or the end of time 
it will just keep trying and keep waiting, now it's not trying to actually start a container. 
it won't even schedule it until it can find enough reservations 


C:\Users\michael.kourbelis>  docker service ps 8
ID             NAME      IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR                              PORTS
vureytai2ba8   8.1       nginx:latest             Running         Pending 14 minutes ago   "no suitable node (insufficien…"


alright for this last example lets use a limit memory. docker service create --limit-memory 100M --name 100  (and we are going to use a new image) bretfisher/stress:256m

*really quick about this stress image, its actually a testing tool that i borrowed from other people on docker hub and i modified it so that it will actually work easier in swarm.
based on the label of this image  it will use a linux stress command to actually consume CPU and memory resources based on what label you are gonna give the image, 
so in our case we are going to use the 256MB label, which means in that container, it's actually going to run the stress command to use a full 256MB of RAM in that container 
but we have limited that container to only 100....so lets see what happens 

 docker service create --limit-memory 100M --name 100 bretfisher/stress:256m


C:\Users\michael.kourbelis>docker service create --limit-memory 100M --name 100 bretfisher/stress:256m
o1ss7s167b2xvnrahvhk1jyi5
overall progress: 0 out of 1 tasks
1/1: ready     [======================================>            ]
verify: Detected task failure


you see that it says detected task failure and it will keep doing that over and over again because it will schedule the task....the task will start....the stress 
command will try to eat up 256MB of RAM which it doesn't have.....it will kill itself because docker will get an out of mempry error and thell the container to stop 
and then it will rewschedule the task somewhere else and this will keep happening over and over.
if we did a docker service ps 100 
we see the history here and each one of them is exiting with an error code of  1



C:\Users\michael.kourbelis> docker service ps 100
ID             NAME        IMAGE                    NODE      DESIRED STATE   CURRENT STATE           ERROR                       PORTS
ldbgl8s759tc   100.1       bretfisher/stress:256m   ii        Ready           Ready 1 second ago
ygoolm92yr4x    \_ 100.1   bretfisher/stress:256m   ii        Shutdown        Failed 1 second ago     "task: non-zero exit (1)"
9ydnt3yxq4c8    \_ 100.1   bretfisher/stress:256m   ii        Shutdown        Failed 7 seconds ago    "task: non-zero exit (1)"
nd4hgbva4ti6    \_ 100.1   bretfisher/stress:256m   i         Shutdown        Failed 12 seconds ago   "task: non-zero exit (1)"
nqqx2hoomygk    \_ 100.1   bretfisher/stress:256m   iii       Shutdown        Failed 18 seconds ago   "task: non-zero exit (1)"

C:\Users\michael.kourbelis>




in the next section we will talkl about docker events where you can actually see these events happenning in real time 
such as the out of memory error. 
remember to clean up after this lecture including removing the services that we created 

docker service rm 800 300 100 8

C:\Users\michael.kourbelis>docker service rm 800 300 100 8
800
300
100
8

C:\Users\michael.kourbelis>


|----------------------------------------------------------------------------------------------------------------------------------|
|  assignment 3...control container placement--->customize a multi service stack file to control container placement in a swarm    |
|----------------------------------------------------------------------------------------------------------------------------------|

31-------------------------------------------------------service resource requirements-------------------------------------------------------



32-------------------------------------------------------section review-------------------------------------------------------

in this section  we saw a set of features that are actually very powerful and you are going to be using them a lot.
so they are going to be very handy to know now because when you go into production you are definitely going to be using at least a couple of these options all the time.
so lets go through these five major ways of controlling container placement and just review real quick. 


#1 NODE LABELS PLUS SERVICE constraints
-hard requirement. only schedules tasks if swarm matches constraint 
-add labels to nodes first,based on location, hardware or purposes
-then use constraints when creating services

the first one was node labels plus service constraints.
this is a big one because its used all over...you use it for all sorts of different reasons and it actually is a pretty easy thing to associate because its metadata connectiong two pieces together.
remember its a hard requirement.
so if you mess it up its not going to work, and they are not going to get assigned where they need to be, so its going to be pretty obvious when they don't work because containers just won't show up. 
and as a reminder just make sure that you are adding labels firts before you create the containers otherwise you will get the 0/1 in your servuice where its never able to actually get the 
task because it doesn't have any place to put it...then you use the constraints on the service to make sure that the apply to where the only need to go.

#2 service modes replicated (--replicas) or global (one per node)
-global is good for monitoring/logging/proxy/security tools

so the second option is the service mode where we have been using replicas this whole time and didn't maybe realize that there was another mode called global,and that's one per node.
the global is good  for monitoring and logging, backups, proxy.....i think ro me  currently the most common thing i use it for is a reverse proxy when i want to put in a layer 7 load balancer
on each of my worker nodes that's going to be accesible to the internet and that's what i usually use it for....these other things sometimes i need them depending on the situation    
but i think the proxy for me is number one.

-only set at service create time
you can only set this at service time, so remember if you get it wrong if you forgot to do global, you are going to have to remove that service and recreate it,  

-#3 (17.04+) placement preferences
next up is placement preferences which is the newest of these options.....its in 17.04 and newer 
so by now you should definitely have it on all the machines you are using.....its a soft requiremtnt and its the only one that's a soft requirement, so it will still deploy the containers 
if it doesn't match the spread algorithm that you've given it, the spread setting.
if it doesn't match then it will just keep going and you won't actually realize it's not distributing. 
but quite honestly this is for availability zones, this was one of the biggest requirements was availability zones, data centers and racks....
it was all about making sure that you had hardware fault tolerance throughout your data center and throughout the region that you are putting your data in. 
the other reasons for it are much smaller in use thatn really the availability zone and aws option. or in azure and other services.

#4 node availability 
next is node availability...
a lot of people don't consider this a placement control option because they don't think of it as controlling containers, but it really does....
you control containers by setting it to either one of the three modes....the drain or pause.
everything is active by default and node availiability does not have anything to do with whether the server is working or not 
the availability is something you set.
only the administrator can set it, i don't recommend using it unless you need to troubleshoot or put something in maintainance mode.
i don't think it's really good to use it in production setting to limit where things go.....as you ve got lots of other options here that are much more flexible.

#5 resource requirements 
and lastly resource requirements, which do affect container placement...so just as a reminder you've really only got a few options in swarm right now even though with docker run 
we have so many more options in swarm services it's all about managing many different nodes and expecting things to maybe use enough RAM over time and making sure there is enough room 
for them to grow.......
so you ve got two options for the minimum set....
the reserve cpu and reserve memory that swarm keeps track of.....
then you ve got the limit cpu and limit memory for making sure the container never uses more than that 

32-------------------------------------------------------section review-------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                           section 6                                                       
                                                               controlling container placement in swarm
------------------------------------------------------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                        section 7
                                                         operating docker swarm in production
33-----------------------------------------------------service logs: when to use them and their limits----------------------------------------

---service logs....same as docker container logs, but aggregates all service task 

service logs is a new feature for swarm in the first half of 2017.
it took the previous only logging option we had...which was docker container logs and it creates a swarm service concept around it, so it actually pulls the logs from each task,
on each node, for a single service and displays them in your console.

---can return all tasks at once, or just one task's logs

now you can do it for the entire service, so if it has 110 tasks in that service, its going to pull all 100 tasks logs from every node that they are on.
or we can just specify an exact task id and it will show you just that task's logs.

the nice thing is that you don't have to hop to the server like we used to have to, so you can stay on a manager node and get the logs from any service task in your swarm.

---create for real time cli troubleshooting 
its actually good for real time cli troubleshooting, specifically on smaller swarms where you maybe  you don't have a third party logging system, and this is the best option you get 


---has options for follow/tail and other common log options 
it does have options for following or tailing the logs so you can see them in real time and other basic common options
 but its got some limitations to it...

--not for log storage or searching or feeding into their systems 
 this is not your enterprise,searchable service log system, 
 its not going to collect those logs and store them long term  in a database that's easy for searching or of providing alerts or gui or anything like that

 ---doesn't work if you use --log driver for sending logs off server 
 it's really just for sitting at the cli and focusing on maybe a small test dev swarm that you have where it doesn't justify the need for a complicated logging solution.
 or if you are not yet using a third party logging solution and then this is the best you can do, so its nice 
 it works out of the box it comes right with swarm 
 but if you are using the logging driver and if you've looked into swarm services or into the docker run, you know that there is all these drivers that we will get into later around how you 
can take for each container,or each service, you can ship the logs for that would normally be in your logs command...it will ship them off to whatever service you want.

 and there is a lot that comes out of the box with that....but if you are using those with anything other than the two default options of either the json file which is the default logging location where docker 
 will put all the logs in the local system or journald, 
 if you put your logs in the journald, which is a common system logger in linux and some linux distributions. 
 if you are using those two, then the service logs command still works 
 but if you are shipping your logs off to other server using a logging driver, you are not going to be able to use this logs 
 command because those logs aren't on the system anymore they are somewher else.....
 so that's why i really recommend that this command is really about small swarms, test dev scenarios or maybe you are just not to the level of needing a long-term storage of your logging drivers

if you are on aws for example and you decide to use cloudwatch for your logging, then if you are shipping off those logs using the logging driver, they won't show up in the command line anymore and the 
service logs command is no good to you.

you will need to use that other system that they are storing them in for your logging searches and analytics......


|-----------------------|
| SERVICE LOG EXAMPLES  |
|-----------------------|

all right lets look at some examples.....

--return all logs for a service......docker service logs <servicename/id>

 the above is a really basic one docker service logs with the service name or the task id. its just going to give you all the logs for that.
 now if its a bunch of nodes in a swarm along you know slow links or something...
 if you have got nodes in your swarm that are somewhere else on the internet do realize that this is going to take some time to pull those logs over the internet 
 this isn't some caching system where it keeps all the logs on a single node.
 it's dynamically in real-time when you run this command.
 its going to go to those nodes and ask them to stream the logs back to your location....so you might see some delays if you have got remote nodes 

 --return all logs for a single task......docker service logs <taskid>
also if you want to do docker service logs in the task id, that's how you would do that above, and there are some other options with this command as well.

--return unformatted logs with no trunking....docker service logs --raw --no-trunc <servicename/id>
if you use the --help, including ones like  no-trunc and raw.
the no-trunc means that if you hit the limit of the logging in your screen and you really just want to wrap the log because maybe it's a really long log entry and you are seeing it cut off at the end, 
you can just make sure that you are seeing the whole error message or whatever you need with the --no -trunc.
the --raw removes some of the niceties when you actually see this command in an example in a minute, you will see that there is the node name, and the date/time stamp in there and the raw 
format is maybe good for maybe dumbing into a log file locally for you to troubleshoot or search later...so you could do that  


---only return last 50 log entries and all future logs......docker service logs --tail 50 --follow <servicename/id>
and lastly here is an example of using --tail and --follow. follow will make sure that it doesn't disconnect and it stays streaminng logs coming in from the nodes.
and --tail is in case you have really long, or large logs and you don't want to stream that entire log over the network that is on that remote host.

you can just specify the number of lines to return here....normally what i do is i do a --tail and a --follow whenever i am looking because i don't want 
you to know if i have five nodes and i have a bunch of replicas, i don't want to have all the logs because my screen would just sit there scrolling for minutes and i wouldn't be able to see anything
so i would use a --tail with something like 50, to get the most recent logs and then follow them in real time as i am troubleshooting. 

ok set up again your visualizer and you should have not anything else excpet of your 3 machines on your swarm 


docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer



and we are going to start an example voting app like we've shown off a few times in this course, 
you need to be in the swarm stack 5 directory of this course repo, in there you should see a few files, 
one of them is the example stack file,


C:\Users\michael.kourbelis>docker swarm init --advertise-addr 192.168.99.135
Swarm initialized: current node (4z17y6uiv1rhe6c2se2tovbqw) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-4ztd7dyot0ob7pvx7jeee5wgdg5m27bgl9aylsfni9ltck3iyd-726zbclluk3ryjqsvuqwto6vv 192.168.99.135:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.


C:\Users\michael.kourbelis> docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer
0d77a8da6ea644e6d7da824a0ef3fed72695a647ac44b227079b40f463cc6bd5

192.168.99.135:8080



C:\Users\michael.kourbelis>docker-machine create i
C:\Users\michael.kourbelis>docker-machine create ii
C:\Users\michael.kourbelis>docker-machine create iii

C:\Users\michael.kourbelis>docker-machine ssh i
 docker swarm join --token SWMTKN-1-4ztd7dyot0ob7pvx7jeee5wgdg5m27bgl9aylsfni9ltck3iyd-726zbclluk3ryjqsvuqwto6vv 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh ii
 docker swarm join --token SWMTKN-1-4ztd7dyot0ob7pvx7jeee5wgdg5m27bgl9aylsfni9ltck3iyd-726zbclluk3ryjqsvuqwto6vv 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh iii
 docker swarm join --token SWMTKN-1-4ztd7dyot0ob7pvx7jeee5wgdg5m27bgl9aylsfni9ltck3iyd-726zbclluk3ryjqsvuqwto6vv 192.168.99.135:2377

 

now we download the stack 5 directory from github and we are ready to give some lessons.....




C:\Users\michael.kourbelis\Desktop\swarmstack5>dir
 Volume in drive C has no label.
 Volume Serial Number is 18FC-E703

 Directory of C:\Users\michael.kourbelis\Desktop\swarmstack5

06/25/2022  10:53 PM    <DIR>          .
06/25/2022  10:53 PM    <DIR>          ..
06/25/2022  10:53 PM               873 example-voting-app-stack.yml
06/25/2022  10:53 PM               337 generate-some-votes.sh
06/25/2022  10:53 PM               355 make-data.py
06/25/2022  10:53 PM                 6 posta
06/25/2022  10:53 PM                 6 postb
               5 File(s)          1,577 bytes
               2 Dir(s)  81,931,132,928 bytes free

C:\Users\michael.kourbelis\Desktop\swarmstack5>



and we are going to start an example voting app like we've shown off a few times in this course, 
you need to be in the swarm stack 5 directory of this course repo, in there you should see a few files, 
one of them is the example stack file,
the file that we need is the " example-voting-app-stack.yml " so lets go ahead and start with this 

C:\Users\michael.kourbelis\Desktop\swarmstack5> docker stack deploy -c example-voting-app-stack.yml vote

Creating network vote_backend
Creating network vote_frontend
Creating service vote_db
Creating service vote_vote
Creating service vote_result
Creating service vote_worker
Creating service vote_redis

C:\Users\michael.kourbelis\Desktop\swarmstack5>

and while that'e creating because it will take a minute you can go over to your visualizer and wait for everything to be green 
or you can use a watch command at the command line, 
now this watch command unfortunately is not on windows because its a linux tool, 
but you cad do a 
watch docker stack service vote....and this will give us a list of all the services and we will be able to see it......"auto-refresh",
that's what the watch command does......it's not related to docker that's actually a linux command, it just so happens it doesn't have a windows equivalent,
you could just repeat this command over and over, but a lot of times i will use the watch command just to wait for things to show up, because i may not always have the 
visualizer...right? especially in a production environment ... i probabaly don't have this visualizer because its not really a productiony tool....so yeah 
now all my stuff has started, and you can see that we have created three replicas of vote, and it's using global mode   
and we ve created three replicas (but i see 4 thats ok) of the worker and its using global mode and this is so that we can get some good, distributed log stuff going so 
that we can see what logging looks like in a swarm 



C:\Users\michael.kourbelis\Desktop\swarmstack5>  docker stack services vote

ID             NAME          MODE         REPLICAS   IMAGE                                       PORTS
8mczm8mhbqtm   vote_db       replicated   1/1        postgres:9.6
7umsaql8adep   vote_redis    replicated   1/1        redis:alpine
z60sr2etqxsl   vote_result   replicated   1/1        bretfisher/examplevotingapp_result:latest   *:5001->80/tcp
an9i65w6bg2y   vote_vote     global       4/4        bretfisher/examplevotingapp_vote:latest     *:5000->80/tcp
64k43nnmufoc   vote_worker   global       4/4        bretfisher/examplevotingapp_worker:latest

C:\Users\michael.kourbelis\Desktop\swarmstack5>




before we search logs, we need to actually generate some logs.
we could go over to the voting app.
go to the "cats vs dogs" and make a couple of votes.

 but that's not really interesting because in a production swarm  you are going to have a lot of logs.
 so lets actually use a little script i created to generate some logs. 

now that script is in here just in case you are curious about what it does.
it actually uses an apache command called " ab " 
and if i did a cat on the file " generate-some-votes.sh " which is basically a script 
you will see that it's basically going to create some fake data with a python command that actually gives us files that are in the format that the ab command can accept.
and then we are using the ab command to generate 50 different concurent connections on each one of them to vote and then throw in some post data a thousand times each, 
so its actually going to create 3000 votes here and do it pretty fast, but we are not going to run this from our local machine 


............generate-some-votes.sh................

#!/bin/sh

# create POST data files with ab friendly formats
python3 make-data.py

# create 3000 votes
ab -n 1000 -c 50 -p posta -T "application/x-www-form-urlencoded" http://vote/
ab -n 1000 -c 50 -p postb -T "application/x-www-form-urlencoded" http://vote/
ab -n 1000 -c 50 -p posta -T "application/x-www-form-urlencoded" http://vote/

............generate-some-votes.sh................


we are going to actually run this from within the vote container we are going to do that with a docker exec command.
in order to do that we have to find where one  of the votes lives and this is achieved with "docker ps"

C:\Users\michael.kourbelis\Desktop\swarmstack5>docker ps
CONTAINER ID   IMAGE                                       COMMAND                  CREATED       STATUS                 PORTS                    NAMES
3eca7b4b5e1e   bretfisher/examplevotingapp_vote:latest     "gunicorn app:app -b…"   3 hours ago   Up 3 hours             80/tcp                   vote_vote.4z17y6uiv1rhe6c2se2tovbqw.mmk44mjryle41jkg6uuz3iuo3
cc966481a2ab   bretfisher/examplevotingapp_worker:latest   "dotnet Worker.dll"      3 hours ago   Up 3 hours                                      vote_worker.4z17y6uiv1rhe6c2se2tovbqw.cr85e9a41znfl37zvel9k7xpq
058634dd3d93   postgres:9.6                                "docker-entrypoint.s…"   3 hours ago   Up 3 hours             5432/tcp                 vote_db.1.ylgcy1bbxdy830500ia48uzrs
0d77a8da6ea6   dockersamples/visualizer                    "/sbin/tini -- node …"   3 hours ago   Up 3 hours (healthy)   0.0.0.0:8080->8080/tcp   thirsty_kilby

C:\Users\michael.kourbelis\Desktop\swarmstack5>


so through docker ps i can see my local system and you will see that i have somewhere .....since you are running in global mode on your local node, 
you should see the example voting app vote (bretfisher/examplevotingapp_vote:latest) 
i should be able to jump into that container " vote_vote.4z17y6uiv1rhe6c2se2tovbqw.mmk44mjryle41jkg6uuz3iuo3 "
that we see right here and run the script  

so what i did is i provided you an image that has this script built in = generate-some-votes.sh.....so we are just going to run it straight on the voting container itself 
so do a docker exec, then we are  going to instead guessing the name of this, right here we are going to actually use a little 
shell trick........

docker exec $(docker ps --filter name=vote_vote -q) ./generate-some-votes.sh

what we basically do is to run a command inside another command.....now if you are a shell guru, you already know all this stuff....its super easy for you...i am sure.

but for those that are new to more advanced shell automation we are asking the docker ps command to return a container that has this word...." vote_vote "
in its name somewhere and return it quietly so it only return the id....the dollar sign implies that it will actually execute it in line and then provide the result to the outside command, which 
is the docker exec 
and we want the docker exec to run the script. " generate some votes.sh ".....generate some votes 
and if we actually go to ur browser and we went to the results page you will see that the vote tally at the bottom will scale rally quickly to 3000

* and apache bench is a pretty neat tool.
you should definitely check it out i terms of being able to throw a bunch of request at a bunch of servers, 
its a pretty easy thing to do as you can see by the script.

now that we have some real live logging data, lets take a look at......docker service ls 

C:\Users\michael.kourbelis\Desktop\swarmstack5>docker service ls
ID             NAME          MODE         REPLICAS   IMAGE                                       PORTS
8mczm8mhbqtm   vote_db       replicated   1/1        postgres:9.6
7umsaql8adep   vote_redis    replicated   1/1        redis:alpine
z60sr2etqxsl   vote_result   replicated   1/1        bretfisher/examplevotingapp_result:latest   *:5001->80/tcp
an9i65w6bg2y   vote_vote     global       4/4        bretfisher/examplevotingapp_vote:latest     *:5000->80/tcp
64k43nnmufoc   vote_worker   global       4/4        bretfisher/examplevotingapp_worker:latest

C:\Users\michael.kourbelis\Desktop\swarmstack5>


and lets pick one....lets say that we pick the worker (64k43nnmufoc).....and use the command docker service logs vote_worker  


so docker service logs vote worker will stream back all the logs for every vote around 3000 log entries for the different nodes
but for me as i run windows and half of the command are not running as they are linux 
take a look below


C:\Users\michael.kourbelis\Desktop\swarmstack5> docker service logs vote_worker
vote_worker.0.cr85e9a41znf@default    | Connected to db
vote_worker.0.cr85e9a41znf@default    | Found redis at 10.0.2.16
vote_worker.0.cr85e9a41znf@default    | Connecting to redis
vote_worker.0.vmxuyga7196w@ii    | Connected to db
vote_worker.0.vmxuyga7196w@ii    | Found redis at 10.0.2.16
vote_worker.0.vmxuyga7196w@ii    | Connecting to redis
vote_worker.0.u8zsiyqn15vl@i    | Connected to db
vote_worker.0.u8zsiyqn15vl@i    | Found redis at 10.0.2.16
vote_worker.0.u8zsiyqn15vl@i    | Connecting to redis
vote_worker.0.u8zsiyqn15vl@i    | Processing vote for 'a' by '19d9458e1748bdd'
vote_worker.0.yh0lvdxyugxk@iii    | Connected to db
vote_worker.0.yh0lvdxyugxk@iii    | Found redis at 10.0.2.16
vote_worker.0.yh0lvdxyugxk@iii    | Connecting to redis
vote_worker.0.yh0lvdxyugxk@iii    | Processing vote for 'b' by '19d9458e1748bdd'


and since we have the vote as a global mode service, it's going to be on every node, so there should be three of them,
we will just see all those returned  


now there is no build in searvhing yet, so we can't just simply search this.
this is a little tedious but lets keep going 


what if we want to look at a specific task in that service?
do a docker service ps vote worker....docker service ps vote_worker
and lets get the tasks of that service.



C:\Users\michael.kourbelis\Desktop\swarmstack5>docker service ps vote_worker
ID                  NAME                                    IMAGE                                       NODE                DESIRED STATE       CURRENT STATE            ERROR                              PORTS
usciufquu9c7        vote_worker.islrhzdmqdedu7b34un3m5283   bretfisher/examplevotingapp_worker:latest   ii                  Running             Running 5 minutes ago
jfr0cjnkckpz        vote_worker.z38c5qce3d3fbhf706i7ed5e7   bretfisher/examplevotingapp_worker:latest   default             Running             Running 6 minutes ago
jpwi8t2rr3h2        vote_worker.k2h7d80y73ta1pdw42xl4go6e   bretfisher/examplevotingapp_worker:latest   iii                 Running             Running 6 minutes ago
ualm1umq3j6c        vote_worker.islrhzdmqdedu7b34un3m5283   bretfisher/examplevotingapp_worker:latest   ii                  Shutdown            Failed 5 minutes ago     "task: non-zero exit (1)"
llrxwlngq6h6        vote_worker.2o97gv0v9l6mfmlivch7n7imu   bretfisher/examplevotingapp_worker:latest   i                   Running             Running 5 minutes ago
y2lojll961wt        vote_worker.z38c5qce3d3fbhf706i7ed5e7   bretfisher/examplevotingapp_worker:latest   default             Shutdown            Rejected 7 minutes ago   "No such image: bretfisher/exa…"

C:\Users\michael.kourbelis\Desktop\swarmstack5>



and we see we have three tasks here, and you will see over at the left...the task id which
is not the same as the container id or the service id 
and in this case we have to get them from this screen that gives the task id....so we have to use the command below together with any part 
of the task id above......and this returns all the logs just fir the specific task that we give to it 



C:\Users\michael.kourbelis\Desktop\swarmstack5>docker service logs usciufquu9c7
vote_worker.0.usciufquu9c7@ii    | Connected to db
vote_worker.0.usciufquu9c7@ii    | Found redis at 10.0.1.2
vote_worker.0.usciufquu9c7@ii    | Connecting to redis

C:\Users\michael.kourbelis\Desktop\swarmstack5>

if i had millions of logs i wouldn't want them all coming to my screen and streaming over the nwetwork but i could do 

docker service logs --tail 5 (return only the last five) --follow vote_worker (and then follown any future logs for the "vote_worker")

docker service logs --tail 5 --follow vote_worker 


C:\Users\michael.kourbelis\Desktop\swarmstack5>docker service logs --tail 5 --follow vote_worker
vote_worker.0.jfr0cjnkckpz@default    | Waiting for db
vote_worker.0.jfr0cjnkckpz@default    | Waiting for db
vote_worker.0.jfr0cjnkckpz@default    | Connected to db
vote_worker.0.jfr0cjnkckpz@default    | Found redis at 10.0.1.2
vote_worker.0.jfr0cjnkckpz@default    | Connecting to redis
vote_worker.0.jpwi8t2rr3h2@iii    | Waiting for db
vote_worker.0.jpwi8t2rr3h2@iii    | Waiting for db
vote_worker.0.jpwi8t2rr3h2@iii    | Connected to db
vote_worker.0.jpwi8t2rr3h2@iii    | Found redis at 10.0.1.2
vote_worker.0.jpwi8t2rr3h2@iii    | Connecting to redis
vote_worker.0.ualm1umq3j6c@ii    |     TableName: pg_type
vote_worker.0.ualm1umq3j6c@ii    |     ConstraintName: pg_type_typname_nsp_index
vote_worker.0.ualm1umq3j6c@ii    |     File: nbtinsert.c
vote_worker.0.ualm1umq3j6c@ii    |     Line: 433
vote_worker.0.ualm1umq3j6c@ii    |     Routine: _bt_check_unique
vote_worker.0.usciufquu9c7@ii    | Connected to db
vote_worker.0.usciufquu9c7@ii    | Found redis at 10.0.1.2
vote_worker.0.usciufquu9c7@ii    | Connecting to redis
vote_worker.0.llrxwlngq6h6@i    | Waiting for db
vote_worker.0.llrxwlngq6h6@i    | Waiting for db
vote_worker.0.llrxwlngq6h6@i    | Connected to db
vote_worker.0.llrxwlngq6h6@i    | Found redis at 10.0.1.2
vote_worker.0.llrxwlngq6h6@i    | Connecting to redis


and you will see that it returned five from each and then it will follow for any future ones, and in order to get out of that i have to hit ctrl+c to get out 

and lets do 

docker service logs --tail 5 --raw --no-trunc vote_worker

so this time we want "raw" and we want no trunc for vote_worker.....now this time its not going to give us a whole of different 
as the trunc....these are short log entries so that we didn't have any trunc going on 

.......--no-trunc will print full lines, incase your log line was cut off


C:\Users\michael.kourbelis\Desktop\swarmstack5>docker service logs --tail 5 --raw --no-trunc vote_worker
Waiting for db
Waiting for db
Connected to db
Found redis at 10.0.1.2
Connecting to redis
Waiting for db
Waiting for db
Connected to db
Found redis at 10.0.1.2
Connecting to redis
Waiting for db
Waiting for db
Connected to db
Found redis at 10.0.1.2
Connecting to redis
    TableName: pg_type
    ConstraintName: pg_type_typname_nsp_index
    File: nbtinsert.c
    Line: 433
    Routine: _bt_check_unique
Connected to db
Found redis at 10.0.1.2
Connecting to redis

C:\Users\michael.kourbelis\Desktop\swarmstack5>





but you can see how removing the raw...removes the the node identification.
so this might be handy in certain situations  



what if you want to search from the command line?....so there is a couple of ways to do this 
none of them are ideal because there is no built-in search, because again this isn't a full enterprise search logging system but we can do grep on linux 
and mac or anything that would ave linx commands.......
docker service logs vote_worker grep and let's search for a transaction id that exported from the command above 

docker service logs vote_worker | grep usciufquu9c7......

and what you should notice is that it didn't work...i got the entire logs back, so that grep for some reason don't work on mac 
and that's because there is this little nuance with the way that logs in docker work on the console screen 
so in order to get them piped properly....that's what this little line here is that's a pipe   " | grep  "
in order for me to pass the result into the next command...
i actually have to use a little trick...which works on both mac and windows.....
so its 2 greater than 1...and that ensures that both the error logs and the out logs go to the next 
command properly 

docker service logs vote_worker 2>&1 | grep <search term>
The syntax of the command is incorrect.

i must install linux 

but wait a minute there is an equivalent command for windows too.......

but replace grep with findstr.....

docker service logs vote_worker 2>&1 | findstr <search term>


and that's a coomand that is roughly equivalent to the grep command and it will work out of the box 




be sure to clean up by reomving the stack we created.....docker stack rm vote

33-----------------------------------------------------service logs: when to use them and their limits----------------------------------------

34-----------------------------------------------------docker events and viewing them in swarm----------------------------------------

in taking care  of your swarm you are going to have logs that we have talked about and other things you got deal with and one of those is using docker events.
you definitely  if you havent check this feature out because it can be quiet handy in figuring out what's going on in my swarm right now............

--actions taken logs of docker engine and swarm
.....e.g. "network create", "service update", "container start"

its not quite an event log....its more of an actions taken log of the engine and swarm, so its deliberate events that the engine is taking and it;s going to log thise in a quite,
friendly wat for both computers and users, it's actually a little bit legible but it is meant to be consumed by the API of something else or like a logging system or monitoring system that yopu might be using,

- docker events received swarmkit events in 17.06
.....services/nodes/secrets/configs got create/update/remove

but there is a command line for it.... 
so the docker events commands line alows you to see, not just swarm events, but everything about what's  going on in docker.....
this isn't necessarily going to solve al of your logging problems write because they still have service logs about things that are going on inside your containers.
then we still have the daemon logs which are running in docker "-d" to journal of your linux system or windows events logs of your windows system, 
those are the places that talk about the low underlying binary and what service is doing on your system like if it gets access deinied, the file systems and stuff like that, that's not what docker 
events is.
Docker events is sort of a clean list of the actions it's taking on networks and services and containers inside your system 

-has searching (filtering) and formatting 
it has that nice searching features and formatting features so unlike the service logs, we actually get to search it and allow some basic filtering out of it 

-limited to last one thousand events (no logs are stored on disk ), so one of the negative things about it is its not a long term log stored on disk, 
it's really more about real time events with a little bit of history in case you forgot to maybe run the command and you need to check from five or ten or twenty minutes ago or something, 
but if you have got a big swarm, a thousand events could happen pretty quickly,  so its not going to store those very long, 
so what i use it for is usually when i am testing something locally i am testing functionality as i want to make sure it works right....i want to make sure that swarm is executing things the 
way i expect it......
i can actually watch the engine and swarm do their work.....you know creating containers and creating networks and stuff like that in real time.

--two scopes, local and swarm 
it's got two scopes that you really need tyo understand before you start using it, in a swarm  you are going to be able to see the swarm events on a swarm manager which is nice because you can 
see  the events that are getting taken for the entire swarm, but you will soon realize that that's not very much of what's actually happening- that the swarm orchestrator and scheduler 
themselves aren't really deciding  a whole lot...they will maybe decide where the tasks need to go but once that lower level activity needs to happen, like networks created on specific hosts and 
containers created on specific nodes, those things won't be showing up in the swarm scope, the will whow up in the local scope.
the local scope is only on the node that it happens, 
which means if you have a bunch of workers and a couple of swarm managers, you can't just watch the events on one manager.....you have to watch them on every node that mighht get work.
that might can be a liitle tricky when you are diagnosing a large problem so  i don't have a complete solution if you are in a hundered node swarm and you want to do the docker events and see what's 
going on everywhere 
quite frequently that would be too much for your screen and you would be able to see what's happening, so you are definately going to need a logging system as we will get into later in this course on long term
storage of that, if you really need to see a larger swarm's activity as it's happening,

--not the same as dockerd (journald) log, also swarm's activity as it's happening

this is also not the same like i said as the docker "-d" that is going to talk about lower level 
functionality like containerd actually launching containers at the low system level and user IDs and file permissions problems and network errors, those are all going to be happening in real time 
in the journald logs  like   any daemon would on a linux system. 
also using it in windows would be the event logs.
the docker events is not an error log, so you are really talking about three types of logs acrosss your whole system
you are talking about the low level daemon logs of the docker engine, you are talking about these docker events that are sort of an  action or a counting of the things that are taking place 
and decisions being made in your swarm 
and thyen even at a higher level, the application logs that we are using the service logs command for at each container.
those three areas are really will combine along with a few other ones that we will talk about later, including metrics that might be the complete solution for logging and understanding of 
what's going on in your swarm....

lets dive in to some examples 

docker events examples

1. follow future events (docker events)

here we just type.....docker events 
which will give as real time stream starting now, swo its not goin g to dump a bumch of stuff to your screen, its just going to start at this instance and listen on the nodes you are on.
note...that node is just a worker, its only going to see local events 
if it is a manager it will also see the swarm events, it wont be clear necessarily which one is local, which one is swarm, so you will have to learn about the different types of events as you 
are seeing them scroll through your screen, you can also filter by those as well, as we will show in a minute   

2. return events from a date until now and future [(docker events --since 2017-12-01)...(docker events --since 2017-12-01T12:30:00)]
the second example shows a date-base search in the past.....so it allows us to specify a date in time, even the hour up to the second an a specific date format, 
and i want to point you in the references to the actual documentation page for this docker events command because there is lots of formats for different variations of this,
i ddn't want to give you all the examples you could possibly use but there is epoch-based dates....

3. return events from 30m ago until now and future [(docker events --since 30m)...(docker events --since 2h10m)]
there even go based dated so you can do go template which is quite human friendly, you can say docker events --since 30 minutes ago or since 2 hours and minutes ago
assuming that your events go back that far because, remember there is only a thousand of them on each server stored, you are going to be able to use that and honesty that when i am using it 
i am using these easier 30 minutes ago ones....i don't like typing up the long dates. 

that's more for maybe an API call that you might do with some other program....

4.return last hour of events filtered by event name (docker events --since 1h --filter event=start)
  here is an example of using docker events to show everything since the last hour  but applying a filter of obly showing me the start events and that will pertain to specific objects   
  that have the type start, and again look at the documentation becauser there is dozens o dfferent types of events that happen, 

5.only return swarm related events for networks (docker events --since 1h --filter scope=swarm --filter type=network) 
and lastly we have an option here where we do --since 1h ago and we are going to filter to only swarm events and we are going to add a second filter of type=network 
so it's an interesting thing here is that i can use multiple filters to get more and more granular on my events and that in a bigger swarm is really important because if you are going to try to watch something 
happening when you've got hundreds and hundrerd of containers you are never going to see what's going on if you just watching everything......you are going to have to keep using that filter to scope 
it down more and more.

now if you do use a filter with the same key maybe you say....--filter type=network --filter type=container
and those would actually act like an  "or" so you would see both of those at the same time, but in our case, because we are using two different filter keys so i am using a key scope and a key of type 
those are going to act like an "end" which means they both have to be true for me to show that event.....read the documentation for more information on that....


nowe lets go to the command line to write the real deal...by the way you should have a clean swarm here where only the visualizer is running

--------------------------------------------------------------------------------------------------------------------------------------------------------
ok set up again your visualizer and you should have not anything else excpet of your 3 machines on your swarm 


docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer



C:\Users\michael.kourbelis>docker swarm init --advertise-addr 192.168.99.135
Swarm initialized: current node (z13853l3yzp7xhh664zp50clj) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-2eu98nsqsm6vwlbwv2p6ahlxydnhqiwphscwwu1ixabst97sqi-18qcl3lm59q1s6c3443pot8mi 192.168.99.135:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.


C:\Users\michael.kourbelis>





C:\Users\michael.kourbelis> docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer
0d77a8da6ea644e6d7da824a0ef3fed72695a647ac44b227079b40f463cc6bd5

192.168.99.135:8080



C:\Users\michael.kourbelis>  docker-machine create i
C:\Users\michael.kourbelis>  docker-machine create ii
C:\Users\michael.kourbelis>  docker-machine create iii

C:\Users\michael.kourbelis>  
  docker swarm join --token SWMTKN-1-51utdwv6opbenliqm95ati76wwm5yuekl379vu1mk80xa6vk06-6lea8n9mrstf2c9j07goj2cya 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh ii
  docker swarm join --token SWMTKN-1-51utdwv6opbenliqm95ati76wwm5yuekl379vu1mk80xa6vk06-6lea8n9mrstf2c9j07goj2cya 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh iii
  docker swarm join --token SWMTKN-1-51utdwv6opbenliqm95ati76wwm5yuekl379vu1mk80xa6vk06-6lea8n9mrstf2c9j07goj2cya 192.168.99.135:2377

 --------------------------------------------------------------------------------------------------------------------------------------------------------

 we are actually going to start here by stopping the visualizer so that we won't have any events happening on our swarm and the way we can do that is instead of removing the service 
 because then if we did that we would have to re-add it again later.....
 we can actually scale it down to zero, with the command
 docker service scale viz=0
 *  and we call it viz, if you call it something else you will need to change that...and here we equalize viz to 0...it will take down the existing replica and leave the service in existence 
 but it won't have any replicas 



C:\Users\michael.kourbelis> docker service scale viz=0
viz: Error: No such service: viz................................i want linux
 
so it will take down the existing replica and leave the service in existence but it wont have any replicas
of you do docker service ls then you will see that i have 0/0 replicas, so thats a preety neat trick so that we can quickly spin that container back up here in a few minutes when we are done,

take a few moments and in your setup you are going to want to open up multiple terminal windows if you really want to get a sense for what's happening on each node.
what i've got here is i ve got on my left 



docker service create   --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock  dockersamples/visualizer

C:\Users\michael.kourbelis>docker service create   --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock  dockersamples/visualizer
s2xd5dh7tvsrlz6x5x7cwdsos
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged





so i dont want linux but to replace this 

docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer 
with this 
docker service create   --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock  dockersamples/visualizer

as the error above tells us that we haven't a service call viz so we google 

docker swarm create service visualizer
and we go to here 
https://dockerlabs.collabnix.com/intermediate/workshop/getting-started-with-swarm.html

and find the command above 




C:\Users\michael.kourbelis>  docker service scale viz=0
viz scaled to 0
overall progress: 0 out of 0 tasks
verify: Service converged

C:\Users\michael.kourbelis>


with the command above we actually stop the visualizer so that we won't have any events happening on our swarm
and the way we can do that is instead of removing the service, because then if we did that we would have to re-add it again later, 
we can actually scale it to down zero by setting viz tp 0 and then it will take down the existing replica and leave the service in existence but it won't have any replicas 
and if you do a docker service ls....then you will see that i have 0/0 replicas 

C:\Users\michael.kourbelis>docker service ls
ID             NAME      MODE         REPLICAS   IMAGE                             PORTS
s2xd5dh7tvsr   viz       replicated   0/0        dockersamples/visualizer:latest   *:8080->8080/tcp

C:\Users\michael.kourbelis>


so that is a preety neat trick so that we can quickly spin that container back up here in a few minutes when we are done 
take a few moments and in your setup you are going to want to open up multiple terminal windows if you really want to get a sense for what's happening on each node.
what i've got here is i've got on my left i am going to have where i am running my commands and then i will be on node i ii and iii 
and have each one scoped to each node and your swarm , so that way what we are going to do is we arte going to type 
docker events with nothing filtering on each one of these 

and then in our original window (now  i want linux.......)
for node i we are going to create a docker service create and call it nginx......docker service create --name nginx nginx and 
what you should see is on the right you will see at the top   remember that node i is my manager node....i have one manager and that's node i 
and the service create is a swarm specific command  and so that's happening on my swarm manager but then on node2 we will see all the details, we will see the container create 
we will see network create, container start 

so now we know that basically the swarm decided that the node ii should start this container.
that's where those local node activities are going to happen 

you will notice that node ii has nothing.....it didn't even notice that anything happened 

lets kill the nginx service......docker service rm nginx 
and we should see the same thing happened 
up to the top i see service remove and at the node ii  
i see container destroy, container stop, network disconnect and there are a lots of metadata here obviously about each one inside our parenthese. 
now rather thatn having you typing all the same commands that we just typed because those are pretty simplistic and i encourage you to type a few on your own,
but for  the shake of time lets do something quite interesting here and see what happens when we take that previous command of docker service create for limiting memeory 
            
so docker service create (and here lets jsut do something interesting. instead of just going through a bunch of different filter options like you saw in the examples lets actually 
create something that fails. so that docker service create --limit-memory=100M......remember this one where we actually use the stress inage a few minutes agoe ? and call 
it 100 hundred and then we rae going to use the bretfisher/stree:256m )

in a nutshell 

docker service create --limit-memory=100M --name 100 bretfisher/stress:256M

so what this is doing remember is we are actually going to limit the amount of memory this container  can use to 100 but we are going to ask the container to try to use 
256 and its going to fail

so it actually kill itself and have to be restarted somewhere else, so you can see it with the top again, we get the service create command but what you can see happening is actually on these other nodes,
we've got container creates, we have got container fails, container dies....that's a new container die and then container oom....
so that we are actually getting this special event called an out of memory
event that creates a process to  kill the container, remove the container, remove the network and then pick the next best place to put it, it may be on the same node =, 
it may be on a different node.
you can see its what's happening here as this container keeps trying to re-deploy itself, the nodes are actually getting assigned and each one of them at some point 
has been getting assigned to this particular task, and if we go ahead and hit ctrl+c on our docker service create and then try to remove it,
docker service rm 100 then eventually we will see that no more logs will happen, all of these containers on these other nodes will stop, the last thing we will do here is we are going to 
scale back up our visualizes.......docker service scale viz=1...
and you will see up at the top i.e. on our manager node that there is a container create and then does a network connect and then a container start and itsw doing that on the manager because the 
visualizer has to run the manager, so hopefully this has given you a good idea of what you can do with docker events
i higlhy suggets you to play around with it and when you are using some of the docker stuff just let it run in the background while you are running, for example 
by jsut having this for a few minutes we have already realized that there is this interesting  little thing of conatiner health status...." container health_status: healthy 21asd323wjhjwd3282wys "
that showed up because status is one of the events, the health status of 
your container and the reason that that exists in this case is because it's actually running the health check  " container exec_status: /bin/sh -c node healthcheck.js || exit 1 32eeft23e732efwe236ef  "
so in this visualizer it jsut so happens that in that file above there is a health check command and it gets  run on accassion depending on the settings in that dockerfile and the container does an actual execution
of that script (healthcheck.js) inside the container.
you can see that it does this or exit  1.......so its expecting a good result or it will exit 1, which is an error and because it exited properly...its going to update the health status to healthy,
you wouldn't necessarily know these things were happening in the background unless you just let this keep running, every once in a while just make it a  habit i.e. 
start the events in the background, leave it in another tab or something, and go check out what's happening while you are working 
and what you will find is that there are things being decided on and happening in your swarm that you may  not actually realize are happening  
34-----------------------------------------------------docker events and viewing them in swarm----------------------------------------


35-----------------------------------------------------using swarm configs to save time----------------------------------------

swarm configs its a neat feature that came out in 17.06...and it allows you to store strings or whole files inside swarm's raft database and then map those to any path in a container.....
this is actually really handy for those times where you just have maybe a mysql database that you need to run a container, and the only thing you need to change about that mysql is the config before 
you start it up or maybe its an nginx container and the only thing you need to do is give it a proxy config inside  of the "etc" directory  for nginx, 

--NOW YOU DONT NEED CUSTOM IMAGE OR BIND MOUNT TO HOST 
And those are common scenarios 
where you really don't have much to change about an image but there wasn't an easier way to do it, its not great to do bind mounts, 
that's not a great option in swarm because now that means, you have to put the files on the hard drive, and ideally i don;t want to be messing with any files on the disk of my swarm servers themselves,
i'd rather just use the docker command line either locally or remotely to store something in a highly available way so it can be accessed 
by the default images in docker hub and that's what this does 
--17.06+ SIMILAR TO SECRETS BT CAN GO ANYWHWERE IN CONTAINER 
it started out in 17.06 and its similar to secrets, so if it sounds very similar that's because it is, its using the same concepts as secrets but in this case its going to actually store them on any path 
that you specify, 
not just in the run secrets locartion, which is a tempfs....if you remember secrets have to use the tempfs because we are very concerned about never writing an unencrypted secret to disk.
we don't have that issue with configs
so you definately should not put them in secrets and configs 
.....immutable, so rotation is key
these are immutable just like secrets. which means you have to replace them. you can't edit them once you ve created them 
.....removable once services are removed 
they are only removable once the service is removed so its sort of a failsafe to make sure that you don't accidentally remove a config before it's no longer being used 
.....strings saved to swarm raft log (instant HA )
 so its again saved in the raft log which means that it's automatically highly available, so unlikely maybe putting a single file on a host and then bind mounting it in again, don't do that 
you now have it in the raft log, which means if a manager goes down it's still available, its available on every system as long as you have raft consensus
....private keys should still use secrets (RAM disks, enc at rest )
 private keys again should not be stored in the configs,  also configs should not replace....just environment variable settings this is really when you just need a couple of config files, maybe an entire php,
mysql, or nginx config thats hundred of lines or whatever and its too complicated for you to put it in the docker file, 
you don't want to do a text replace in a docker file with that much complexity, so you would rather just   copy the file in, 
but you don't want to have to make an entire image just for that one file...right ? that's the perfect place for swarm configs, lets look at some examples 


now lets look at some examples..........

1.create a new config from a nginx config
.....docker config creat nginx01 ./nginx.config

very similar to secrets and the way we manage them. 
so in this casewe arew just creating and we ar calling it nginx01. 
and we are actually telling it to use the file on a current directory of nginx.conf and just store that inside of a new config called nginx01.

2.create a service with aconfig 
......docker service create --config source=nginx01, target=/etc/nginx/conf.d/default.conf

and then we are going to creat5e a service and we will use a "--" config to map in the service   name to a target file location.
and that can be anywhere on the system because its not like a secret.
it doesn't have to stay in a specific place in a RAM file system  this is just going to be a file mapped on the hard drive,
it will think that it's in the container but its actually mapped in at runtime, similar to a bind mount but its actually coming in from the raft.

3.creating a new config to replace old 
....docker config create nginx02 ./nginx.conf

then if we want to replace it its an easy two step process, i want to create a new config, not edit the original, because you can't edit, 
so i am going to create a new one and maybe i will just increment it ny a number maybe i will use a date in the name, maybe i will use a commit id or some other value to identify that 
this is a certain config   

4.updating service with new config
.....docker service update --config-rm nginx01 --config-add source=nginx02, target=/etc/nginx/conf.d/default.conf

and then when i map it in, i have to remove the old one and then add the new one...and when i add the  new one i am going to specify that same target location but the source is going to 
be different because it will be  a new name for a new config.

then once i've created that service and update it, i can then remove the old config only after the service has been fully updated with the new one.....


so below there is a stack file and you will notice two distinct new parts at the bottom just like with secret's networks and volumes, we are defining configs
and you will see that we have something  like "nginx-proxy" which is just the name that i am giving this config, then i am telling it where the file location is based on this stack file location, that it's going 
to suck in when it deploys a stack file.
and then up top at this file in the service is where i am defining the configs that the service will use....of course many different services can use the same config if needed, 
and you will notice the version requirement at the top, because configs is a new option in 17.06 
so they had to make these additional new options available in compose files and whenever they do that they have to increase the version of the schema of a compose file....so 3.3. is the minimum 
this should make a lot of sense,
you don't actually by the way need the target location, but if you don't specify it then the config will be mounted at the root under the same name as its source name.


version: "3.3."  #3.3. or higher required
services: 
  web:
   image: nginx
   configs:
     -source:nginx-proxy
      target:/etc/nginx/conf.d/default.conf
 configs:
   nginx-proxy:
     file:./nginx-app.conf     


there is a shorthand version but idon't recommend using it because usually, in almost every case you are going to need that config to go somewher else in a very specific location 
for whatever the app needs it to be...right ? 
so you are going to want that source and target location 

ok lets do some config work at the command line. like before you want to make sure that your visualizer is the only thing running, so that we have a clean slate 
and now we are going to be on swarm stack 6 directory.......and that directory has two files.....

example-voting-app-stack.yml.......that's the distributed application with the databases and the websites and stuff for voting cats and dogs 
nginx-app.conf.....................the new file for an nginx config 

C:\Users\michael.kourbelis\Desktop\swarmstack6>dir
 Volume in drive C has no label.
 Volume Serial Number is 18FC-E703

 Directory of C:\Users\michael.kourbelis\Desktop\swarmstack6

06/25/2022  10:53 PM    <DIR>          .
06/25/2022  10:53 PM    <DIR>          ..
06/25/2022  10:53 PM               873 example-voting-app-stack.yml
06/25/2022  10:53 PM               299 nginx-app.conf
               2 File(s)          1,172 bytes
               2 Dir(s)  79,299,964,928 bytes free

C:\Users\michael.kourbelis\Desktop\swarmstack6>



now in this scenario we are going to use a real world scenario of....we have an app and we need to put a simple proxy in front of it, just because we want to protect it from the internet by 
another layer.
this nginx app config isn't going to be actually that complex, but lets first launch the voting stack, so that we can get it rolling while we talk about nginx.
but let's first launch the voting stack, so that we can get it rolling while we talk about nginx 

---------------------------------------------------------------------------------------

C:\Users\michael.kourbelis>   docker swarm init --advertise-addr 192.168.99.135
Swarm initialized: current node (u3y2sq5gzhtx7zl2eer8t5wca) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-4gfxkagv4ah8u4drz9p5hkn3mx0ehr8yztpvlnx0nqd41shfpc-7f5jh7a7mlqip2ut3ot9ohntd 192.168.99.135:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.


C:\Users\michael.kourbelis>





192.168.99.135:8080



C:\Users\michael.kourbelis>  docker-machine create i
C:\Users\michael.kourbelis>  docker-machine create ii
C:\Users\michael.kourbelis>  docker-machine create iii

C:\Users\michael.kourbelis>  docker-machine ssh i
  docker swarm join --token SWMTKN-1-4gfxkagv4ah8u4drz9p5hkn3mx0ehr8yztpvlnx0nqd41shfpc-7f5jh7a7mlqip2ut3ot9ohntd 192.168.99.135:2377
C:\Users\michael.kourbelis> docker-machine ssh ii
  docker swarm join --token SWMTKN-1-4gfxkagv4ah8u4drz9p5hkn3mx0ehr8yztpvlnx0nqd41shfpc-7f5jh7a7mlqip2ut3ot9ohntd 192.168.99.135:2377
C:\Users\michael.kourbelis>docker-machine ssh iii
  docker swarm join --token SWMTKN-1-4gfxkagv4ah8u4drz9p5hkn3mx0ehr8yztpvlnx0nqd41shfpc-7f5jh7a7mlqip2ut3ot9ohntd 192.168.99.135:2377

docker service create   --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock  dockersamples/visualizer


192.168.99.135:8080


and now we are going back to the swarm 6 folder and execute the following command,

docker stack deploy -c example-voting-app-stack.yml voote


and while its creating all of that lets talk about this nginx real quick....it's preety simple config that's basically going to take all the incoming traffic that goes to its app and then relay it to the 
 backend voting app.

 and if you remember, if we go to our browser that if you bring up the port 50000, we have the cat versus dogs   voting page.
 we want to put a proxy in front of this so lets use nginx.
 now in this case we are not going to do anything really fancy.
 let's just cat out that nginx config  e.g. to display the text of that file........

type nginx-app.conf..........which is the equivalent phrase of cat


C:\Users\michael.kourbelis\Desktop\swarmstack6>type nginx-app.conf
server {

        listen 80;

        location / {

                proxy_pass         http://vote;
                proxy_redirect     off;
                proxy_set_header   Host $host;
                proxy_set_header   X-Real-IP $remote_addr;
                proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header   X-Forwarded-Host $server_name;

        }
}

C:\Users\michael.kourbelis\Desktop\swarmstack6>


so we see that what its doing here is it's listening on port 80 and its going to send all traffic to the backend server, which is going to be vote
so this needs to be connected to the same virtual network as vote and then it's going to listen on a different port which in this case we are just going to use port 9000, 
and that way any traffic that hits port 9000 will go through this proxy and then hit our voting service, in later section you will learn all about 
reverse proxies and different options,but for now this is just a simple example of how we need an nginx container running,  we have a config that need to go in it, and before configs existed 
we would probably have to create a new image and then create a docker file that takes this specific customizatrion file and puts it in to the image for us to then run.

And that'sd a lot of work because now you've got to have a repo somewhere storing this config that has to be built in a docker file, and then that image has to be stored somewhere and that's not ideal...right?
this is a much easier way to do it, so by now your vote should have been totally deployed and if you go to your visualizer they should all be green. 

now it is high time to create the config......now we have the nginx app config in this directory.....so lets do 

docker config create vote-nginx-20171211


and we call this one "vote-nginx-20171211"....in this case what i am doing is i am naming it something that's familiar that i will say ok yeah its the vote and its going to be the nginx config,
but i am putting a date on it  .......and this is a tip when you go to production, it's because you are going to change configs and this is the same rule for secrets, you are going to change them 
over time.... 
so you need to come up with a scheme for how you are going to name them in your swarms. maybe their date base which is what i prefer or maybe they have their own commit ID from github or maybe
you are using a single number like nginx01   amnd nginx02 as you increment it, but i personally like dates, so let's use that......

docker config create vote-nginx-20171211 ./nginx-app.conf



we are going to pull in this config file....and by using the above command we just create a config 

C:\Users\michael.kourbelis\Desktop\swarmstack6>docker config create vote-nginx-20171211 ./nginx-app.conf
nzcoi57zgqxuv96ec1t5vx84d


now if i do a docker config ls, i see that i have this one here 

C:\Users\michael.kourbelis\Desktop\swarmstack6>  docker config ls

ID                          NAME                  CREATED              UPDATED
nzcoi57zgqxuv96ec1t5vx84d   vote-nginx-20171211   About a minute ago   About a minute ago

C:\Users\michael.kourbelis\Desktop\swarmstack6>


now we are free to create that service, and if you remember this service has to be created on the network for the voting app which is a frontend network 
so if we do a real quick "type" of the example config, just to refresh our memory, there's two networks here and the web voting app is on the frontend



C:\Users\michael.kourbelis\Desktop\swarmstack6>type example-voting-app-stack.yml
networks:
    frontend:
    backend:

so we need to connect this new proxy service to that network.......we are not going to domit through a stack file, we are just going to need to specify that all at the command line, 

docker service create --config 
and this is where we are going to specify the source and target of our config file.....and the source was if you remember, vote dash nginx dash 20171211 
and then the target is going to be the path of nginx....according to the documentation for the default website config and that is the etc/nginx/conf.d/default.conf
and now we are going to put a port (-p) on that in order to access it we are going to put 9000 because nothing's running on 9000
and then we are going to put it into port 80 in the container because that's what nginx default is 
and then we  are going to specify a network.
that network is going to be the vote frontend 
and then we are going to give it a name of proxy and we are going to run it on nginx 

so this is kind of a long service command but hopefully it makes sense 



docker service create --config source=vote-nginx-20171211,target=/etc/nginx/conf.d/default.conf -p 9000:80 --network vote_frontend --name proxy nginx



C:\Users\michael.kourbelis\Desktop\swarmstack6> docker service create --config source=vote-nginx-20171211,target=/etc/nginx/conf.d/default.conf -p 9000:80 --network vote_frontend --name proxy nginx
Error: No such network: vote_frontend

C:\Users\michael.kourbelis\Desktop\swarmstack6>



so we need to create a network " vote_frontend "

noooooo now i remember that the newtork that i created above has the name voote_frontend 
with this 


-----------------------------------------------------------------------------------------------------------
C:\Users\michael.kourbelis\Desktop\swarmstack6>docker stack deploy -c example-voting-app-stack.yml voote
Creating network voote_backend
Creating network voote_frontend
Creating service voote_result
Creating service voote_worker
Creating service voote_redis
Creating service voote_db
Creating service voote_vote
-----------------------------------------------------------------------------------------------------------

soooooooooooooooooooo


docker service create --config source=vote-nginx-20171211,target=/etc/nginx/conf.d/default.conf -p 9000:80 --network voote_frontend --name proxy nginx

C:\Users\michael.kourbelis\Desktop\swarmstack6> 

docker service create --config source=vote-nginx-20171211,target=/etc/nginx/conf.d/default.conf -p 9000:80 --network voote_frontend --name proxy nginx
image nginx:latest could not be accessed on a registry to record
its digest. Each node will access nginx:latest independently,
possibly leading to different nodes running different
versions of the image.

cwu77de67ze4zcsc9th7cqeg0
overall progress: 1 out of 1 tasks
1/1: running   [==================================================>]
verify: Service converged

C:\Users\michael.kourbelis\Desktop\swarmstack6>


okay so now lets hpe to our browser and jump on the same ip address (http://192.168.99.135:9000) on port 9000 and see what to see 
if it doesn't work you are going to see the nginx config page which says welcome to mykonos
if it didi work you will see the cats and dogs, now cats and dogs actually running on port 5000
but 
our proxy is acting as a reverse proxy in front of it on 9000, so that means that oour config worked 
and if we jump into a docker exec on that container we could actually see that that file's in there and that it was the config that we put in there 

now if we give the following command......docker config inspect vote-nginx-20171211
and that inspect performed on vote-nginx-20171211.....................
we will see that it actually keeps the data and not in clear text 

and i am not really sure that the date are encrypted or not..........


C:\Users\michael.kourbelis\Desktop\swarmstack6>docker config inspect vote-nginx-20171211
[
    {
        "ID": "nzcoi57zgqxuv96ec1t5vx84d",
        "Version": {
            "Index": 95
        },
        "CreatedAt": "2022-07-06T10:00:59.068266455Z",
        "UpdatedAt": "2022-07-06T10:00:59.068266455Z",
        "Spec": {
            "Name": "vote-nginx-20171211",
            "Labels": {},
            "Data": "c2VydmVyIHsKCglsaXN0ZW4gODA7CgoJbG9jYXRpb24gLyB7CgoJCXByb3h5X3Bhc3MgICAgICAgICBodHRwOi8vdm90ZTsKC
            Qlwcm94eV9yZWRpcmVjdCAgICAgb2ZmOwoJCXByb3h5X3NldF9oZWFkZXIgICBIb3N0ICRob3N0OwoJCXByb3h5X3NldF9oZWFkZXIgICBYL
            VJlYWwtSVAgJHJlbW90ZV9hZGRyOwoJCXByb3h5X3NldF9oZWFkZXIgICBYLUZvcndhcmRlZC1Gb3IgJHByb3h5X2FkZF94X2ZvcndhcmRlZF
            9mb3I7CgkJcHJveHlfc2V0X2hlYWRlciAgIFgtRm9yd2FyZGVkLUhvc3QgJHNlcnZlcl9uYW1lOwoKCX0KfQo="
        }
    }
]

C:\Users\michael.kourbelis\Desktop\swarmstack6>



but you see the names here if we want to assign any labels. but you notice it doesn't tell us what service is connected to.
that's the same problem as secrets where we can't see where they are all used and we have to actually go through the services to see what's mounted  
so if we give the following command  

docker service inspect proxy and we scroll up near the top then we actually see the configs 

 "DNSConfig": {},
                    "Configs": [
                        {
                            "File": {
                                "Name": "/etc/nginx/conf.d/default.conf",
                                "UID": "0",
                                "GID": "0",
                                "Mode": 292
                            },
                            "ConfigID": "nzcoi57zgqxuv96ec1t5vx84d",
                            "ConfigName": "vote-nginx-20171211"
                        }
                    ],


and you will see that it even has options around the file mode and the user and group 
that could  have specified if we needed to 


now as before if we tried to remove it with docker config rm for vote nginx 20171211....docker config rm vote-nginx-20171211
 it won't lets us and it will actually tell us where it's used.
 so that's a nice way to figure out where your configs are being used, is just try to remove them, although maybe its not the best idea in case 
 they aren't being used as you would then lose them 



C:\Users\michael.kourbelis\Desktop\swarmstack6>docker config rm vote-nginx-20171211
Error response from daemon: rpc error: code = InvalidArgument desc = config 'vote-nginx-20171211' is in use by the following service: proxy


so lets create an imaginery new version of this config file, so we are not going to actually change the config because that's not the point of this lecture.
but if we did a docker config create vote-nginx-20171212 ./nginx-app.conf 
and you see that we use that same nginx app dot conf, now we have created a new config.....and if we do a docker config ls we see that we have two now 


C:\Users\michael.kourbelis\Desktop\swarmstack6>docker config create vote-nginx-20171212 ./nginx-app.conf
48d3r8qnr3o90x2gbpimff5d0

C:\Users\michael.kourbelis\Desktop\swarmstack6>docker config ls
ID                          NAME                  CREATED          UPDATED
nzcoi57zgqxuv96ec1t5vx84d   vote-nginx-20171211   3 hours ago      3 hours ago
48d3r8qnr3o90x2gbpimff5d0   vote-nginx-20171212   13 seconds ago   13 seconds ago

C:\Users\michael.kourbelis\Desktop\swarmstack6>


and the way we update our service is to do 

docker service update......and this time we are going to do config-rm

docker service update --config-rm vote-nginx-20171211 --config-add source=vote-nginx-20171212,target=/etc/nginx/conf.d/default.conf proxy


and we only have to specify the name of the old one (vote-nginx-20171211) and then we are going to do the same thing for the config add (--config-add source=vote-nginx-20171212) 
as we did it  before where we specify the source 
and then the the target and then we will specify the proxy service at the end....and there we go our service update was succesfully....
again very similar to secrets 

before jumping to the next lecture lets remove the stack. service and the two configs that we have created 


docker stack rm voote
docker service rm proxy
docker config rm vote-nginx-20171211 vote-nginx-20171212




C:\Users\michael.kourbelis\Desktop\swarmstack6>docker stack rm voote
Removing service voote_db
Removing service voote_redis
Removing service voote_result
Removing service voote_vote
Removing service voote_worker
Removing network voote_frontend
Failed to remove network fl71r7tfpm5gk5d4lfk27358c: Error response from daemon: rpc error: code = FailedPrecondition desc = network fl71r7tfpm5gk5d4lfk27358c is 
in use by service cwu77de67ze4zcsc9th7cqeg0Removing network voote_backend
Failed to remove some resources from stack: voote

C:\Users\michael.kourbelis\Desktop\swarmstack6>docker service rm proxy
proxy

C:\Users\michael.kourbelis\Desktop\swarmstack6>docker config rm vote-nginx-20171211 vote-nginx-20171212
vote-nginx-20171211
vote-nginx-20171212

C:\Users\michael.kourbelis\Desktop\swarmstack6>docker stack ls
NAME      SERVICES   ORCHESTRATOR

C:\Users\michael.kourbelis\Desktop\swarmstack6>

---------------------------------------------------------------------------------------

35-----------------------------------------------------using swarm configs to save time----------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                        section 7                                                      
                                                         operating docker swarm in production
------------------------------------------------------------------------------------------------------------------------------------------------------------------------






                                                                       section 8
------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                limit downtime with rolling updates healthchecks and rollbacks 
36-----------------------------------------------------section overview - rolling updates----------------------------------------

the name of this section is about healthchecks and updates....but reallywhat this section is focused on is similar to the last few sections 
which is day-to-day operations or what the industry now calls day two operations 
day 1 vs. day 2 ops
day one operations would be the actions of the initial setup of new servers and deploying an application for the very first time...we've all been there 
fresh new servers ....bright new application  nobody's using it yet and you could set it up exactly like you want it....those are day one operations 
and those are usually very easy to do in comparison to updates and backups and restores and recovering from failure and auto scaling and growing and shrinking 
and all those update kind of problems that break things after day one.
so we don't have day four and day five operations, the term in the industry really is just day two impying everything after day one, so that's what this section is about.

it happens to be mostly around the update lifecycle of your swarm services because that will manage most of the things you are doing

sure you are going to change some volumes and some stuff like secrets and configs and you know networks, but really it's your applications that you are updating on a regular basis  
either due to security patches or new versions of code that your teams have created or maybe you are using someone else's like mysql databases and they've got new updates for mysql 
anyway you ve got lots of update options  and we are going to get thropugh a lot of them in this section and get into some really good details about the specifics of rolling updates
and the nuances of how those take place down to the individual actions  your swarm is taking, so first we are gonna start out with simple rolling updates and getting you into the details 
of how that happens per container in a swarm.

and then give you some nice visuals on the timeline of a service update and what if things don't alwasy go as planned...
how does that update happen even during failure ?


and then we will see how the healthchecks improve it and then we are going to test all sorts of different kinds of healthchecks, common healtchecks  with cURLS, 
custom code that runs a healtcheck that's a specific to our application persistent database, 
healtcheck using redis or Mysql or something like that and a lot more 

then we are going to look rollbacks and how those can be implemented into our updates for quick rollback to a known safe value if we ever had any sort of problems 
with failed updates, i am sure you've all been there,
you had an update you thought was well tested....you had an update you thought was well tested you deployed into production and things don't go as planned.
so that's were rollbacks happen.....you can do them manually or automatically and we will get to test with those too, and then finally you are going to get lost of assignments.
at the end of this section you are goin g to have an assignment where you are going to go through a complete dostributed app and use all the things you learned to update that app to make it more production ready.
and update ready 
so real quick the requirements for this section to get started are that  yiou have that 3 node swarm set up and it can be in any environment that you choose as long as its working and healthy 
------------------------------------------------------------------------------------------------
section requirements 
created a 3 node (or more ) swarm with your environment of choice
craeted the swarm visualizer service from previous lecture 
cleared other stacks/services/containers/volumes/networks
-------------------------------------------------------------------------------------------------

and then you are going to use the swarm visualizer on occasion, so you might as well have that running as well 

---------------------------------------------------------------------------------------------------------
run the swarm visualizer?
-start a service for the docker swarm visualizer
-this is a useful learning graphic that shows us how tasks move around 
-https://github.com/dockersamples/docker-swarm-visualizer

docker service create\
 --name=viz\
 --publish=8080:8080/tcp\
 --constraint=node.role==manager\
 --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock\

 dockersamples/visualizer
---------------------------------------------------------------------------------------------------------
the slide above is that you cann use  as a reference in case you forgot the command to run as a service , so that can be there, 
it's also available in the course materials in the repo. just make sure that you have cleaned out everything else on your swarm and there is nothing runnig on it 
other than the visualizer and lets jump in 
36-----------------------------------------------------section overview - rolling updates----------------------------------------

37-----------------------------------------------------rolling service updates----------------------------------------

once you ve built your swarm and got your app working inside of a cluster and you ve deployed your applications the first time 
everything from then on is usually docker service update  and you will be doing docker service update a lot as you keep updating you app every version hat you want to roll out 
is a docker service update.

that means that you really need to understand how your apps behave in a rolling update style situation, these connections can be quite complicated, so we 
need to go through some of the details of exactly what's going on

make sure you see the previous videos on service updates  as well as healhchecks, because we will assume that you remember that staff starting here   
and just to remember that by default the rolling updates style that swarm does does one replica at a time, but we can customize a lot of that, 
we can changer different options for dozens of different settings....everything from healtchecks to startup and shutdown and how long to wait between that 
when it comes to updates 
and before you take this into production and start updating things on the fly thinking that everything's going to be  fine, test this a lot 
and that we are going to do some examples here where we test applications from the command line and we see how the little things that we change actually change the nature of how the update happens 
and that's really importnat for you to do in your apps as well, 
and obviously different apps handle connections differently 

A mysql or postgres connection is going to be way different during an update cycle than a http simple website 
or a website connection is going to handle things differently than a long polling connection for http 
so you need to understand the nuances of how your apps work and connect to each other and how your users connect to your apps, before you are able to really test that thoroughly 
in swarm.
Swarm is not going to magically solve all these problems, and orchestrator can't do that it's part of the puzzle, but it's also up to your app to know how do i handle connections during shutdown 
how do i move  connections to a different system and so on.
but don't worry few applications do this well out of the box....

i usually work with customers and clients that think that their apps behave ok...but once we rerally start digging in with tools and testing it, we realize that there's things they need to change to 
make it better and that's fine. 
this is all the process of learning how nto get your apps into a distributed model where they are constantly getting updated and you have as little impact on your customers as possible.

new testing tool: httping  
 
 -test a https(s) connection similar to how ping works 

so we need to learn a few tools before we go to the command line.
we are gonna start with a new tool that is the "httping" which is kind of like a real ping like you would use ping from the command line but 
its not using the icmp protocol, if you know your ip protocols  you know that an http connection is actually a TCP protocol 
and the ping command that we are used to uses a totally different protocol called ICMP which is not even related to TCP connections.

so whe we are using web apps or anything web based, we want to use that real http connection sting when we do our testing and this tool does that for us 


-shows http response code, colors, CLI GUI and more 

the nice thing about this tool is it has some nice little features it has colors at the gui if you give it that option 
it has some really great settings for showing headers, so we can see what's coming back out of our servers 
and then it even has a nice little shell GUI that you could elect to use that's similar to like maybe top or something else like that 

-https://hub.docker.com/r/bretfisher/httping/

for you i have set up a docker image and you can look at what it's made of in this github repo, 
and this image is the one that we will be using to test, and it;s basically the same as the standard linux version of this tool that 
you could install with something like appget, but the nice thing is this container that i've built for you and runs everywhere....
-we will be using my container image, but also available on
....brew install httping
....apt-get install httping
....no easy build for windows 
.....docker run bretfisher/httping localhost works everywhere 

you can install this tool on your host OS if you are running MAC or linux but windows doesn't have a binary for it.
the nice thing is as i package this all up into an updated image that's going to stay fresh, and that you can use on your own 
or in this tutorial and hopefully we will learn something about how apps work  with rolling updates.

and tool we need to use is the browncoat....

-new testing App: browncoat

this tool is something that bretfisher created specifically for this type of testing with an orchestrator like swarm.
its a simple HTTP website but we can give it commands that cause it to misbehave in various ways that are pretty common in distributed systems.
Especially when you are doing rolling updates or any type of update 
and i named it after a sci-fi movie that i just love called serenity   
and is based on the idea that the app is designed to misbehave based on commands we give it.....
https://hub.docker.com/r/bretfisher/browncoat/



by default no healthcheck, but functions correctly
different image tags and env vars for changing behavior
purpose: test rolling updates, rollbacks and healtchecks

now this isn't anything like that fancy netflix application called "chaos monkey" that's really more about destroying infrastracture and seeing if the app survives,
this is more like us giving different environment variables or using different images that will cause the application to maybe be slow on startup or it will shut down 
when you hit a certain url and it will shut down with an error code...or maybe the healtcheck won't work 
those kinds of things that you need to be able to test for and know how swarm will change your app or even replace the containers of your app based on these beaviors 
and as you see on the next lectures we are going to test rollbacks of failed updates as well as the updates using these two tools 


1. https://hub.docker.com/r/bretfisher/httping/     
2. https://hub.docker.com/r/bretfisher/browncoat/

37-----------------------------------------------------rolling service updates----------------------------------------

38-----------------------------------------------------testing rolling service updates----------------------------------------

we just finished refreshing our memories on service updates as well as learning the two main tools that we will be using in the next few lectures 
browncoat and httping,

--run a basic service, then update it while we httping 

first we need to get browncoat started so that we can test it with the ping command and in order to get the service started we need to create a network
that it is going to be attachable and remember "attachable" enables an overlay network to allow docker run containers to join it, it lowers the security a little bit of your 
overlay networks so be very careful if you are going to do this in production and you know what you are doing with it 
because attachable means that any random container on any worker could be run by anyone that has access to that server, including someone who's not supposed to have access to it.
and that docker run container could join an overlay network.
normally overlay networks can only have containers that have been given permission by a manager node which persumambly is more secure and stored behind the network more protected 
so attachable kind of lowers our security profile a little bit when it comes to production workloads, but we are just using it ther for testing anywhay, 
so attachable and then "verse" is the name of this network  


docker network create --driver overlay --attachable verse


now that we have a network lets create our service of the browncoat app and set up some replicas..
we are going to open up the port 80, we are going to attach it to the network verse and we are going to set up 5 replicas 
5 replicas is not a magic number but it gives a nice number of replicas to see how each container will be replaced  one at a time 
while we are testing it with the httping command.......and then we are going to run bretfisher/browncoat and we are going to use the v1 image  

docker service create --name firefly -p 80:80 --network verse --replicas 5 bretfisher/browncoat:v1

i encourage you before we keep going through this to make sure you ve read some of the readme of the readmes on my images 
for both httping command  and browncoat app.....just so you get familiar with them and their purposes and what different features they have 
with browncoat we have different images and each image does a little bit different things, in this case the version 1 image will simpy run the app properly 
but it won't have a healtchech for it so docker can't truly know the health of what's going on in the container, so we will start that service up and while we are 
starting that up lets go over to a different tab or a different shell window 
and we are going to run the httping comand in that same swarm against that service so that we can see connections happening in real time,
we are gonna use docker run for this, so you need to make sure that you are on the same swarm and able to run docker run 

we are going to run docker run --rm because we are just running a command line utility that we want to go away when we are done with it 
then we are going to attach it to that same network "verse" and we are gonna run the the  bretfisher/httpinh image 
and we are gonna give it a few options that aren't default 

we are gonna give it....by the way all these options are just httping options that you can find in main page or in the --help of that command 

-lets watch it with http (another window)
docker run --rm --network verse bretfisher/httping -i.1 -GsY firefly/health


so -i is the interval and by default it's every second, we want to go faster so we ar going to say .1 seconds, so that it does it every 100 milliseconds. 
and then we are going to add three other options G s Y

the G has  us using the get instead of header option
the s  gives us back the status codes....the http status codes......more on that in a minute
the Y gives us color because color is pretty 
then we have to give it the url that we want it to connect to over and over again.......firefly/healthz
rememeber that the name of our service was firefly and inside of that browncoat app i created a route or a url that is called healthz i just named it that and this basically 
the idea here is that this would be something in your app that would have a url that would cause a bunch of testing to happen and it would return back an http status code of 200 or something in the 200 
range for healthy and if something was wrong with the app, it should return something else like a 500 or 400 

soooo once you start that you will see that the connections are going to flood your screen pretty fast and that's fine,
we just want this to be a rolling style connection showing us that what's happening in the background is it's actually connecting   each one of those containers, one after the other
the first one the second one and the third one and the fourth and the fifth and then it goes back 1,2,3,4,5 and then 1,2,3,4,5 just like round robin style normally does.

now let's jump back over to our first terminal...and we are going to do a rolling update..the default right, for swarm and we are going to see how this happens in relation to the connections coming in to the service 
while it's being changed........
so lets do a docker service  update, and we are going to update the image to the v2 image 

docker service update --image bretfisher/browncoat:v2 firefly  

we are using v2 and the only difference between v2 and v1 in this case....the way i design my image for this example  is that v2 will change the http status code,
now if you don't understand http status codes  that's ok, thet are just...you know you've probably seen them all the time, if apps on the internet have errors they are 500 hundreds.
if the server or the web page aren't there you get a 400 or 4040 and 200 hundreds are for ok  
now  there are lots' of different of 200 hundreds and i am simply using the http status codes in the 200 range like 201 and 202, to show us visually at the command line which container version 
or which image version is getting the connections.
i don't think you should be using this in production, this is not a way you would normally run a website but it is a great teaching tool for us in the lab to figure out how connections are connecting.
all right, once we start that what we should start looking at, on the right is the green area where it says 201 created, 
that should start changing to a 202 and at first it will be every fifth connection.....and then two out of five connections and then three out of five connections and as it is going through and replacing 
each container onr at a time.
Now remember swarm does not know if your app is healthy or not....so it doesn't know if it's ready for connection so it might start sending connections to your app before it's ready and thats why you see the 
connection is refused sometimes, in fact we can make this worse because most apps in the real world have way longer startups thatn a half a second like this test app....
most big apps take seconds  and sometimes many seconds 

so lets add some delay to the startup of this app as if it was a big app with lots of stuff to do before it's ready for connections 
and we can do that in my browncoat by updating the service...add an environment variable  and in this case all we need to do instead of changing the image we just need "env-add DELAY_STARTUP=5000"
and this app is programmed to look for this environmet variable on startup and then delay its own startup to simulate what it would be like if you had a slow-starting app, which is tottaly normal
and when we do this we go through and  we will change out each container again and on your right you should see it's going to lose a lot of the connections 
because there is 5 seconds now where the container has started but it's not reeady for connections but swarm doesn't know that because there is no healtcheck...it doesn't know the difference between an app 
that it had started and the app that's ready to receive connections, that's a key part of what we are going to learn  in the next lecture    where we break down every step 
of what docker is doing in the background when it replaces that container 


-slow our container startup 
---not too bad, but what if containers take 5 seconds to start up
----docker service update --env-add DELAY_STARTUP=5000 firefly





docker service create   --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock  dockersamples/visualizer

and remember to clean up these steps before moving to the next lecture 

-stop the httping container 
---windows...ctrl+c and docker stop <container name>
---linux/macOS...ctrl+C

-remove the service and network we created 
---docker service rm firefly
---docker network rm verse


38-----------------------------------------------------testing rolling service updates----------------------------------------

39-----------------------------------------------------timeline of service update----------------------------------------


---swarm will upgrtade N instances at a time, change with update-parallelism
------------------------------------------------------------------------------
here we are going to do a detailed description around the individual actions that your swarm takes during an update.
because the nice thing about swarm is it makes the update process look and feel very easy but there is a lot going on in the background 
there is a lot of decisions that are made by the engine.....
and the first thing that happpens with swarm is that it has to decide the parallelism........
the update parallelism is defaulting to 1, which you've experienced this throughout the course when you do an update command it will replace one task 
and then when that one's finished it will replace the second task, and so on.
if you have a very large swarm with a large amount of task in  your service   you might want to increase the update parallelism to speed up te process, 
maybe if you had a dozen or so and you didn't really need two of them at a time, you could do an upddate parallelism 2,
then it would try both tasks at the same time and that basically shortens the update process by half....not a normal thing in most cases because you usually don't have so many extra 
that you can deal with two down ay a time and be fine with it  


---new tasks are created and their desired state is set to ready
.......ensures resource availability (pending)
.......pulls the image if necessary  (pending)
.......creates the container ...without starting it (ready)
------------------------------------------------------------------------------
next we have a multi-step process that is all about creating the task on the specific node that it needs to be on, so for this example, 
lets assume we are doing one at a time and then we leave the update parallelism at default, whats going to happen for each task is we go from pending to preparing....to ready.

the pending is where swarm is looking for a node that matches your requirements that is resource requirements  and your constraints.
if it can find a node that has those two things, it will tell the task to start there.

it then goes through preparing.....and preparing is the mostly image download, 
this will be the one that if you're checking....if you are usinng a watch command or if you are constantly checking the status of your service update, you will see preparing preparing preparing,
if you have a large image or if your network is slow.

once the image is downloaded, assuming that it has access to that image,. it will then go into the ready state where it creates the actual container on the node  but doesn't started it yet
it just has it sitting there with all of its configuration ready to start.

---if a task fails to get to ready state, it retries with a new task
----------------------------------------------------------------------------
now  if any part of that messes up maybe you misnamed the image or it doesn't have access to the image   or maybe there is no node 
with resource requirements that match what you need.
if any of those kind of things have problems....it will delete that task and then it will start the whole thing again with the pending state,

---when tasks are ready, it sets the old tasks desired state to shutdown 
but let's assume that all worked and you are now at a ready state then swarm will take  the old task and set its desired state to shutdown.
when it shuts down there is that unknown period of time where your app may want to gracefully shut down connections or gracefully wait for those connections to expire....that could be minutes 
maybe even hours depending on the type of services you are running, if you are running a fancy upload service for your users and maybe they are streaming video or data to,
you might have to have a very long shutdown wait period set inside of your swarm  commands, that would be on your service create   or in your stack file....that means that this part may take a while 

---when the old tasks are shutdown, it starts the new tasks, set to running 
but once it's shutdown properly or that shutdown wait period has expired and it's killed the application because its tired of waiting, once that happens, then that's when it starts the container for the new task.

---then it waits for the update-delay, and continues with the next task batch.
and that's all just for one task.....then it goes to the next task   ands there is a delay between them,. normally that is 0 that update delay can be set in case you want to spread these out   
and you want a task to sit there and maybe wait for five minutes before you go to the next one.

that's not normally the case. normally you've done enough testing and you have enough things like healthchecks and whatnot in your app  that you don't need this additional set of delay, but you can do that 

now as a refresher here are some of the update options that will affect your update process....



----update options 
batteries  included, but swappable

--stop-grace-period          time to wait before force killing a container (ns| us|ms|s|m|h)
--stop-signal string         signal to stop the container
--update-delay               delay between updates 
--update-failure-action      action on update failure ("pause"|"continue"|"rollback")
--update=max-failure-ratio   failure rate to tolerate during an update 
--update-monitor             duration after each task update to monitor for failure 
--update-order               update order ("start-first" | "stop-first")
--update-parallelism         maximum number of tasks updates simultaneously


i include the first two there of the stop grace and the stop signal because the grace period is what we just talked  
about. 
that will affect the update process if by default the 10 seconds isn't long enough and you didn't change this then it will kill the old task, potentiallt severing connections 
and this why it is really important for you to know your apps well and in a distributed system   know 
how long the connection should last whether you need to wait an additional time 
hot you end those connections and shutdown that gracefully..............that's a  complicated process per app where you need to know more about it.
and in most applications  in the old days we didn't have to know that because we rarely updated them, and we usually did it during a maintenance window right....
and that window we were sort of allowed to break things but in this  now, new style of automated deployments and deployoing possibly multiple times a day, you now have to know 
your connections and how the clients and the different parts of your app will handle all of that, that's where these options come in and that's where you really need to test 


-----more timeline things to care about 

....healthchecks, if enabled, affect running state 
and there is even more here that you need to care about....the next lecture we will go through some detailed healtchecks....and how that really affects your update process....super important....

....what if my new version fails starting or healtcheck?
but what if you are during the middle of your update process and things start going badly...
what if tasks start to fail during the update process, maybe the first one worked...maybe the second one failed and so on 
.......--update-max-failure-ratio
the first thing here is that you have a failure ratio, that is the number of tasks in my service that can fail during the update process and i still mopve forward, right....
maybe you are like google and you have so many of these things that a few failing all the time is tottaly normal for you...well most of us are not like that but 
that's why they set the update failure ratio to 0, in other words if any task fails during the update process or if the healtcheck was there and it failed it would then initiate the failure 
action......
.......--update-failure-action ("pause"|"continue"|"rollback")
the failure action defaults to pause which means it will freeze frame and then let you decide what you want to do next  

........usually : test = pause or continue to troubleshoot 

i give you some recommendations here......these are general guidelines but really in a test environment, you usually want to pause or continue 
in other words i just want to deploy the whole thing even if it breaks it, because i am going to want to learn and debug, and try to figure out why its not working in the first place 

........usually: prod = rollback 
in production lets be more conservative.....lets set the default to rollback, rollback means if i see any problems with any tasks i will stop going forward in the update process 
and i will bring back the old image or the old changes, or whatever the changes are you are doing in an update it will undo those for the tasks that it's tried so far ,  
we will go through an update rollback in a future lecture 


.....--update-order, default to stop-first ("start-first"|"stop-first")

 but remember your update order that's another one we will talk about that in the next slide 



 service update examples 
 so here are some quick examples that you will get to play with later but i just want to cover them real quick  and why they are special 

 ......monitor for 5 minutes before next, rollback on failure 
 ......docker service update --update-failure-action rollback --update-monitor 5m node 

 the first one here is an example of us doing an update  where we are adding a monitor delay   which this isn't a delay per se, but it seems like that when you are doing it....
we have a failure action of rollback as well, so let's imagine we ran this command, what's going to happen in the background is that it's going to deploy each task one at a time,
because that's the default right, but it's not waiting with nothing to do like the update delay does and in this case  it's actually using your healtcheck assuming you have one,
so it considers that as period of potential failure and if anything happens at all with the healtcheck or if the application just crashes it will then initiate the rollback   


......update 5 at a time, up to 25% can fail until failure action 
..... docker service update --update-parallelism 5 --update-max-failure-ratio .25
..... for if you have lots of containers and distributed failures are ok  
  
  and the above one is a more aggressive this is maybe the other side of that coin wher lets say youhave lots of tasks, lost of replicas, 
  and you are saying up to 25 per percent of my tasks can fail the update process  and i want you to keep going   i want, i dont care because i ve got so many that a quarter 
  of them could have issues...
  and ion this case we are doing the parallelism of 5 which and this means you must have many many dozens of replica tasks because doing 5 at a time is a really aggressive way,
  but it can be done, so you might have a big farm  and that  might be fine, 
  
  ......start new container first before killing old one 
  ......docker service update --update-order start-first wordpress
  ......good for single-replica services (you didn't need HA)
  ......not good for database with volume storage (avoid file multi-access)

  and if we do this update order start first that might work really well if you have a single-node setup. 
  let me explain 
  a lot of times we have the need for a production environment but we don't have the need for hardware failover, so we just need a one-node swarm 
  perfectly legitimate use case.....i see people do it all the time especially for small hobby projects  or things that aren't really business critical.
  totally fine. i would still use swarm in that case because swarm gives us all these extra features that you don't get with docker run  
  a single-node swarm, though, means that during the update process if you are only running one replica the default of stop first means that it will kill the current task before it starts the new task.
  that means more downtime, so you can do a start first which means keep the old one running  start the new one and then when the new one's healthy then shut the old one down. that works really well for 
  something like wordpress or drupal....if you know how to deal with the volumes and the shared storage of all that, but you don't want to do that for a database.
  A database typically if you are going to replace the container on the same volume thode database files cannot be accessed by multiple databse applications at the same time 
  that's a big no no.....
  so even in a case if you have to update the database application and you want zero downtime even on one server, that's where you are going to need to run multiple databse containers  
  and use whatever the database's configuration is for replication maybe its a database cluster like in mysql or maybe its mirroring like in redis 
  thats depends on your application and the situation you are in, but just be careful with that update start first, because the start first can bite you if you are dealing with persistent data  

39-----------------------------------------------------timeline of service update----------------------------------------

40-----------------------------------------------------assignment-try update options----------------------------------------

now that you ve seen a bunch of examples and maybe you were following along like i hoped you would or maybe you didn't.
now is a good time to take a break and do some assignments on your own where you are playing around with commands and seeing results in real time 
so i am going to give you some examples 
first what i recommend you do is delete the service  and the network or at least the service that you created in previous lectures, 
and you are going to recreate it, with a new constraint.
we want to make sure we are starting from the same base point instead of worrying about which current options we have set on the swarm service.
so go ahead and create your own network if it wasn't already created, the network called versee
and recreate the service and this time i would like you to use a constarint on it before you start playing around .
and have that constraint limit to the node that you are on.

basically, if you are using docker machine or however you are managing your three-node swarm here you want to be on the node that you can use docker events on.
and docker events are really handy for seeing how docker swarm and the nodes themselves are controlling each of the actions  as you do these samples.
in order to see those you don't alwasy see those all in the cli when you do the docker service update commands.....you reall need to be looking at docker events as well
now if you remember from the previous lecture on docker events, not alldocker events show up on a specific server, if it's a node event such as creating or deleting a container 
that will only show up on the node in the docker events of that node that it's happening, 
so here to ensure that you are getting all the events on your screen you want to limit all the things happening to one node, it doesn't really matter if there's 100 nodes
or one node, swarm will take the same actions every time, so events won't really change, it will jyst ensure that you are getting them all.

1.create service (constraint to current node)
---------------------------------------------
docker network create --driver overlay --attachable verse
docker service create --name firefly -p 80:80 --network verse --replicas 5 --constraint "node.hostname==node1" bretfisher/browncoat:v1


C:\Users\michael.kourbelis>docker network create --driver overlay --attachable versee
88nbyqzaaupyc58vs1zkkvr4m

C:\Users\michael.kourbelis>docker service create --name fireflyy -p 9091:9091 --network versee --replicas 5 --constraint "node.hostname==node1" bretfisher/browncoat:v1
overall progress: 0 out of 5 tasks
1/5: no suitable node (scheduling constraints not satisfied on 1 node)
2/5: no suitable node (scheduling constraints not satisfied on 1 node)
3/5: no suitable node (scheduling constraints not satisfied on 1 node)
4/5: no suitable node (scheduling constraints not satisfied on 1 node)
5/5: no suitable node (scheduling constraints not satisfied on 1 node)
5/5: no suitable node (scheduling constraints not satisfied on 1 node)


and this is because i dont have node 1 maybe and not


you must be in the manager node 


C:\Users\michael.kourbelis>docker-machine ls
NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER      ERRORS
default   *        virtualbox   Running   tcp://192.168.99.135:2376           v19.03.12
i         -        virtualbox   Running   tcp://192.168.99.160:2376           v19.03.12
ii        -        virtualbox   Running   tcp://192.168.99.161:2376           v19.03.12
iii       -        virtualbox   Running   tcp://192.168.99.162:2376           v19.03.12
node1     -        virtualbox   Running   tcp://192.168.99.163:2376           v19.03.12

C:\Users\michael.kourbelis>docker-machine ssh default


docker@default:~$ docker network create --driver overlay --attachable versee
in73ejfvfk4m4fks07enqbyjt

docker@default:~$ docker service create --name firefly -p 80:80 --network versee --replicas 5 --constraint "node.hostname==node1" bretfisher/browncoat:v1
pbviom7h74fs4rh63mw4b9k3i
overall progress: 0 out of 5 tasks
1/5: no suitable node (scheduling constraints not satisfied on 4 nodes)
2/5: no suitable node (scheduling constraints not satisfied on 4 nodes)
3/5: no suitable node (scheduling constraints not satisfied on 4 nodes)
4/5: no suitable node (scheduling constraints not satisfied on 4 nodes)
5/5: no suitable node (scheduling constraints not satisfied on 4 nodes)
^COperation continuing in background.
Use `docker service ps pbviom7h74fs4rh63mw4b9k3i` to check progress.



docker@default:~$ docker service create --name fireflyy -p 81:10 --network verse --replicas 5 --constraint "node.hostname==default" bretfisher/browncoat:v1
xn1z10ym5zipqqonn6jnljxb7
overall progress: 5 out of 5 tasks
1/5: running   [==================================================>]
2/5: running   [==================================================>]
3/5: running   [==================================================>]
4/5: running   [==================================================>]
5/5: running   [==================================================>]
verify: Service converged
docker@default:~$


so here's a few examples that i can give you and you can always try your own, but here some that i want you to walk through as well 

2. monitor for 15 seconds before next task (no -op)
---------------------------------------------------

docker service update --update-monitor 15s fireflyy
docker service inspect --pretty fireflyy

first i want you to update the service to monitor for 15 seconds.....now remember this is at the end of the service update  not at the end of each task in that update 
but if you change it to 15 seconds what you will notice if you are watching events  and you are watching your command line at the same time 
what you will notice is this....is what called a NO-OP command   
in other words this is something that changes metadata in the service description but it doesn't require redeploying new containers  and there is lots of options like that 
ones that are really just about changing the configuration of the service that doesn't affect the container....things that would affect the container are a new image or how many replicas 
or maybe adding an environment or removing an environment variable......stuff like that, adding and removing a constarint may affect the service but there is one such as labels the update monitor, 
anything about rollbacks or updates, 
those things are really about changing the metadata and the service that will affect that in future updates that might require an actual change.
and then just to see the update and monitor chnage you just made, you obviously see at the end of that command...it will wait 15 seconds now instead of the default of 5 
but it is a must to do a....docker service inspect......because again inspect is a great tool for looking at any object inside of docker and swarm.

if you do an inspect here and you use the pretty flag you get a nice, clean, small output of just the current settings on the service definition and you can see in there the update settings 
as well as the one you just changed, so that's a neat little trick i want you to try it a couple of times if you can 


docker@default:~$ docker service update --update-monitor 15s fireflyy
fireflyy
overall progress: 5 out of 5 tasks
1/5: running   [==================================================>]
2/5: running   [==================================================>]
3/5: running   [==================================================>]
4/5: running   [==================================================>]
5/5: running   [==================================================>]
verify: Service converged



docker@default:~$ docker service inspect --pretty fireflyy

ID:             xn1z10ym5zipqqonn6jnljxb7
Name:           fireflyy
Service Mode:   Replicated
 Replicas:      5
Placement:
 Constraints:   [node.hostname==default]
UpdateConfig:
 Parallelism:   1
 On failure:    pause
 Monitoring Period: 15s
 Max failure ratio: 0
 Update order:      stop-first
RollbackConfig:
 Parallelism:   1
 On failure:    pause
 Monitoring Period: 5s
 Max failure ratio: 0
 Rollback order:    stop-first
ContainerSpec:
 Image:         bretfisher/browncoat:v1@sha256:bce112dc20a9112fec64696d047ebf5cca98bc97ab9290d6aa99a652f7ea2357
 Init:          false
Resources:
Networks: versee
Endpoint Mode:  vip
Ports:
 PublishedPort = 81
  Protocol = tcp
  TargetPort = 10
  PublishMode = ingress

docker@default:~$


3.update 5 at a time, force an update without changes 
---------------------------------------------------
docker service scale fireflyy=15
docker service update --update-parallelism 5 --force fireflyy


the next one here is updating five at a time so we talked about the parallelism of updating different tasks at the same time, in a large service ,
so here you are going to change the scale to 15, thats going to give you a lot of tasks to be able to do a parallel update on and then you can do the update parallelism 5 
and we are going to do a force here.
remember the force is to make sure that every conatiner is redeployed in every task rgardless of whether there was a op change,
so in the previous one, where you ad a NO-Op where it doesn't change the actual tasks themselves, it just changes the service definition, 
we are showing ann example here of how if you really want to go ahead and just redeploy all the tasks for some reason, you can throw in the force and the force will always make sure that every task gets 
replaced even if it doesn't need to be replaced

4.start new container first before killing old one (watch with docker events )
------------------------------------------------------
docker service scale fireflyy=1
docker service update --update-order start-first --force fireflyy

and the last one i want you to try here is starting the new task before the old one shuts down...and this is a really key one for using the docker events.
i wouldn't mind if you actually went through it twice, once where you are forcing an update before you change it to start first  and then forcing an update after you ve changed it to start first.
and what you will see if you are looking at the docker events, is you will see that action taking place .

and in the next video i am going to gop through these myself and explain a little bit about them in case you are curious, you don't have to watch it if you are don't want to 
and if you are already running these tools and youa re already understanding of this, you can skip my answers for this assignment 

but i might throw in a few tipbits along the way about how it works 


what i forgot to do was to implement the visualizer.......

40-----------------------------------------------------assignment-try update options----------------------------------------


41-----------------------------------------------------assignment answers-try update options----------------------------------------

all right hopefully you have done the assignment on your own played around with some things and you are just going to watch this real quick to see if there is anything 
you can clean out of maybe how i am doing it...because we all do it a little bit differently, 
there will be a couple of things i will do that i did not mention in the assignment but you have learned before 




C:\Users\michael.kourbelis>  docker-machine ssh default
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@default:~$ docker service ls
ID                  NAME                MODE                REPLICAS            IMAGE               PORTS
docker@default:~$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
6baf6531e09d        bridge              bridge              local
b1b92aac500e        docker_gwbridge     bridge              local
5b2809a7630c        host                host                local
8j0wnrye4qjt        ingress             overlay             swarm
09077b67034b        none                null                local
in73ejfvfk4m        versee              overlay             swarm
docker@default:~$ exit
logout

C:\Users\michael.kourbelis>docker network ls
NETWORK ID     NAME              DRIVER    SCOPE
6baf6531e09d   bridge            bridge    local
b1b92aac500e   docker_gwbridge   bridge    local
5b2809a7630c   host              host      local
8j0wnrye4qjt   ingress           overlay   swarm
09077b67034b   none              null      local
in73ejfvfk4m   versee            overlay   swarm

C:\Users\michael.kourbelis>docker network rm versee
versee

C:\Users\michael.kourbelis>   docker-machine ssh default
   ( '>')
  /) TC (\   Core is distributed with ABSOLUTELY NO WARRANTY.
 (/-_--_-\)           www.tinycorelinux.net

docker@default:~$ docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
6baf6531e09d        bridge              bridge              local
b1b92aac500e        docker_gwbridge     bridge              local
5b2809a7630c        host                host                local
8j0wnrye4qjt        ingress             overlay             swarm
09077b67034b        none                null                local
docker@default:~$


so we understand that if we perform an action outside of a docker-machine it affects and the docker-machine and maybe vice versa.

C:\Users\michael.kourbelis>  docker-machine ssh default
docker@default:~$            docker network create --driver overlay --attachable verse
d2pnosz8fzlnd0dmo3nvj743h

docker@default:~$            docker service create --name fireflyy -p 81:10 --network verse --replicas 5 --constraint "node.hostname==default" bretfisher/browncoat:v1
kf3rk9uibb96m25601aonsjf0
overall progress: 5 out of 5 tasks
1/5: running   [==================================================>]
2/5: running   [==================================================>]
3/5: running   [==================================================>]
4/5: running   [==================================================>]
5/5: running   [==================================================>]
verify: Service converged




i am going to do over here in this one tab i.e. give the command 

docker events -f service=fireflyy 

docker events....so i can see each action happening in real time through the swarm API....
but i m also going to an " -f " here or a filter, 
you can look at the docker documentation at batman and superman site an the events command and the filters that you can use,
and after the -f i am going to say this "service=fireflyy" 

in this case i am only going to see swarm and node events in the swarm related to the fireflyy service that i create so that anything else including the visualizer will not show up 
and confuse me 


docker service create   --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock  dockersamples/visualizer


if you go on the browser and see the visualizer to be a mini little window with the name "viz" which is the visualizer running and is a very useful tool as 
it shows to you what's going on so you can learn how things are scaling up and down and how the changes are being made in real time 
but between the two of those hopefully we can see a lot of things happening here,
all right first i need to create the network that i created above...........and then the service 



1.create service (constraint to current node)
docker network create --driver overlay --attachable verse
docker service create --name fireflyy -p 81:10 --network verse --replicas 5 --constraint "node.hostname==default" bretfisher/browncoat:v1


and i am going to add this constraint like i mentioned in the assignment of node.hostname == default 

all right, so again the constraint here is so that i simply have everything running on the same node i am on so that way assuming that yours i scalled node1
.....if you have a series of swarm server that are called something different, you would need to put the node in there...the node name 
for example if i do a docker node ls 
i have the default node which also i am cuyrreently talk to this node, 
so for you its important that it's name there.....the node.hostname == is whatever the nodename is 
hpoefully you are familiar now with the constraints from the previous section on different ways to control placement that you are familiar enough that taht makes sense to you 


so now i have my services running and this is illustrated to the visualizer...you can see that you have five red ones 
and the green one is the visualizer, the five red ones are the firefly.
i see those but really what i want to do is just want to get a sense for what's happening over there 

i am now in the middle one here  and i am going to run the docker events and over on the left i am going to start typing the commands that i gave you as an example.
so the first one here  is 

2. monitor for 15 seconds before next task (no-op)

docker service update --update-monitor 15s fireflyy.....okay it will cause this command to wait 15  seconds before it ends but you will notice it didn't replace any 
                                                        tasks, this is known as a NOOP and we explain that in the assignment that you will see over in the middle section 
                                                        you will see that it updated the service (through docker events -f service=fireflyy)  
                                                        but you didn't see any container dying or creating or stopping 
                                                        because it didn't require that,

so now if we do a docker service inspect......docker service inspect --pretty fireflyy 

you will see the constraint that i ve added, we have 5 replicas 


Placement:
 Constraints:   [node.hostname==default]

but the most important here is i was wanting to show you that the update config is stop-first "  Update order:      stop-first "

like that's the defsault, you know that's the default for now, and the the monitoring period,
which we just changed to 15 seconds, now i am going to go ahead and change this monitoring back to 5 seconds because i don't want to have to keep waiting for every one of these other commands.
that's what will hapen is at the end of every service update command, it will wait 15 seconds to make sure that this stuff is stable before moving forward, 
we don't want to do that permanently 

docker service update --update-monitor 5s fireflyy


now the next one here is 

3.update 5 at a time, force an update without changes 

docker service scale fireflyy=15.....
and you will see on the right, i get a whole bunch of containers creates,container starts, and you will notice that in my three-node swarm 
the constraints are following the rules its only putting them on the default node, you will see this big long line in default node and you see the container creates and then the
container starts    
and you will notice that in my three node swarm the constraints are following the rules, its only putting them on the default node you see this big long line of default node 
and you will see the containers creates and then the container start....
so you will see that stuff in here in the middle pane, so that all looks good,
now what we want to do is i want to do an update of these and we are going to do a force because normally just changing the update parallelism value is just a NOOP metadata change, 
it will not deploy new containers.....so i need to do a force in order to get this example to work 

docker service update --update-parallelism 5 --force fireflyy




we are going to say 5 at a time we are going to force it  and then what we should see....yeah it does all five at a time and once those are ready 
you will change five more and you will see in the visualizer because we are doing stop first you will see the visualizer shrink and then make more and then shrink and make more 
because that's how the stop first works.



okay so the last one here is to just change a startup order in case you are in a situation where you only have one replica and you need to make sure that the new oe actually starts first 
before the old one goes away, 
so right now we have the 15 replicas that we scaled up to a while ago  


4.start a new container first before killing old one (watch with events )

and lets do a docker service scale to get it back down 
--docker service scale firefly=1......
and then we see that a bunch of them get killed, you will see that a bunch of them get killed  a whole bunch of container  dies, container stops,   or something and these all over the 
docker events window.

now i am going to do this a little differently than i did in the assignment i am going to do a 

docker service update --update-order start-first fireflyy....

you see that i didn't do a force, so n the middle section i am going to do 

 docker events -f service=fireflyy

 and when i run this, this is another NOOP change, now interestingly enough why did we just see a bunch of these container destroys ?
 all we did was a NOOP right. 
 it was supposed to be just update order changing the metadata to start firts....well this has to do with a little nuance and it doesn't affect most people in most situations 
 but it's just curious, if you are watching this video this is like a bonus   

 what's happening is there's a limited number of tasks that can stay around after you have updated a container, 
 previously we had scaled down to one so there was all these containers, there was 14 technically, these containers sitting around that weren't running but that were still thgere in the task 

 so if you ever do a
 
  docker service ps fireflyy 

you will see that there are three or four other formerly running, right so if i sort of move out a little bit, you can....kind of hard to see there, 
but basically there are four tasks that are the old shutdown tasks. 
that gives you the history right, and that's known as task history by default......it's the current one plus four others or five total,
you can use a docker info command to see that task history we have talked a little bit about it before but what's happening here is that those containers were existing 
they were stopped but the hadn't yet been destroyed because when you destroy a container you destroy its logs....

if you are using the default docker logs they stay around along with the container and the container stays around as long as the task histroy allows it to stay around.
so when we changed this again  and we did another NOOP command, it gave another version for rollback purposes and this in effect the two things collide and basically means that 
it's going to get rid of a whole lot of those old tasks and destroy them as well as their logs,
so we saw those here, it wasn't actually stopping......you notice that there was no stops, there was no kills, there was no shutdowns, it was simply destroying the objects 
because we had multiple commands since that onbject.

Anyway. it's a weird little thing  and the more you play with this stuff, the more you watch docker events the more this stuff will start to make sense to you.
but that's just a little thing, it doesn't affect our testing right now.

so the last command we want to run is to do an actual update and see this 

docker service update --update-order start-first --force fireflyy


of course on the side i am going to do a docker events and the last one is to do a docker service update --force fireflyy 

so i am going to break the above command into two and i am first doing the NOOP update order of start first.
we can actually verify this with 

docker service inspect --pretty firefly 

and you see there taht we are now in an update order of start-first 


so now when we do this command.....docker service update --force firefly 

this will be something that forces us to replace a container. but you will see here in the middle, only a few things will happen 
it will create the new container, it will bring that one up and then it will shut down the old container 

of course we run the command 

docker events -f service=firefly


lets break that down real quick before we end this video.....you will see from the output of " docker events -f service=firefly "
that we did a service update and then create some new containers 
that's tottally normal 
but then it starts the container, that's whats different is it started the new one before it changed the old one, and then once that started  its going to destroy the old one 
which prompts a kill, a die and eventually a stop 
then the service update is now gone from updating status to a completed status .....and thats the final line 
if i wanted to change it back obviosly i would have to do a docker service update to update order stop first which is the default 

i see you in the next section that we are gonna say about healtchecks.....

41-----------------------------------------------------assignment answers-try update options----------------------------------------

42-----------------------------------------------------how healtchecks affect updates----------------------------------------

batteries included, but swappable

--health-cmd            command to run to chaeck health 
--health-interval       (defaults 30s) time between running the check (ms|s|m|h)
--health-retries        (default 3) consecutive failures needed to report unhealthy
--health-start-period   (default 0s) time for container to start before counting to unstable
--health-timeout        (default 30s) maximum time to allopw one check to run 

--stop-grace-period     time to wait before force killing a container (ns|us|ms|hs)
--no-healtcheck          disable any container-specific HEALTCHECK




hopefully you've gotten the point that healtcheck are important in swarm or in any container production system and healtchecks in swarm are nice and simple 
right out of the gate, we have already talked about the other options for healtchecks either in your dockerfiles your compose files or in the service create commands,
any of those places will work...
you have five main options here, they all start with the health, so they are easy to find in the help,
they have defaults that you can leave there the one that i see people changing the most is the interval, 
usually people want a healtcheck more often thatn every 30 seconds,especially if it's a rather inexpensive one.
by that i mean a healtcheck that's a really simple healtcheck that doesn't require a lot of work to happen,
maybe just checking the default page for a 200 return something simple like that, 
maybe as you get a more complex, these will get more costly in terms of the resources required because you don't really want to make your app work really   
hard all the time just to keep health right, so you maybe have to sort of balance the backend and forth of how often should i do it, versus how complicated of the healtcheck should it be, 
there are other optons though that will affect your healtchecks particullarly the stop grace period and that's the time that you allow for your containers to properly shutdown.
by default i believe this is 10 seconds its just like the docker run where if you are stopping a docker container by default it will wait ten seconds for it to properly shutdown and if not
it will then kill it.
we have talked about that in the docker mastery course and that's no different than in docker swarm so stop grace period allows you to give a longer timeline and that's pretty important in 
production when you are dealing  with connections that maybe need to last a little bit longer, maybe even five minutes or ten minutes, maybe you have user uploads that take a long time,
so you need to ensure that you don't cut them off in the middle, that will be very specific to your application, but that's going to affect the healtchecks because if you are shutting down a container
and the health starts to go bad, that's not good, you want your application to be able to shutdown in a reasonable way, without being killed and not losing connections 
but also not going unhealthy 

the no-healtcheck her simply disables any healtcheck options that might have been set elsewhere, so if you are running a swarm service and you run that, anything you put in the dockerfile will be ignored.

but it always helps to have some way to look at how this is functioning in examples, in a visual way.


so mark church from the docker team has a really greate example of the lifecycle of a task throughout its healtcheck life, and so on the left,

this is an example of a task starting on the left here, the light blue area is the portion where healtchecks have not been run yet and then in the first healtcheck kicks off that is healthy 
and that is waht activated docker saying ok now that this app has been considered healthy in this container i will always expect all future healtcheck to be healthy 
so what if we had in this case at the first orange line the application failuresmaybe you ve got ten seconds of time where the application is broken but we don't know it.
then the first healtcheck fails, the default with the healtchecks is that three in a row have to fail. the application is still considered healthy by healtcheck, 
but it's not responding in some way, so there's probably you know some users getting errors or something at this point.
then the second healtcheck happens it also fails the healtcheck so now we are two and then before the thord strike the application suddenly recovers maybe it got its connections straightened out 
or maybe it recovered from a low memory problem or something likr that, so it recoveres right before that third healtcheck, the third healtcheck is validated as good and that means that nothiong will happen 
swarm will do nothing to your app it may not even alert you depending on how you set up your monitoring systems like prometheus or something else, 
if you only have them set to alert you when a task goes unhealthy this technically never had ann unhealthy result,
now you may not like this, you may think wow that's really bad i want to know right away, 
the reallity is that applications can misbehave in all sorts of ways, you are going to have seconds of downtime in a misbehaving app.
i don't know any app that can guarantee zero downtime on this kind of situation  
so if you are not happy with those results, maybe you have it chack every five seconds for a healthy app,
maybe you only allow two failures in a row before it takes action.
you can always tighten this stuff up  but i always advise you at first to start conservatively.
start with these defaults and then as you get comfortable with healtchecks, make them more complicated, more robust and tighten them up 
whats youo don't want to do is go out of the gate hardcore with healtcheck happening every one second and a single failure it replaces the task because if you do that 
you are going to probabaly end up with.....you are making your apps work really hard and just to keep health they are not able to do a lot of work for users because they are so busy spending their time 
replying to healtchecks and secondly in distributed systems especially on the internet we learn that all sorts of random    things can happen for just even a second or two.
you can have hiccups that only cause your application to be slow for a few seconds or cause a blip of something and that doesn't mean that the application should completely be replaced   
so you are going to need to figure out how to balance those needs of uptime versus exhausting your system with too much checking 

so here is a slightly different result same thing on the left, we have an application that failed right before the first healtcheck or during any time of its lifecycle 
the first healtcheck happens it is considered failure, the second one fails, then the third healtcheck fails,

now we are assuming the default again, so the third healtcheck means that it's now marked as unhealthy and when a container goes unhealthy in swarm swarm does something about it 
it will then remove that task from the load balancer it will send a sigterm which is the proper way to healthy shutdown a container to that unhealthy container 
now if that container is really unhealthy and it doesn't respond to that sigterm that's where the stop grace period comes into effect, if it takes longer to shutdown than that grace  period by default ten seconds 
 it will then kill it,


 once its killed it it will replcae it with a new task, sop its greate that mark made these because i think that looking at rthese and trying to understand and go back and forth between them, you can sort of see now 
 the lifecycle of an app and how the healtcheck really makes it  a more robust solution and really out of the box you get all this free intelligence if you jsut put your healtchecks in there with swarm 
 auto managing your app and replacing it when it goes down 6.50








42-----------------------------------------------------how healtchecks affect updates----------------------------------------


------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                        section 8                                                     
                                                limit downtime with rolling updates healthchecks and rollbacks 
------------------------------------------------------------------------------------------------------------------------------------------------------------------------